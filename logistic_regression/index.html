<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../linear_regression/ rel=prev><link href=../gaussian_discriminant_analysis/ rel=next><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>Logistic Regression - Machine Learning</title><link rel=stylesheet href=../assets/stylesheets/main.342714a4.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#logistic-regression class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Machine Learning" class="md-header__button md-logo" aria-label="Machine Learning" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Logistic Regression </span> </div> </div> </div> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Introduction </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../linear_regression/ class=md-tabs__link> Supervised Learning </a> </li> <li class=md-tabs__item> <a href=../kmeans/ class=md-tabs__link> Unsupervised Learning </a> </li> <li class=md-tabs__item> <a href=../activation_function/ class=md-tabs__link> Supporting Concepts </a> </li> <li class=md-tabs__item> <a href=../notebooks/ class=md-tabs__link> Notebook Gallery </a> </li> <li class=md-tabs__item> <a href=../about/ class=md-tabs__link> About Author </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Machine Learning" class="md-nav__button md-logo" aria-label="Machine Learning" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Supervised Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Supervised Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../linear_regression/ class=md-nav__link> <span class=md-ellipsis> Linear Regression </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Logistic Regression </span> </a> </li> <li class=md-nav__item> <a href=../gaussian_discriminant_analysis/ class=md-nav__link> <span class=md-ellipsis> Gaussian Discriminant Analysis </span> </a> </li> <li class=md-nav__item> <a href=../knn/ class=md-nav__link> <span class=md-ellipsis> K-Nearest Neighbors (KNN) </span> </a> </li> <li class=md-nav__item> <a href=../perceptron/ class=md-nav__link> <span class=md-ellipsis> Perceptron Algorithm </span> </a> </li> <li class=md-nav__item> <a href=../neural_network/ class=md-nav__link> <span class=md-ellipsis> Neural Networks </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Unsupervised Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Unsupervised Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../kmeans/ class=md-nav__link> <span class=md-ellipsis> K-Means Clustering </span> </a> </li> <li class=md-nav__item> <a href=../pca/ class=md-nav__link> <span class=md-ellipsis> Principal Component Analysis (PCA) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Supporting Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Supporting Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../activation_function/ class=md-nav__link> <span class=md-ellipsis> Activation Functions </span> </a> </li> <li class=md-nav__item> <a href=../loss_function/ class=md-nav__link> <span class=md-ellipsis> Loss Functions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../notebooks/ class=md-nav__link> <span class=md-ellipsis> Notebook Gallery </span> </a> </li> <li class=md-nav__item> <a href=../about/ class=md-nav__link> <span class=md-ellipsis> About Author </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=logistic-regression>Logistic Regression</h1> <h2 id=table-of-contents>Table of Contents</h2> <ol> <li><a href=#introduction>Introduction</a></li> <li><a href=#data-representation>Data Representation</a></li> <li><a href=#model-hypothesis>Model Hypothesis</a></li> <li><a href=#loss-function>Loss Function</a></li> <li><a href=#gradient>Gradient Of Loss Function</a></li> <li><a href=#method-2-when--y-in--1-1->Method 2: When <span class=arithmatex>\(y \in \{-1, 1\}\)</span></a></li> <li><a href=#conclusion>Conclusion</a></li> </ol> <h2 id=introduction>Introduction</h2> <p>Logistic regression models a linear relationship between input features <span class=arithmatex>\(X\)</span> and a binary target vector <span class=arithmatex>\(y\)</span>. It applies the sigmoid function to the linear combination of inputs to produce an output between 0 and 1, which is then thresholded to make a binary class prediction. Matrix operations enable efficient computation and scalability to large datasets.</p> <hr> <h2 id=data-representation>Data Representation</h2> <p>Let:</p> <ul> <li><span class=arithmatex>\(n\)</span>: number of training examples </li> <li><span class=arithmatex>\(d\)</span>: number of features </li> </ul> <p>We define:</p> <ul> <li><span class=arithmatex>\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span>: feature matrix </li> <li><span class=arithmatex>\(\mathbf{y} \in \{0,1\}^n\)</span>: target vector </li> <li><span class=arithmatex>\(\beta \in \mathbb{R}^{d \times 1}\)</span>: weight vector</li> </ul> <div class=arithmatex>\[ \mathbf{X} = \begin{bmatrix} x_1^{(1)} &amp; x_2^{(1)} &amp; \dots &amp; x_d^{(1)} \\ x_1^{(2)} &amp; x_2^{(2)} &amp; \dots &amp; x_d^{(2)} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_1^{(n)} &amp; x_2^{(n)} &amp; \dots &amp; x_d^{(n)} \end{bmatrix} \]</div> <hr> <h2 id=model-hypothesis>Model Hypothesis</h2> <p>In logistic regression, we compute a linear combination of input features and model parameters, and then apply the <strong>sigmoid function</strong> to make the result into the range <span class=arithmatex>\([0, 1]\)</span>. The hypothesis function is:</p> <div class=arithmatex>\[ \hat{\mathbf{y}} = \sigma(\mathbf{X} \beta) \]</div> <p>where the sigmoid function <span class=arithmatex>\(\sigma(z)\)</span> is defined as:</p> <div class=arithmatex>\[ \sigma(z) = \frac{1}{1 + e^{-z}} \]</div> <hr> <h2 id=loss-function>Loss Function</h2> <p>To derive the loss function for logistic regression, we use maximum likelihood estimation (MLE), assuming that each target label is drawn from a Bernoulli distribution.</p> <h3 id=assumption-bernoulli-targets>Assumption: Bernoulli Targets</h3> <p>We assume that the training dataset <span class=arithmatex>\(\{(x_i, y_i)\}_{i=1}^n\)</span> are independent and identically distributed (iid), and that the target <span class=arithmatex>\(y_{i} \in \{0, 1\}\)</span> follows a Bernoulli distribution parameterized by <span class=arithmatex>\(\theta_i\)</span>:</p> <div class=arithmatex>\[ y_{i} \sim \text{Bernoulli}(\theta_i) \]</div> <p>where <span class=arithmatex>\(\theta_i = \hat{y}_{i} = \sigma(\beta^\top x_{i})\)</span>. The probability mass function of the Bernoulli distribution is:</p> <div class=arithmatex>\[ P(y_{i}\;\theta_i) = \theta_{i}^{y_{i}} (1 - \theta_{i})^{1 - y_{i}} \quad \text{ where } 0\le\theta_i \le 1, y_{i} = 0,1 \]</div> <p>Now computing the likelihood <span class=arithmatex>\(L(\beta)\)</span>:</p> <div class=arithmatex>\[ \begin{align*} L(\beta) &amp;= \prod_{i=1}^n P(y_{i}; \theta_i) \quad \text{ because of iid}\\ &amp;= \prod_{i=1}^n \theta_{i}^{y_{i}} (1 - \theta_{i})^{1 - y_{i}}\\ &amp;= \theta_{i}^{\sum_{i=1}^n y_{i} } (1 - \theta_{i})^{\sum_{i=1}^n (1 - y_{i})} \end{align*} \]</div> <p>Next we computing the Log-Likelihood (<span class=arithmatex>\(\log L(\beta)\)</span>),</p> <div class=arithmatex>\[ \begin{align*} \log L(\beta) &amp;=\log \left(\theta_{i}^{\sum_{i=1}^n y_{i} } (1 - \theta_{i})^{\sum_{i=1}^n (1 - y_{i})}\right)\\ &amp;= \sum_{i=1}^n y_{i} \log \theta_{i} + \sum_{i=1}^n (1 - y_{i}) \log (1 - \theta_{i})\\ &amp;= \sum_{i=1}^n \left(y_{i} \log \theta_{i} + (1 - y_{i}) \log (1 - \theta_{i}) \right) \end{align*} \]</div> <p>Finding the negative of Log-Likelihood <span class=arithmatex>\(-\log L(\beta)\)</span> we have our final loss function as Negative-Log-Likelihood <span class=arithmatex>\(NLL\)</span> given as:</p> <div class=arithmatex>\[ NLL = - \sum_{i=1}^n \left( y_{i} \log \theta_{i} + (1 - y_{i}) \log (1 - \theta_{i})\right) \]</div> <p>Since <span class=arithmatex>\(0\le\theta_{i} \le 1\)</span> then we can use a sigmoid function to generate it hence we have <span class=arithmatex>\(\theta_{i} = \hat{y}_{i} = \sigma(\beta^\top x_{i})\)</span>. And finally we have a loss function known as binary cross entropy given as </p> <div class=arithmatex>\[ NLL = - \sum_{i=1}^n \left( y_{i} \log \sigma(z_{i}) + (1 - y_{i}) \log (1 - \sigma(z_{i}))\right) \quad \text{ where } \sigma(z_{i}) = \frac{1}{1 + e^{-z_{i}}}, z_{i} = \beta^\top x_{i} \]</div> <div class=arithmatex>\[ \boxed{\text{binary cross entropy} = - \sum_{i=1}^n\left(y_{i} \log \sigma(z_{i}) + (1 - y_{i}) \log (1 - \sigma(z_{i}))\right)} \]</div> <hr> <h2 id=gradient-of-the-loss-function>Gradient of the Loss Function</h2> <p>Now we want to find the gradient of our loss function Negative-Log-Likelihood <span class=arithmatex>\(NLL\)</span> using chain rule i.e. $$ L = - \sum_{i=1}^n\left(y_{i} \log \sigma(z_{i}) + (1 - y_{i}) \log (1 - \sigma(z_{i}))\right) $$ $$ \frac{d L}{d \beta} = \frac{d L}{dz_{i}} \times \frac{d z_{i}}{d \beta} $$</p> <h3 id=note><strong>NOTE</strong></h3> <p>$$ \sigma(z_{i}) = \frac{1}{1 + e^{-z_{i}}}, \quad \text{where } z_{i} = \beta^\top x_{i} $$ To find the derivative of <span class=arithmatex>\(\sigma(z_{i})\)</span> with respect to <span class=arithmatex>\(z_{i}\)</span>, we have:</p> <div class=arithmatex>\[ \boxed{ \begin{aligned} \frac{d\sigma}{dz_{i}} &amp;= \frac{d}{dz_{i}} \left( \frac{1}{1 + e^{-z_{i}}} \right) \\ &amp;= \frac{d}{dz_{i}} \left( (1 + e^{-z_{i}})^{-1} \right) \\ &amp;= -1 \cdot (1 + e^{-z_{i}})^{-2} \cdot \frac{d}{dz_{i}}(1 + e^{-z_{i}}) \\ &amp;= - (1 + e^{-z_{i}})^{-2} \cdot (-e^{-z_{i}}) \\ &amp;= \frac{e^{-z_{i}}}{(1 + e^{-z_{i}})^2} \\ \\ \text{Now recall:} \quad \sigma(z_{i}) &amp;= \frac{1}{1 + e^{-z_{i}}} \quad \Rightarrow \quad 1 - \sigma(z_{i}) = \frac{e^{-z_{i}}}{1 + e^{-z_{i}}} \\ \\ \text{So:} \quad \frac{d\sigma}{dz_{i}} &amp;= \sigma(z_{i})(1 - \sigma(z_{i})) \end{aligned} } \]</div> <p>Now continueing with the derivative of our binary cross entropy loss <span class=arithmatex>\(L\)</span> w.r.t <span class=arithmatex>\(z_{i}\)</span> we have</p> <div class=arithmatex>\[ \begin{aligned} \frac{dL}{dz_{i}} &amp;= -\left(y_{i}\cdot \frac{1}{\sigma(z_{i})} \cdot \sigma(z_{i})(1 - \sigma(z_{i})) + (1 - y_{i}) \cdot \frac{1}{(1 - \sigma(z_{i}))} \cdot -\sigma(z_{i})(1 - \sigma(z_{i}))\right)\\ &amp;= - \left[y_{i}(1 - \sigma(z_{i})) - (1 - y_{i}) \sigma(z_{i})\right]\\ &amp;= - \left[y_{i} - y_{i}\sigma(z_{i}) - \sigma(z_{i}) + y_{i}\sigma(z_{i})\right]\\ &amp;= - \left[y_{i} - \sigma(z_{i})\right]\\ &amp;= \sigma(z_{i})-y_{i} \end{aligned} \]</div> <p>Next we solve for <span class=arithmatex>\(\frac{d z_{i}}{d \beta}\)</span></p> <p><span class=arithmatex>\(z_{i} = \beta^\top x_{i}\)</span></p> <p><span class=arithmatex>\(\frac{d z_{i}}{d \beta} = x_{i}\)</span></p> <p>Therefore we have our final gradient as</p> <div class=arithmatex>\[ \begin{align*} \frac{d L}{d \beta} &amp;= \frac{d L}{dz_{i}} \times \frac{d z_{i}}{d \beta}\\ &amp;= \sum_{i=1}^n\left(\sigma(z_{i})-y_{i}\right) x_{i} \end{align*} \]</div> <p>The gradient in vectorized form is:</p> <div class=arithmatex>\[ \boxed{ \frac{d L}{d \beta} = \sum_{i=1}^n \left( \sigma(z_i) - y_i \right) x_i } \]</div> <p>When $ X $ is a data matrix, the gradient becomes:</p> <div class=arithmatex>\[ \boxed{ \frac{d L}{d \beta} = X^\top \left( \sigma(X\beta) - y \right) } \]</div> <h2 id=method-2-when-y-in-1-1>Method 2: When <span class=arithmatex>\(y \in \{-1, 1\}\)</span></h2> <p>In some variants, target labels are represented using <span class=arithmatex>\(-1\)</span> and <span class=arithmatex>\(+1\)</span>. The logistic regression model and loss function can be adjusted accordingly.</p> <p>Now we are going to derive the hypothesis, loss function, and the gradient.</p> <h3 id=hypothesis>Hypothesis</h3> <p>Let</p> <div class=arithmatex>\[ \mathcal{D} = \left\{ (x_i, y_i) \right\}_{i=1}^{n} \quad \text{where } x_i \in \mathbb{R}^{d},\quad y_i \in \{-1,1\},\quad w \in \mathbb{R}^{d} \]</div> <p>and <span class=arithmatex>\(\hat{y} = w^\top x\)</span></p> <p>Assuming <span class=arithmatex>\(P(Y=-1\mid X)\)</span> and <span class=arithmatex>\(P(Y=1\mid X)\)</span> are defined, then</p> <div class=arithmatex>\[ \frac{P(Y=1\mid X)}{P(Y=-1\mid X)} \in (0,\infty) \]</div> <p>Now introducing the logarithm, we have</p> <div class=arithmatex>\[ \log \left(\frac{P(Y=1\mid X)}{P(Y=-1\mid X)} \right) = w^\top x \in \mathbb{R} \]</div> <p>This is equivalent to</p> <div class=arithmatex>\[ \begin{align*} &amp;\frac{P(Y=1\mid X)}{P(Y=-1\mid X)} = e^{w^\top x}\\ &amp; P(Y=1\mid X) = \left[1-P(Y=1\mid X)\right]e^{w^\top x} \quad \text{since } P(Y=1\mid X) + P(Y=-1\mid X) =1\\ &amp; P(Y=1\mid X)\left[1+e^{w^\top x}\right] = e^{w^\top x}\\ &amp; P(Y=1\mid X) = \frac{e^{w^\top x}}{1+e^{w^\top x}} = \frac{1}{1+e^{-w^\top x}} \end{align*} \]</div> <p>Therefore we have</p> <div class=arithmatex>\[ P(Y=1\mid X) = \frac{1}{1+e^{-w^\top x}} = \sigma(w^\top x) \]</div> <p>And</p> <div class=arithmatex>\[ P(Y=-1\mid X) = \frac{1}{1+e^{w^\top x}} = \sigma(-w^\top x) \]</div> <p>Therefore, we can conclude our <strong>Hypothesis</strong> as</p> <div class=arithmatex>\[ \boxed{P(Y=y\mid X) = \frac{1}{1+e^{-yw^\top x}} = \sigma(yw^\top x)} \]</div> <p>where <span class=arithmatex>\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is the sigmoid function.</p> <h3 id=loss-function_1>Loss Function</h3> <p>From <strong>Maximum Likelihood Estimation (MLE)</strong>, the negative log-likelihood is given as</p> <div class=arithmatex>\[ NLL(w) = -\sum_{i=1}^{n} \log P(y_i \mid x_i; w) \]</div> <p>From the hypothesis derived earlier, we have</p> <div class=arithmatex>\[ P(y_i \mid x_i; w) = \frac{1}{1 + e^{-y_i (w^\top x_i)}} \]</div> <p>Now solving for <span class=arithmatex>\(NLL(w)\)</span>:</p> <div class=arithmatex>\[ \begin{align*} NLL(w) &amp;= -\sum_{i=1}^{n} \log \left(\frac{1}{1 + e^{-y_i (w^\top x_i)}}\right)\\ &amp;= -\sum_{i=1}^{n} \log \left(1 + e^{-y_i (w^\top x_i)}\right)^{-1}\\ &amp;= \sum_{i=1}^{n} \log \left(1 + e^{-y_i (w^\top x_i)}\right) \end{align*} \]</div> <p>Therefore, our <strong>Loss Function</strong> is given as</p> <div class=arithmatex>\[ \boxed{\mathcal{L}(w) = \sum_{i=1}^{n} \log \left(1 + e^{-y_i (w^\top x_i)}\right)} \]</div> <h3 id=gradient>Gradient</h3> <p>Now we find the gradient of our loss function <span class=arithmatex>\(\mathcal{L}(w)\)</span> with respect to <span class=arithmatex>\(w\)</span>:</p> <div class=arithmatex>\[ \begin{align*} \frac{d \mathcal{L}(w)}{d w} &amp;= \sum_{i=1}^{n} \frac{d}{d w} \log \left(1 + e^{-y_i (w^\top x_i)}\right) \\ &amp;= \sum_{i=1}^{n} \frac{1}{1 + e^{-y_i (w^\top x_i)}} \cdot \left(-e^{-y_i (w^\top x_i)} \cdot y_i x_i\right) \\ &amp;= -\sum_{i=1}^{n} \left( \frac{e^{-y_i (w^\top x_i)}}{1 + e^{-y_i (w^\top x_i)}} \right) y_i x_i \\ &amp;= -\sum_{i=1}^{n} \left(1 - \frac{1}{1 + e^{-y_i (w^\top x_i)}} \right) y_i x_i \\ &amp;= -\sum_{i=1}^{n} \left(1 - \sigma(y_i w^\top x_i)\right) y_i x_i \end{align*} \]</div> <p>where <span class=arithmatex>\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is the sigmoid function.</p> <p>Therefore our gradient <span class=arithmatex>\(\nabla_w L\)</span> is given as; $$ \boxed{ \nabla_w L = -\sum_{i=1}^{n} \left(1 - \sigma(y_i w^\top x_i)\right) y_i x_i } $$</p> <h2 id=conclusion>Conclusion</h2> <p>Here&rsquo;s a table comparing the two formulations of logistic regression based on the label encoding: <strong><span class=arithmatex>\(y \in {0, 1}\)</span></strong> vs. <strong><span class=arithmatex>\(y \in {-1, 1}\)</span></strong>.</p> <table> <thead> <tr> <th>Feature</th> <th><strong><span class=arithmatex>\(y \in {0, 1}\)</span></strong></th> <th><strong><span class=arithmatex>\(y \in {-1, 1}\)</span></strong></th> </tr> </thead> <tbody> <tr> <td><strong>Label Encoding</strong></td> <td>0 for negative class, 1 for positive class</td> <td>-1 for negative class, +1 for positive class</td> </tr> <tr> <td><strong>Hypothesis Function</strong></td> <td><span class=arithmatex>\(\hat{y} = \sigma(\beta^\top x)\)</span></td> <td><span class=arithmatex>\(P(y \mid x) = \sigma(y \cdot w^\top x)\)</span></td> </tr> <tr> <td><strong>Sigmoid Argument</strong></td> <td><span class=arithmatex>\(\sigma(z) = \frac{1}{1 + e^{-\beta^\top x}}\)</span></td> <td><span class=arithmatex>\(\sigma(y \cdot w^\top x)\)</span></td> </tr> <tr> <td><strong>Loss Function</strong></td> <td><span class=arithmatex>\(-\sum_{i=1}^n \left[y_i \log \sigma(z_i) + (1-y_i) \log(1-\sigma(z_i))\right]\)</span></td> <td><span class=arithmatex>\(\sum_{i=1}^n \log(1 + e^{-y_i (w^\top x_i)})\)</span></td> </tr> <tr> <td><strong>Loss Name</strong></td> <td>Binary Cross-Entropy</td> <td>Log-Loss (equivalent in nature, different form)</td> </tr> <tr> <td><strong>Gradient of Loss Function</strong></td> <td><span class=arithmatex>\(\nabla_\beta = X^\top(\sigma(X\beta) - y)\)</span></td> <td><span class=arithmatex>\(\nabla_w = -\sum_{i=1}^n (1 - \sigma(y_i w^\top x_i)) y_i x_i\)</span></td> </tr> </tbody> </table> <p>Logistic regression models binary outcomes using a sigmoid function over a linear combination of features. We derived the binary cross-entropy loss using maximum likelihood and computed its gradient using the chain rule. The model can be expressed for both <span class=arithmatex>\(y \in \{0, 1\}\)</span> and <span class=arithmatex>\(y \in \{-1, 1\}\)</span>, with slightly different forms of the loss function and gradient. The final vectorized gradient enables efficient optimization using gradient descent.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../linear_regression/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Linear Regression"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Linear Regression </div> </div> </a> <a href=../gaussian_discriminant_analysis/ class="md-footer__link md-footer__link--next" aria-label="Next: Gaussian Discriminant Analysis"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Gaussian Discriminant Analysis </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.copy", "content.math", "navigation.footer", "navigation.tabs", "toc.integrate", "header.autohide"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../assets/mathjax/es5/tex-mml-chtml.js></script> </body> </html>