{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Machine Learning","text":"<p>This documentation covers the fundamental concepts and theory behind key machine learning algorithms. It is designed to provide clear explanations and mathematical intuition to help deepen your understanding of how these algorithms work.</p> <p>Use this as a starting point to explore the core ideas and techniques that power many machine learning applications today.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Linear Regression</li> <li>Linear Model Evaluation</li> <li>K-Fold Cross Validation</li> <li>Bias-Variance Decomposition</li> <li>Maximum Likelihood Estimation</li> <li>Examples: MLE Leading to Common Loss Functions</li> <li>Maximum A Posteriori (MAP) Estimation</li> <li>Gradient Descent from First-Order Taylor Approximation</li> <li>Gradient Descent (GD) Convergence</li> <li>Stochastic Gradient Descent (SGD)</li> <li>Mini-Batch Stochastic Gradient Descent</li> <li>Stochastic Gradient Descent (SGD) with Momentum</li> <li>Neural Network</li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>In supervised machine learning we usually start with the dataset given as:</p> \\[ \\mathcal{D} = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{n} \\] <p>Where \\(x_i \\in \\mathcal{X}, y_i \\in \\mathcal{Y}, (x_i, y_i) \\in \\mathcal{X} \\times \\mathcal{Y}\\) and \\(n\\) is the number of samples.</p> <p>\\(\\mathcal{X}\\) is known as the input space while \\(\\mathcal{Y}\\) is the output space.</p> <p>If \\(\\mathcal{Y}\\) is a finite set of discrete labels then the task is Classification. But if \\(\\mathcal{Y} \\in \\mathbb{R}\\) then the task is Regression.</p> <p>In ML we are looking for the best \\(F\\) such that:</p> \\[ F: \\mathcal{X} \\mapsto \\mathcal{Y} \\] <p>\\(F\\) is known as a function, mapping, or hypothesis. And where \\(F\\) comes from is known as the hypothesis class/space \\(\\mathcal{H}\\), i.e.,</p> \\[ F \\in \\mathcal{H} \\] <p>We also have an objective function (or cost function/criterion) where our main task is to optimize it.</p> <p>Now we look at one of the common hypothesis spaces known as linear mapping, which leads us to linear regression.</p>"},{"location":"#linear-regression","title":"Linear Regression","text":"<p>Here our hypothesis is given as:</p> \\[ f(x) = w^\\top x \\] <p>where \\(x\\) is known as the input data and \\(w\\) is the weights (coefficients).</p> <p>From the above linear mapping, we can have our objective function as:</p> \\[ L(w) = \\min_w \\;  \\left\\| w^\\top x - y \\right\\|^2 \\] <p>Now we can find the analytic solution \\(\\hat{w}\\) which minimizes our objective function \\(L(w)\\). To solve, we will find the derivative \\(L(w)\\) (gradient) then equate it to zero. Let\u2019s solve:</p> \\[ \\begin{align*}     L(w) &amp;= \\left\\| Xw - Y \\right\\|^2 \\\\          &amp;= (Xw - Y)^\\top (Xw - Y) \\\\          &amp;= w^\\top X^\\top X w - w^\\top X^\\top Y - Y^\\top X w + Y^\\top Y \\\\          &amp;= w^\\top X^\\top X w - w^\\top X^\\top Y - Y^\\top X w + Y^\\top Y \\end{align*} \\] <p>Now we find the gradient w.r.t \\(w\\):</p> \\[ \\begin{align*} \\frac{d L(w)}{d w} &amp;= X^\\top X w + (w^\\top X^\\top X)^\\top - X^\\top Y - (Y^\\top X)^\\top \\\\                    &amp;= X^\\top X w + X^\\top X w - X^\\top Y - X^\\top Y \\\\                    &amp;= 2 X^\\top X w - 2 X^\\top Y \\end{align*} \\] <p>Therefore, our gradient \\(\\nabla_w L(w)\\) is given as:</p> \\[ \\nabla_w L(w) = 2 X^\\top X w - 2 X^\\top Y \\] <p>Taking \\(\\nabla_w L(w) = 0\\):</p> \\[ \\begin{align*}     &amp; 2 (X^\\top X w - X^\\top Y) = 0\\\\     &amp; X^\\top X w =  X^\\top Y \\\\     &amp; \\hat{w} = (X^\\top X)^{-1} X^\\top Y \\end{align*} \\] <p>Therefore, the analytical solution \\(\\hat{w}\\) that minimizes \\(L(w)\\) is:</p> \\[ \\hat{w} = (X^\\top X)^{-1} X^\\top Y \\] <p>But the above solution exists only if \\(X^\\top X\\) is invertible, i.e., the inverse exists.</p> <p>If the inverse doesn\u2019t exist, we introduce a regularizer. Let\u2019s use the \\(L_2\\) regularizer (also known as ridge), where we will have a new objective function given as:</p> \\[ L_{\\text{ridge}}(w) = \\min_w \\;  \\left\\| w^\\top X - Y \\right\\|_2^2 + \\lambda  \\left\\| w \\right\\|_2^2 \\] <p>Where \\(\\lambda &gt; 0\\) (positive), since if it\u2019s negative and \\(\\left\\| w \\right\\|_2^2\\) is too big, then our \\(L_{\\text{ridge}}(w)\\) will not be bounded.</p> <p>Let\u2019s find the analytic solution of \\(L_{\\text{ridge}}(w)\\):</p> \\[ \\begin{align*}     L_{\\text{ridge}}(w) &amp;= \\left\\| Xw - Y \\right\\|_2^2 + \\lambda  \\left\\| w \\right\\|_2^2\\\\                         &amp;= (Xw - Y)^\\top (Xw - Y) + \\lambda w^\\top w \\\\                         &amp;= w^\\top X^\\top X w - w^\\top X^\\top Y - Y^\\top X w + Y^\\top Y + \\lambda w^\\top w \\end{align*} \\] <p>Now we find the gradient of \\(L_{\\text{ridge}}(w)\\) w.r.t \\(w\\):</p> \\[ \\begin{align*} \\frac{d L_{\\text{ridge}}(w)}{d w} &amp;= X^\\top X w + (w^\\top X^\\top X)^\\top - X^\\top Y - (Y^\\top X)^\\top + \\lambda w + \\lambda w \\\\                                   &amp;= X^\\top X w + X^\\top X w - X^\\top Y - X^\\top Y + 2\\lambda w \\\\                                   &amp;= 2 X^\\top X w - 2 X^\\top Y + 2\\lambda w \\end{align*} \\] <p>Therefore, our gradient \\(\\nabla_w L_{\\text{ridge}}(w)\\) is given as:</p> \\[ \\nabla_w L_{\\text{ridge}}(w) = 2 X^\\top X w - 2 X^\\top Y + 2\\lambda w \\] <p>Taking \\(\\nabla_w L_{\\text{ridge}}(w) = 0\\):</p> \\[ \\begin{align*}     &amp; 2 (X^\\top X w - X^\\top Y + \\lambda w) = 0\\\\     &amp; X^\\top X w + \\lambda w = X^\\top Y\\\\     &amp; (X^\\top X + \\lambda I)w =  X^\\top Y \\\\     &amp; \\hat{w} = (X^\\top X + \\lambda I)^{-1} X^\\top Y \\end{align*} \\] <p>Therefore, the analytical solution \\(\\hat{w}\\) that minimizes \\(L_{\\text{ridge}}(w)\\) is:</p> \\[ \\hat{w} = (X^\\top X + \\lambda I)^{-1} X^\\top Y \\] <p>The above solution from ridge regression always exists since \\(X^\\top X + \\lambda I\\) is always invertible.</p>"},{"location":"#linear-model-evaluation","title":"Linear Model Evaluation","text":"<p>For linear regression most time we evaluate our model using mean square error (mse).</p> <p>Given the dataset:</p> \\[ \\mathcal{D} = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{n} \\] <p>and a model \\(f(x) = w^\\top x\\), the Mean Squared Error (MSE) is defined as:</p> \\[ Loss_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( f(x_i) - y_i \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w^\\top x_i - y_i \\right)^2 \\] <p>Given the loss/criterion abive our aim is to find a function \\(\\hat{f}\\) which minimizes our loss i.e $$ \\hat{f} = \\arg\\min_{f \\in \\mathcal{H}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left( f(x_i) - y_i \\right)^2 $$  where;</p> <ul> <li>\\(f\\) - hypothesis</li> <li>\\(\\mathcal{H}\\) - hypothesis class</li> <li>\\(\\hat{f}\\) - best in \\(\\mathcal{H}\\)</li> <li>\\(\\frac{1}{n} \\sum_{i=1}^{n} \\left( f(x_i) - y_i \\right)^2\\) - emperical risk</li> </ul> <p>We have two type of risk:</p>"},{"location":"#1-emperical-risk","title":"1). Emperical Risk","text":"<p>We get it from our train dataset, which is given as our loss i.e  $$  R_{train} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( f(x_i) - y_i \\right)^2  $$</p> <ul> <li>This is also called training error or empirical risk \\(\\hat{R}(f)\\).</li> <li>It\u2019s computable since we have the training dataset \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n\\).</li> </ul>"},{"location":"#2-true-risk","title":"2). True Risk","text":"\\[  R = \\mathbb{E}_{(x,y)\\sim \\tau} \\left[ (f(x) - y)^2 \\right] \\] <ul> <li>\\(\\tau\\) is the true (unknown) data distribution.</li> <li>This is called expected risk or generalization error.</li> <li>We usually can\u2019t compute this directly because we don\u2019t know the true distribution \\(\\tau\\), only samples from it (the dataset).</li> </ul>"},{"location":"#why-dont-we-use-empirical-risk-to-evaluate-our-model-performance","title":"Why don\u2019t we use Empirical Risk to evaluate our model performance?","text":"<p>This is because Empirical (Training) Risk is typically a biased (optimistic) estimator of the True Risk as it underestimates how well the model will perform on unseen data.</p> <p>Proof</p> <p>Let \\(f_{\\mathcal{H}}\\) be the best-in-class predictor that minimizes the true risk, i.e.,</p> \\[ f_{\\mathcal{H}} = \\arg\\min_{f \\in \\mathcal{H}} R(f) = \\arg\\min_{f \\in \\mathcal{H}} \\; \\mathbb{E}_{(x, y)\\sim \\tau} \\left[ (f(x) - y)^2 \\right] \\] <p>Let \\(\\hat{f}\\) be the best-in-class predictor that minimizes the empirical risk, i.e.,</p> \\[ \\hat{f} = \\arg\\min_{f \\in \\mathcal{H}} R_{\\text{train}}(f) = \\arg\\min_{f \\in \\mathcal{H}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left( f(x_i) - y_i \\right)^2 \\] <p>By definition of \\(\\hat{f}\\), we have:</p> \\[ R_{\\text{train}}(\\hat{f}) \\leq R_{\\text{train}}(f), \\quad \\forall f \\in \\mathcal{H} \\] <p>Taking the expectation over training samples (drawn from distribution \\(\\tau\\)):</p> \\[ \\mathbb{E}_{\\mathcal{D} \\sim \\tau} \\left[ R_{\\text{train}}(\\hat{f}) \\right] \\leq \\mathbb{E}_{\\mathcal{D} \\sim \\tau} \\left[ R_{\\text{train}}(f) \\right], \\quad \\forall f \\in \\mathcal{H} \\] <p>However, we can go further to have</p> \\[ \\mathbb{E} \\left[ R_{\\text{train}}(\\hat{f}) \\right] &lt; \\mathbb{E} \\left[ R(\\hat{f}) \\right] \\] <p>That is, the empirical risk of \\(\\hat{f}\\) evaluated on the same data it was trained on is generally less than the true/generalization risk of \\(\\hat{f}\\) on unseen data. Therefore:</p> \\[ R_{\\text{train}}(\\hat{f}) \\text{ is a biased estimate of } R(\\hat{f}) \\]"},{"location":"#since-we-have-seen-that-empirical-risk-is-a-biased-estimate-of-the-true-risk-we-introduce-the-concept-of-the-test-dataset-to-evaluate-our-models-generalization-performance","title":"Since we have seen that Empirical Risk is a biased estimate of the True Risk, we introduce the concept of the test dataset to evaluate our model\u2019s generalization performance.","text":"<p>Given a test dataset:</p> \\[ \\mathcal{D}_{\\text{test}} = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{k} \\] <p>Then the empirical test risk is defined as:</p> \\[ R_{\\text{test}} = \\frac{1}{k} \\sum_{i=1}^{k} \\left( f(x_i) - y_i \\right)^2 \\] <p>Proof:</p> \\[ \\begin{align*} \\mathbb{E} \\left[ R_{\\text{test}} \\right] &amp;= \\mathbb{E} \\left[ \\frac{1}{k} \\sum_{i=1}^{k} \\left( f(x_i) - y_i \\right)^2 \\right] \\\\                                          &amp;= \\frac{1}{k} \\sum_{i=1}^{k} \\mathbb{E} \\left[ \\left( f(x_i) - y_i \\right)^2 \\right] \\\\                                          &amp;= \\frac{1}{k} \\sum_{i=1}^{k} R \\\\                                          &amp;= \\frac{1}{k} \\cdot k \\cdot R \\\\                                          &amp;= R \\end{align*} \\] <p>Therefore, $$ \\mathbb{E} \\left[ R_{\\text{test}} \\right] = R $$</p> <p>This shows that the empirical test risk is an unbiased estimate of the true risk, assuming the test data is sampled independently from the same distribution \\(\\tau\\) as the training data and is not used during training.</p> <p>Empirical Risk should not be used as a performance metric because it\u2019s biased. It doesn\u2019t reflect how well the model will generalize to new, unseen data \u2014 for that, we must estimate the true risk, typically using a validation or test set.</p>"},{"location":"#k-fold-cross-validation","title":"K-Fold Cross Validation","text":"<p>K-Fold Cross Validation is a model evaluation and selection technique used to estimate how well a model generalizes to unseen data. It is also used to tune hyperparameters or select the best hypothesis \\(f \\in \\mathcal{H}\\).</p> <p>Given a dataset:</p> \\[ \\mathcal{D} = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{n} \\] <p>And a learning algorithm that finds:</p> \\[ \\hat{f} = \\arg\\min_{f \\in \\mathcal{H}} \\; \\frac{1}{n} \\sum_{i=1}^{n} \\left( f(x_i) - y_i \\right)^2 \\] <p>We partition the dataset into \\(k\\) folds:</p> \\[ \\mathcal{D} = \\{ \\mathcal{D}_i \\}_{i=1}^k \\] <p>Where:</p> <ul> <li>\\(\\mathcal{D}_i = \\{ (x_j, y_j) \\}_{j=1}^{n/k}\\)</li> <li>\\(\\bigcup_{i = 1}^{k} \\mathcal{D}_i = \\mathcal{D}\\)</li> <li>\\(\\mathcal{D}_i \\cap \\mathcal{D}_j = \\emptyset \\quad \\forall i \\neq j\\)</li> </ul>"},{"location":"#how-k-fold-cross-validation-works","title":"How K-Fold Cross Validation Works","text":"<ol> <li> <p>Split the dataset \\(\\mathcal{D}\\) into \\(k\\) approximately equal-sized folds:    $$    \\mathcal{D}_1, \\mathcal{D}_2, \\dots, \\mathcal{D}_k    $$</p> </li> <li> <p>For each fold \\(i \\in \\{1, 2, \\dots, k\\}\\):</p> <ul> <li>Training set: \\(\\mathcal{D}_{\\text{train}} = \\mathcal{D} \\setminus \\mathcal{D}_i\\)</li> <li>Validation set: \\(\\mathcal{D}_i\\)</li> <li>Train the model \\(f_i\\) on \\(\\mathcal{D}_{\\text{train}}\\)</li> <li> <p>Evaluate the validation loss on \\(\\mathcal{D}_i\\):</p> <p>\\(  L^{(i)} = \\frac{1}{|\\mathcal{D}_i|} \\sum_{(x, y) \\in \\mathcal{D}_i} \\mathcal{l}(f_i(x), y)  \\)</p> </li> </ul> </li> <li> <p>Average the validation losses across all folds:</p> <p>\\(   \\hat{L}_{\\text{cv}} = \\frac{1}{k} \\sum_{i=1}^{k} L^{(i)}   \\)</p> </li> </ol>"},{"location":"#how-cross-validation-helps-select-better-f","title":"How Cross Validation Helps Select Better \\(f\\):","text":"<ol> <li>Provides a more reliable estimate of generalization performance on unseen data.</li> <li>Allows for fair comparison of different models or hyperparameters (e.g., regularization terms).</li> <li>Helps select the best model \\(f^*\\) that minimizes the cross-validation loss:</li> </ol> <p>$$    f^* = \\arg\\min_{f \\in \\mathcal{H}} \\; \\hat{L}_{\\text{cv}}(f)    $$</p> <p>You can find other cross validation techniques explanations here: </p> <ul> <li> <p>Top Cross-Validation Techniques with Python Code</p> </li> <li> <p>Cross Validation Explained \u2014 Leave One Out, K Fold, Stratified, and Time Series Cross Validation Techniques</p> </li> </ul>"},{"location":"#bias-variance-decomposition","title":"Bias-Variance Decomposition","text":"<p>Given a dataset \\(\\mathcal{D} = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{k}\\)</p> <p>Let,</p> \\[ y = f(x)+\\epsilon \\] <p>where \\(\\epsilon\\) is the noise in the dataset and it\u2019s independent from \\(x_i's\\). Also we assume the noise \\(\\epsilon\\) is from a Gaussian distrubtion with mean zero and variance \\(\\sigma^2\\) (ie \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)).</p> <p>Given \\(\\hat{f}\\) then the True Risk is given as;</p> \\[ \\begin{align*} \\mathbb{E}\\left[ (y - \\hat{f}(x))^2\\right] &amp;= \\mathbb{E}\\left[ (f(x)+\\epsilon - \\hat{f}(x))^2\\right] \\quad \\text{ since } y = f(x)+\\epsilon\\\\ &amp;=\\mathbb{E}\\left[ (f(x)-\\hat{f}(x))^2+2(f(x)-\\hat{f}(x))\\epsilon + \\epsilon^2\\right]\\\\ &amp;=\\mathbb{E}\\left[(f(x)-\\hat{f}(x))^2\\right] +2\\mathbb{E}\\left[(f(x)-\\hat{f}(x))\\right]\\mathbb{E}\\left[\\epsilon\\right] + \\mathbb{E}\\left[\\epsilon^2\\right] \\quad \\text{ but } \\mathbb{E}\\left[\\epsilon^2\\right] = \\sigma^2 \\text{ and } \\mathbb{E}\\left[\\epsilon\\right]=0\\\\ &amp;=\\mathbb{E}\\left[(f(x)-\\hat{f}(x))^2\\right] + \\sigma^2  \\end{align*} \\] <p>Now lets solve \\(\\mathbb{E}\\left[(f(x)-\\hat{f}(x))^2\\right]\\). Let \\(\\bar{f}\\) be the average predictor which is given as  \\(\\bar{f} = \\mathbb{E}\\left[\\hat{f}(x)\\right]\\). Then we will have,</p> \\[ \\begin{align*}  \\mathbb{E}\\left[(f(x)-\\hat{f}(x))^2\\right] &amp;= \\mathbb{E}\\left[(f(x)-\\bar{f}(x)+\\bar{f}(x)-\\hat{f}(x))^2\\right]\\\\  &amp;= \\mathbb{E}\\left[(f(x)-\\bar{f}(x))^2+2(f(x)-\\bar{f}(x))(\\bar{f}(x)-\\hat{f}(x))+(\\bar{f}(x)-\\hat{f}(x))^2\\right]\\\\  &amp;= \\mathbb{E}\\left[(f(x)-\\bar{f}(x))^2\\right] + 2\\mathbb{E}\\left[f(x)-\\bar{f}(x)\\right]\\mathbb{E}\\left[\\bar{f}(x)-\\hat{f}(x)\\right]+\\mathbb{E}\\left[(\\bar{f}(x)-\\hat{f}(x))^2\\right]\\\\  &amp;= \\mathbb{E}\\left[(f(x)-\\bar{f}(x))^2\\right] +\\mathbb{E}\\left[(\\bar{f}(x)-\\hat{f}(x))^2\\right] \\quad \\text{ since } \\mathbb{E}\\left[\\bar{f}(x)-\\hat{f}(x)\\right] = 0 \\text{ as } \\bar{f}(x) = \\mathbb{E}\\left[\\hat{f}(x)\\right] \\end{align*} \\] <p>Therefore our final equestion can be expressed as;</p> \\[ \\begin{align*}  \\mathbb{E}\\left[ (y - \\hat{f}(x))^2\\right] &amp;= \\mathbb{E}\\left[(f(x)-\\bar{f}(x))^2\\right] +\\mathbb{E}\\left[(\\bar{f}(x)-\\hat{f}(x))^2\\right] + \\sigma^2\\\\  &amp;= \\text{Bias}^2 + \\text{Variance} + \\text{Noise} \\end{align*} \\] <p>where;</p> \\[ \\mathbb{E}_{\\mathcal{D}, \\epsilon}[(y - \\hat{f}(x))^2] = \\underbrace{(f(x) - \\bar{f}(x))^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_{\\mathcal{D}}[(\\hat{f}(x) - \\bar{f}(x))^2]}_{\\text{Variance}} + \\underbrace{\\sigma^2}_{\\text{Noise}} \\] <p>Thus the Bias Variance threshold is given as;</p> \\[ \\text{True Risk} = \\text{Bias}^2 + \\text{Variance} + \\text{Noise} \\] High Bias High Variance - Underfitting (high training and test error) - Overfitting (low training error, high test error) - Model has not learned enough - Model is complex ** Train longer - Small training data - Model is too simple ** Feature selection. 1, 2, 3 ** Increase the features ** Dimesionality Reduction ** Increase the features ** Data Augmentation <p>The above table compares high bias (underfitting) and high variance (overfitting). Entries marked with <code>**</code> indicate common solutions to address each issue.</p>"},{"location":"#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>Here we are going to use the concept of MLE to find negative log-likelihood and also come up with some of the objective/loss functions.</p> <p>Given a dataset </p> \\[ \\begin{align*} &amp;\\mathcal{D} = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{n}\\\\ &amp;X = \\left\\{ x_i\\right\\}_{i=1}^{n}\\\\ &amp;Y = \\left\\{ y_i\\right\\}_{i=1}^{n}\\\\ \\end{align*} \\] <p>Let \\(h_{w} \\in \\mathcal{H}\\) i.e a function \\(h\\) with parameter \\(w\\) in hypothesis space \\(\\mathcal{H}\\). If we assuming the outputs are drawn independently from a distribution \\(P(y_i | x_i; w)\\),the likelihood of the data is:</p> \\[ \\begin{align*} \\mathcal{L}(w) &amp;= P(Y | X; w)\\\\                &amp;= \\prod_{i=1}^{n} P_i(y_i | x_i; w) \\quad \\text{ since they are independent}\\\\                &amp;= \\prod_{i=1}^{n} P(y_i | x_i; w) \\quad \\text{ since they are identically ditributed} \\end{align*} \\] <p>Since the product of probabilities tends to zero or are very small (i.e between 0 and 1) we take its \\(log\\) as \\(log\\) is an increasing function to have log-likelihood:</p> \\[ \\begin{align*} \\log \\mathcal{L}(w) &amp;= \\log \\prod_{i=1}^{n} P(y_i | x_i; w)\\\\                     &amp;= \\sum_{i=1}^{n} \\log P(y_i | x_i; w) \\end{align*} \\] <p>Then, the Maximum Likelihood Estimation (MLE) objective is:</p> \\[ \\begin{align*} w^* &amp;= \\arg\\max_w \\log \\mathcal{L}(w)\\\\     &amp;= \\arg\\max_w \\sum_{i=1}^{n} \\log P(y_i | x_i; w)\\\\     &amp;= \\arg\\min_w \\underbrace{- \\sum_{i=1}^{n} \\log P(y_i | x_i; w)}_{\\text{negative log-likelihood (NLL)}} \\end{align*} \\] <p>Or equivalently, we minimize the negative log-likelihood (NLL):</p> \\[ NLL(w) = -\\sum_{i=1}^{k} \\log P(y_i | x_i; w) \\]"},{"location":"#examples-mle-leading-to-common-loss-functions","title":"Examples: MLE Leading to Common Loss Functions","text":""},{"location":"#1-mean-squared-error-mse","title":"1. Mean Squared Error (MSE)","text":"<p>Given a dataset:</p> \\[ \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n \\] <p>Assume that the target variable \\(y_i\\) is generated as:</p> \\[ y_i = w^\\top x_i + \\epsilon_i \\] <p>where \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\). So the conditional probability of \\(y_i\\) given \\(x_i\\) is:</p> \\[ P(y_i \\mid x_i; w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - w^\\top x_i)^2}{2\\sigma^2} \\right) \\] <p>Assuming i.i.d. data:</p> \\[ \\mathcal{L}(w) = \\prod_{i=1}^{n} P(y_i \\mid x_i; w) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - w^\\top x_i)^2}{2\\sigma^2} \\right) \\] <p>Take the log:</p> \\[ \\begin{align*} \\log \\mathcal{L}(w) &amp;= \\sum_{i=1}^{n} \\log \\left[ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - w^\\top x_i)^2}{2\\sigma^2} \\right) \\right]\\\\                     &amp;= \\sum_{i=1}^{n} \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(y_i - w^\\top x_i)^2}{2\\sigma^2} \\right] \\end{align*} \\] <p>We now minimize the negative log-likelihood:</p> \\[ \\begin{align*} w^*  &amp;= \\arg\\min_w - \\log \\mathcal{L}(w)\\\\      &amp;= \\arg\\min_w - \\sum_{i=1}^{n} \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(y_i - w^\\top x_i)^2}{2\\sigma^2} \\right]\\\\      &amp;= \\arg\\min_w \\frac{n}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - w^\\top x_i)^2\\\\      &amp;= \\arg\\min_w \\sum_{i=1}^{n}(y_i - w^\\top x_i)^2 \\quad \\text{ since they dont depend on } w ,\\frac{n}{2} \\log(2\\pi\\sigma^2),\\frac{1}{2\\sigma^2} \\end{align*} \\] <p>Since \\(n\\log(2\\pi\\sigma^2)\\) and \\(\\frac{1}{2\\sigma^2}\\) are constants (do not depend on \\(w\\)), minimizing \\(\\mathcal{L}_{\\text{NLL}}(w)\\) is equivalent to minimizing:</p> \\[ \\sum_{i=1}^{n} (y_i - w^\\top x_i)^2 \\] <p>Thus, minimizing the Mean Squared Error (MSE):</p> \\[ \\text{MSE}(w) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - w^\\top x_i)^2 \\] <p>is equivalent to maximum likelihood estimation under the assumption of Gaussian noise in the regression model.</p>"},{"location":"#2-binary-cross-entropy","title":"2. Binary cross Entropy","text":"<p>Given a dataset:</p> \\[ \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n \\] <p>Assume that the target variable \\(y_i\\) is generated from a Bernoulli distribution i.e</p> \\[ P(y_i|x_i;w) = \\hat{y_i}^{y_i}(1-\\hat{y_i})^{1-y_i} \\quad \\text{ where } y_i \\in \\{0,1\\}, 0\\leq \\hat{y_i}\\leq 1 \\] <p>Since \\(\\hat{y_i}\\) are probalities we can use sigmoid function to generate them;</p> \\[ \\hat{y_i} = \\frac{1}{1+e^{w^\\top x_i}} \\] <p>Assuming i.i.d. data:</p> \\[ \\mathcal{L}(w) = \\prod_{i=1}^{n} P(y_i \\mid x_i; w) = \\prod_{i=1}^{n} \\hat{y_i}^{y_i}(1-\\hat{y_i})^{1-y_i} \\] <p>Take the log:</p> \\[ \\begin{align*} \\log \\mathcal{L}(w) &amp;= \\sum_{i=1}^{n} \\log \\left[ \\prod_{i=1}^{n} \\hat{y_i}^{y_i}(1-\\hat{y_i})^{1-y_i} \\right]\\\\                     &amp;= \\sum_{i=1}^{n} \\log \\left[ \\hat{y_i}^{y_i}(1-\\hat{y_i})^{1-y_i} \\right]\\\\                     &amp;= \\sum_{i=1}^{n} \\left[ y_i\\log \\hat{y_i} +(1-y_i)\\log (1-\\hat{y_i}) \\right] \\end{align*} \\] <p>We now minimize the negative log-likelihood:</p> \\[ \\begin{align*} w^*  &amp;= \\arg\\min_w - \\log \\mathcal{L}(w)\\\\      &amp;= \\arg\\min_w - \\sum_{i=1}^{n} \\left[ y_i\\log \\hat{y_i} +(1-y_i)\\log (1-\\hat{y_i}) \\right] \\end{align*} \\] <p>Thus the resulting loss function is called the Binary Cross-Entropy given as:</p> \\[ BCE(w) = - \\sum_{i=1}^{n} \\left[ y_i\\log \\hat{y_i} +(1-y_i)\\log (1-\\hat{y_i}) \\right] \\] <p>So minimizing binary cross-entropy is equivalent to maximum likelihood estimation under the assumption that labels are drawn from a Bernoulli distribution with probability given by \\(\\hat{y} = \\frac{1}{1+e^{w^\\top x}}\\)</p> <p>NOTE:</p> <p>In Maximum Likelihood Estimation (MLE), to derive the negative log-likelihood, we assume that we know the distribution of the outputs \\(y_i\\) given the inputs \\(x_i\\) and model parameters \\(w\\); that is, we assume a probabilistic model of the form:</p> \\[ y_i \\sim P(y_i \\mid x_i; w) \\] <p>where \\(w\\) represents the model parameters.</p> <p>Now, we introduce another concept known as Maximum A Posteriori (MAP) Estimation.</p>"},{"location":"#maximum-a-posteriori-map-estimation","title":"Maximum A Posteriori (MAP) Estimation","text":"<p>Given a dataset:</p> \\[ \\begin{aligned} \\mathcal{D} &amp;= \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{n} \\\\ X &amp;= \\left\\{ x_i \\right\\}_{i=1}^{n} \\\\ Y &amp;= \\left\\{ y_i \\right\\}_{i=1}^{n} \\end{aligned} \\] <p>Let \\(h_w \\in \\mathcal{H}\\), i.e., a function \\(h\\) parameterized by \\(w\\), in hypothesis space \\(\\mathcal{H}\\).</p> <p>In Maximum A Posteriori (MAP) Estimation, we assume we have prior knowledge about the distribution of the parameters \\(w\\). Using Bayes\u2019 Rule:</p> \\[ P(w \\mid \\mathcal{D}) = \\frac{P(\\mathcal{D} \\mid w) \\cdot P(w)}{P(\\mathcal{D})} \\] <p>Where:</p> <ul> <li>\\(P(w \\mid \\mathcal{D})\\): Posterior \u2014 what MAP tries to maximize</li> <li>\\(P(\\mathcal{D} \\mid w)\\): Likelihood \u2014 same as in MLE</li> <li>\\(P(w)\\): Prior over parameters</li> <li>\\(P(\\mathcal{D})\\): Marginal likelihood (a constant with respect to \\(w\\))</li> </ul> <p>The MAP estimate maximizes the posterior:</p> \\[ \\begin{aligned} w_{\\text{MAP}} &amp;= \\arg\\max_w P(w \\mid \\mathcal{D}) \\\\                &amp;= \\arg\\max_w \\frac{P(\\mathcal{D} \\mid w) \\cdot P(w)}{P(\\mathcal{D})} \\\\                &amp;= \\arg\\max_w P(\\mathcal{D} \\mid w) \\cdot P(w) \\quad \\text{(since \\( P(\\mathcal{D}) \\) is constant)} \\end{aligned} \\] <p>Because probabilities are small and prone to numerical instability, we apply the log function (monotonic, so it preserves maxima):</p> \\[ \\begin{aligned} w_{\\text{MAP}} &amp;= \\arg\\max_w \\left[ \\log P(\\mathcal{D} \\mid w) + \\log P(w) \\right] \\\\                &amp;= \\arg\\max_w \\left[ \\underbrace{\\log P(\\mathcal{D} \\mid w)}_{\\text{log-likelihood}} + \\underbrace{\\log P(w)}_{\\text{log-prior}} \\right] \\end{aligned} \\] <p>In practice, we often minimize the negative log-posterior:</p> \\[ w_{\\text{MAP}} = \\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) - \\log P(w) \\right] \\] <ul> <li>The first term is the negative log-likelihood (same as MLE)</li> <li>The second term is the negative log-prior \u2014 acts as a regularization term</li> </ul> <p>MAP is like MLE plus regularization, where the regularization reflects our prior belief about the parameters.</p>"},{"location":"#example-1-map-with-a-gaussian-prior-l2-regularization","title":"Example 1: MAP with a Gaussian Prior (L2 regularization)","text":"<p>Assume </p> \\[ w \\sim \\mathcal{N}(0,\\sigma^2) \\] <p>Where \\(d\\) is the number of features and \\(n\\) is number of samples i.e \\(X \\in \\mathbb{R}^{n\\times d}\\) which implies that \\(w \\in \\mathbb{R}^d\\).</p> <p>Now our prior will be </p> \\[ \\begin{align*} P(w) &amp;= \\prod_{j=1}^{d} P(w_j)\\\\      &amp;= \\prod_{j=1}^{d} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{w_j^2}{2\\sigma^2} \\right) \\end{align*} \\] <p>Now solving log-prior,</p> \\[ \\begin{align*} \\log P(w) &amp;= \\log \\prod_{j=1}^{d} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{w_j^2}{2\\sigma^2} \\right)\\\\           &amp;= \\sum_{j=1}^{d} \\left[ \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}}  -\\frac{ w_j^2}{2\\sigma^2} \\right]\\\\           &amp;= d \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2\\sigma^2}\\sum_{j=1}^{d} w_j^2\\\\           &amp;= d \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2\\sigma^2} \\|w\\|_2^2  \\end{align*} \\] <p>Therefore we will have;</p> \\[ \\begin{align*} w_{\\text{MAP}} &amp;= \\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) - \\log P(w) \\right]\\\\                &amp;= \\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) -\\left(d \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2\\sigma^2} \\|w\\|_2^2\\right) \\right]\\\\                &amp;=\\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) -d \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\frac{1}{2\\sigma^2} \\|w\\|_2^2 \\right]\\\\                &amp;=\\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) + \\frac{1}{2\\sigma^2} \\|w\\|_2^2 \\right]\\\\                &amp;=\\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) + \\lambda \\|w\\|_2^2 \\right] \\quad \\text{ where } \\lambda = \\frac{1}{2\\sigma^2}  \\end{align*} \\] <p>This shows that a Gaussian prior on \\(w\\) leads to L2 regularization in the MAP objective which is the same as Ridge Regression.</p>"},{"location":"#example-2-map-with-a-laplace-prior-l1-regularization","title":"Example 2: MAP with a Laplace Prior (L1 regularization)","text":"<p>Assume the prior on each weight \\(w_j\\) is a Laplace distribution (double exponential distribution):</p> \\[ w_j \\sim \\text{Laplace}(0, b) \\] <p>Which implies</p> \\[ P(w_j) = \\frac{1}{2b} \\exp\\left(-\\frac{|w_j|}{b}\\right) \\] <p>Since weights are assumed to be iid, then the prior is:</p> \\[ P(w) = \\prod_{j=1}^{d} \\frac{1}{2b} \\exp\\left(-\\frac{|w_j|}{b}\\right) \\] <p>Now computing the log-prior:</p> \\[ \\begin{align*} \\log P(w) &amp;= \\log \\prod_{j=1}^{d} \\frac{1}{2b} \\exp\\left(-\\frac{|w_j|}{b}\\right) \\\\           &amp;= \\sum_{j=1}^{d} \\left[ \\log \\frac{1}{2b} - \\frac{|w_j|}{b} \\right] \\\\           &amp;= d \\log \\frac{1}{2b} - \\frac{1}{b} \\sum_{j=1}^{d} |w_j| \\\\           &amp;= d \\log \\frac{1}{2b} - \\frac{1}{b} \\|w\\|_1 \\end{align*} \\] <p>Apply Bayes\u2019 rule to get the MAP estimate:</p> \\[ \\begin{align*} w_{\\text{MAP}} &amp;= \\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) - \\log P(w) \\right] \\\\                &amp;= \\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) - d \\log \\frac{1}{2b} + \\frac{1}{b} \\|w\\|_1 \\right] \\\\                &amp;= \\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) + \\frac{1}{b} \\|w\\|_1 \\right] \\\\                &amp;= \\arg\\min_w \\left[ -\\log P(\\mathcal{D} \\mid w) + \\lambda \\|w\\|_1 \\right] \\quad \\text{ where } \\lambda = \\frac{1}{b} \\end{align*} \\] <p>This shows that a Laplace prior on \\(w\\) leads to L1 regularization in the MAP objective which is the same as Lasso Regression.</p>"},{"location":"#gradient-descent-from-first-order-taylor-approximation","title":"Gradient Descent from First-Order Taylor Approximation","text":"<p>The gradient of a function \\(f\\) at a point \\(w\\) yields the first-order Taylor approximation of \\(f\\) around \\(w\\), given as:</p> \\[ f(u) \\approx f(w) + \\langle u - w, \\nabla f(w) \\rangle \\] <p>When \\(f\\) is a convex function, this approximation is a lower bound of \\(f\\). That is:</p> \\[ f(u) \\geq f(w) + \\langle u - w, \\nabla f(w) \\rangle \\] <p>Therefore, for \\(w\\) close to \\(w^{(t)}\\), we can approximate \\(f(w)\\) as:</p> \\[ f(w) \\approx f(w^{(t)}) + \\langle w - w^{(t)}, \\nabla f(w^{(t)}) \\rangle \\] <p>We note that the above approximation might become loose if \\(w\\) is far away from \\(w^{(t)}\\). Therefore, we minimize jointly the distance between \\(w\\) and \\(w^{(t)}\\) and the approximation of \\(f\\) around \\(w^{(t)}\\). We also introduce a parameter \\(\\eta\\) which controls the trade-off between the two terms. This leads to the update rule as:</p> \\[ w^{(t+1)} = \\arg\\min_w \\left\\{ \\frac{1}{2} \\| w - w^{(t)} \\|^2 + \\eta \\left[ f(w^{(t)}) + \\langle w - w^{(t)}, \\nabla f(w^{(t)}) \\rangle \\right] \\right\\} \\] <p>Solving the optimization problem by taking the derivative with respect to \\(w\\) and setting it to zero yields the gradient descent update rule.</p> <p>Taking the objective function as \\(J(w)\\):</p> \\[ J(w) = \\frac{1}{2} \\| w - w^{(t)} \\|^2 + \\eta \\left[ f(w^{(t)}) + \\langle w - w^{(t)}, \\nabla f(w^{(t)}) \\rangle \\right] \\] <p>Now solving for \\(\\nabla_w J(w)\\)</p> \\[ \\begin{align*}    \\nabla_w J(w) &amp;= \\nabla_w \\left[ \\frac{1}{2} \\| w - w^{(t)} \\|^2 + \\eta \\left[ f(w^{(t)}) + \\langle w - w^{(t)}, \\nabla f(w^{(t)}) \\rangle \\right] \\right]\\\\     &amp;= \\nabla_w \\left[ \\frac{1}{2} (w - w^{(t)})^\\top(w - w^{(t)}) + \\eta f(w^{(t)}) + \\eta (w - w^{(t)}) ^\\top \\nabla f(w^{(t)}) \\right]\\\\     &amp;= \\nabla_w \\left[ \\frac{1}{2} (w^\\top w-w^\\top w^{(t)} -(w^{(t)})^\\top w +(w^{(t)})^\\top (w^{(t)})  )  + \\eta f(w^{(t)}) + \\eta( w^\\top \\nabla f(w^{(t)}) - (w^{(t)})^\\top \\nabla f(w^{(t)}) )  \\right]\\\\     &amp;=  \\frac{1}{2} (2w -  2w^{(t)}) + \\eta \\nabla f(w^{(t)})\\\\     &amp;=  (w -  w^{(t)}) + \\eta \\nabla f(w^{(t)}) \\end{align*} \\] <p>Next we set the gradient to zero i.e \\(\\nabla_w J(w) = 0\\)</p> \\[ \\begin{align*}    &amp; (w -  w^{(t)}) + \\eta \\nabla f(w^{(t)}) = 0\\\\    &amp;w =  w^{(t)} -  \\eta \\nabla f(w^{(t)}) \\end{align*} \\] <p>Therefor the gradient descent update rule is given as;</p> \\[ \\boxed{w^{(t+1)} = w^{(t)} - \\eta \\nabla f(w^{(t)})} \\]"},{"location":"#gradient-descent-gd-convergence","title":"Gradient Descent (GD) Convergence","text":"<p>We want to show that GD converges, i.e.,</p> \\[ f(\\bar{w}) - f(w^*) \\leq \\varepsilon \\] <p>after a sufficient number of steps \\(T\\), where:</p> <ul> <li>\\(f\\) is convex and \\(\\rho\\)-Lipschitz</li> <li>\\(\\bar{w} = \\frac{1}{T} \\sum_{t=1}^T w^{(t)}\\)</li> <li>\\(w^*\\) is the optimal point and \\(B\\) be an upper bound on \\(\\|w^*\\|\\) i.e \\(\\|w^*\\| \\leq B\\)</li> <li>\\(w^{(t+1)} = w^{(t)} - \\eta \\nabla f(w^{(t)})\\)</li> </ul> <p>Proof:</p> <p>From the definition of \\(\\bar{w}\\) and using Jensen\u2019s inequality we have;</p> \\[ f(\\bar{w}) - f(w^*) \\leq \\frac{1}{T} \\sum_{t=1}^T \\left[ f(w^{(t)}) - f(w^*) \\right] \\] <p>Due to convexity of \\(f\\) we have:</p> \\[ f(w^{(t)}) - f(w^*) \\leq \\langle w^{(t)} - w^*, \\nabla f(w^{(t)}) \\rangle \\] <p>So combining the above two inqualies we obtain:</p> \\[ f(\\bar{w}) - f(w^*) \\leq \\frac{1}{T} \\sum_{t=1}^T \\langle w^{(t)} - w^*, \\nabla f(w^{(t)}) \\rangle \\] <p>To bound the right-hand side of the above inequality we use this lemma: Given the update rule \\(w^{(t+1)} = w^{(t)} - \\eta \\nabla f(w^{(t)})\\), then it satisfies;</p> \\[ \\sum_{t=1}^T \\langle w^{(t)} - w^*, \\nabla f(w^{(t)}) \\rangle \\leq \\frac{B^2}{2\\eta} + \\frac{\\eta}{2} \\sum_{t=1}^T \\| \\nabla f(w^{(t)}) \\|^2 \\] <p>If \\(f\\) is \\(\\rho\\)-Lipschitz, then \\(\\| \\nabla f(w^{(t)}) \\| \\leq \\rho\\). Therefore:</p> \\[ \\sum_{t=1}^T \\langle w^{(t)} - w^*, \\nabla f(w^{(t)}) \\rangle \\leq \\frac{B^2}{2\\eta} + \\frac{\\eta T \\rho^2}{2} \\] <p>Now Choosing an optimal learning rate let say,</p> \\[ \\eta = \\frac{B}{\\rho \\sqrt{T}} \\] <p>Then:</p> \\[ \\sum_{t=1}^T \\langle w^{(t)} - w^*, \\nabla f(w^{(t)}) \\rangle \\leq B \\rho \\sqrt{T} \\] <p>Dividing by \\(T\\):</p> \\[ f(\\bar{w}) - f(w^*) \\leq \\frac{B \\rho}{\\sqrt{T}} \\] <p>If we run the GD algorithm on \\(f\\) for \\(T\\) steps with  \\(\\eta = \\frac{B}{\\rho \\sqrt{T}}\\), then we will have,</p> \\[     f(\\bar{w}) - f(w^*) \\leq \\frac{B \\rho}{\\sqrt{T}} \\] <p>To guarantee that the left-hand side is at most \\(\\varepsilon\\), we solve:</p> \\[ \\frac{B \\rho}{\\sqrt{T}} \\leq \\varepsilon \\quad \\Rightarrow \\quad \\sqrt{T} \\geq \\frac{B \\rho}{\\varepsilon} \\quad \\Rightarrow \\quad T \\geq \\frac{B^2 \\rho^2}{\\varepsilon^2} \\] <p>Thus, Gradient Descent converges to within error \\(\\varepsilon\\) after:</p> \\[ \\boxed{T \\geq \\frac{B^2 \\rho^2}{\\varepsilon^2}} \\] <p>iterations, when minimizing a convex and \\(\\rho\\)-Lipschitz function \\(f\\), under the assumption that the optimal solution satisfies \\(\\|w^*\\| \\leq B\\).</p>"},{"location":"#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>Stochastic Gradient Descent is an iterative optimization algorithm used to minimize a differentiable objective function. Unlike standard (batch) gradient descent, which computes the gradient using the entire dataset, SGD approximates the gradient using a single randomly chosen data point at each iteration.</p> <p>From above, we have seen that standard gradient descent has the following update rule:</p> \\[ w^{(t+1)} = w^{(t)} - \\eta \\nabla f(w^{(t)}) = w^{(t)} - \\eta \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(w^{(t)}) \\right) \\] <p>Here, $ f(w) = \\frac{1}{n} \\sum_{i=1}^n f_i(w) $, where each $ f_i(w) $ typically represents the loss on the $ i $-th training example.</p> <p>In Stochastic Gradient Descent, the full gradient $ \\nabla f(w^{(t)}) $ is approximated by the gradient on a single example:</p> \\[ w^{(t+1)} = w^{(t)} - \\eta \\nabla f_i(w^{(t)}) \\] <p>where $ i $ is chosen uniformly at random from $ {1, \\dots, n} $ at each iteration.</p> <p>This stochastic estimate is unbiased but introduces variance, which affects convergence stability:</p> \\[ \\mathbb{E}_i [\\nabla f_i(w)] = \\nabla f(w) \\] <p>Proof:</p> <p>Given that \\(i\\) is selected uniformly at random from \\(\\{1, 2, \\dots, n\\}\\), the probability of selecting any particular index \\(i\\) is \\(\\frac{1}{n}\\). Therefore, the expectation of the stochastic gradient is:</p> \\[ \\mathbb{E}_i[\\nabla f_i(w)] = \\sum_{i=1}^{n} \\frac{1}{n} \\nabla f_i(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(w) = \\nabla f(w) \\] <p>This confirms that the stochastic gradient estimator is unbiased.</p>"},{"location":"#mini-batch-stochastic-gradient-descent","title":"Mini-Batch Stochastic Gradient Descent","text":"<p>Mini-Batch SGD is an improvement between standard gradient descent and stochastic gradient descent (SGD). Instead of computing the gradient using the entire dataset or a single sample, it computes the gradient over a random subset (mini-batch) of data points at each iteration.</p> <p>Given \\(B_t \\subset \\{1, 2, \\dots, n\\}\\) be a randomly selected mini-batch of \\(m\\) examples at iteration \\(t\\). Then the update rule is given as:</p> \\[ w^{(t+1)} = w^{(t)} - \\eta \\cdot \\frac{1}{m} \\sum_{i \\in B_t} \\nabla f_i(w^{(t)}) \\] <p>Where:</p> <ul> <li>\\(m\\): mini-batch size</li> <li>\\(B_t\\): the mini-batch sampled uniformly without replacement</li> </ul> <p>We can also show that the Mini-Batch SGD  gradient estimate is unbiased i.e</p> \\[ \\mathbb{E}_{B_t} \\left[ g(w) \\right] = \\nabla f(w) \\quad \\text{ where } g(w) =  \\frac{1}{m} \\sum_{i \\in B_t} \\nabla f_i(w^{(t)}) \\] <p>Proof:</p> <p>We\u2019re taking the expected value of the mini-batch gradient over all possible random mini-batches \\(B_t\\) of size \\(m\\). Since the indices are sampled uniformly, each data point \\(i\\) has the same probability \\(\\frac{m}{n}\\) of being included in the mini-batch.</p> <p>Using linearity of expectation:</p> \\[ \\mathbb{E}_{B_t}\\left[ \\frac{1}{m} \\sum_{i \\in B_t} \\nabla f_i(w) \\right] = \\frac{1}{m} \\sum_{i=1}^{n} \\mathbb{P}(i \\in B_t) \\cdot \\nabla f_i(w) \\] <p>Since:</p> \\[ \\mathbb{P}(i \\in B_t) = \\frac{m}{n} \\] <p>We substitute:</p> \\[ \\frac{1}{m} \\sum_{i=1}^{n} \\frac{m}{n} \\nabla f_i(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(w) = \\nabla f(w) \\] <p>Therefore:</p> \\[ \\mathbb{E}_{B_t}[g(w)] = \\nabla f(w) \\] <p>We can also note that Mini-Batch SGD  has a lower variance than the SGD due to averaging over multiple samples.</p>"},{"location":"#stochastic-gradient-descent-sgd-with-momentum","title":"Stochastic Gradient Descent (SGD) with Momentum","text":"<p>SGD with momentum is used to accelerate convergence and to smoothen updates, especially in directions of consistent gradients, by adding a \u201cmemory\u201d of past gradients.</p> <p>In SGD with Momentum we have the following update Rule:</p> <p>We introduce a velocity vector \\(v^{(t)}\\), which accumulates an exponentially decaying moving average of past gradients.</p> \\[ \\begin{aligned} v^{(t+1)} &amp;= \\beta v^{(t)} + \\nabla f_i(w^{(t)}) \\\\ w^{(t+1)} &amp;= w^{(t)} - \\eta v^{(t+1)} \\end{aligned} \\] <p>Where:</p> <ul> <li>\\(\\beta \\in [0,1)\\) is the momentum coefficient,</li> <li>\\(v^{(0)} = 0\\),</li> <li>\\(\\nabla f_i(w^{(t)})\\) is a stochastic gradient sampled at iteration \\(t\\).</li> </ul> <p>We can see that the velocity term is a weighted sum of previous gradients given as:</p> \\[ v^{(t)} = \\sum_{k=0}^{t-1} \\beta^k \\nabla f_{i_{t-k}}(w^{(t-k)}) \\] <p>This shows that recent gradients have more weight (since \\(\\beta^k\\) decays with \\(k\\)).</p>"},{"location":"#neural-network","title":"Neural Network","text":"<p>A neural network is a machine learning model inspired by the human brain that maps input data to output predictions by learning weights through layers of interconnected nodes (neurons). Neural networks are capable of modeling complex non-linear decision boundaries by composing multiple linear transformations and non-linear activation functions.</p> <p>A feedforward neural network is made up of:</p> <ul> <li>An input layer that takes the features.</li> <li>One or more hidden layers that learn intermediate representations.</li> <li>An output layer that produces predictions.</li> </ul> <p>Each layer applies a linear transformation followed by a non-linear activation function.</p>"},{"location":"#data-representation","title":"Data Representation","text":"<p>Let:</p> <ul> <li>\\(n\\): number of training examples</li> <li>\\(d\\): number of features</li> <li>\\(X \\in \\mathbb{R}^{d \\times n}\\): input matrix</li> <li>\\(y \\in \\mathbb{R}^{1 \\times n}\\): target vector for regression or \\(y \\in {0, 1}^{1\\times n}\\) for binary classification</li> </ul> <p>For a 2-layer neural network (i.e., one hidden layer), we define:</p> <ul> <li>\\(W^{[1]} \\in \\mathbb{R}^{h \\times d}\\): weights for hidden layer</li> <li>\\(b^{[1]} \\in \\mathbb{R}^{h \\times 1}\\): bias for hidden layer</li> <li>\\(W^{[2]} \\in \\mathbb{R}^{1 \\times h}\\): weights for output layer</li> <li>\\(b^{[2]} \\in \\mathbb{R}\\): bias for output layer</li> </ul>"},{"location":"#neural-network-architecture","title":"Neural Network Architecture","text":"<p>Here we are going to use an example of a 2-layer feedforward neural network with:</p> <ul> <li>Input layer (\\(d\\) neurons)</li> <li>Hidden layer (\\(h_1\\) neurons)</li> <li>Output layer (\\(h_2\\) neurons)</li> </ul> <p>Assuming our task is binary clasification and we use \\(\\sigma\\) (sigmoid) activation function on both layers.</p> <p>Now let\u2019s see how our data and parameters are represented.</p> <ul> <li> <p>Data: </p> <p>Features: \\(X \\in \\mathbb{R}^{d\\times n}\\) which is the transpose of our original input data.</p> <p>Targets: \\(Y \\in \\mathbb{R}^{h_2\\times n}\\) which is the transpose of our original target data.</p> </li> <li> <p>Layer 1:</p> <p>Weight: \\(W_1 \\in \\mathbb{R}^{h_1\\times d}\\)</p> <p>Bias: \\(b_1 \\in \\mathbb{R}^{h_1\\times 1}\\)</p> </li> <li> <p>Layer 2:</p> <p>Weight: \\(W_2 \\in \\mathbb{R}^{h_2\\times h_1}\\)</p> <p>Bias: \\(b_2 \\in \\mathbb{R}^{h_2\\times 1}\\)</p> </li> <li> <p>Loss Function: Since this is a binary classification problem, we use binary cross entrpy. $$ \\mathcal{L} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$</p> </li> </ul>"},{"location":"#forward-propagation","title":"Forward Propagation","text":"<p>The Feedforward Propagation, also called Forward Pass, is the process consisting of computing all network nodes\u2019 output values, starting with the first hidden layer until the last output layer, using at start either a subset or the entire dataset samples.</p> <p>Layer 1</p> <ul> <li>linear transformation  $$ Z_1 = W_1X+b_1 \\quad \\text{where } Z_1 \\in \\mathbb{R}^{h_1\\times n} $$</li> <li>non-linear transformation with \\(\\sigma\\) activation function $$ A_1 = \\sigma (Z_1) \\quad \\text{where } A_1 \\in \\mathbb{R}^{h_1\\times n} $$</li> <li>Number of parameters in layer 1 is given as: \\((h_1 \\times d) + h_1 = h_1(d + 1)\\).</li> </ul> <p>Layer 2</p> <ul> <li>linear transformation </li> </ul> \\[ Z_2 = W_2A_1+b_2 \\quad \\text{where } Z_2 \\in \\mathbb{R}^{h_2\\times n} \\] <ul> <li>non-linear transformation with \\(\\sigma\\) activation function</li> </ul> \\[ A_2 = \\sigma (Z_2) \\quad \\text{where } A_2 \\in \\mathbb{R}^{h_2\\times n} \\] <ul> <li>Number of parameters in layer 2 is given as: \\((h_2 \\times h_1) + h_2 = h_2(h_1+1)\\).</li> </ul> <p>Output Layer</p> <ul> <li>Output</li> </ul> <p>$$    A_2 = \\hat{Y} \\in \\mathbb{R}^{h_2\\times n}    $$</p>"},{"location":"#loss-function","title":"Loss Function","text":"<p>For binary classification, we use binary cross-entropy loss:</p> \\[ \\mathcal{L} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\] <p>Matrix form:</p> \\[ \\mathcal{L} = - \\frac{1}{n} \\left[ \\mathcal{Y} \\log(A_2) + (1 - \\mathcal{Y}) \\log(1 - A_2) \\right] \\]"},{"location":"#backward-propagation","title":"Backward Propagation","text":"<p>Backpropagation, or backward propagation of errors, is an algorithm working from the output nodes to the input nodes of a neural network using the chain rule to compute how much each activation unit contributed to the overall error.</p> <p>Backpropagation automatically computes error gradients to then repeatedly adjust all weights and biases to reduce the overall error.</p> <p>From our example our aim is to find </p> \\[ \\displaystyle \\frac{\\partial L}{\\partial A_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial Z_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial W_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial b_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial A_1}, \\quad \\displaystyle \\frac{\\partial L}{\\partial Z_1}, \\quad \\displaystyle \\frac{\\partial L}{\\partial W_1} \\quad \\text{ and } \\displaystyle \\frac{\\partial L}{\\partial b_1} \\] <p>Note that we are using our loss and binary cross entropy. But since we want to find its partial derivative w.r.t \\(A_2\\) we will modify it to be</p> \\[ L =  - \\frac{1}{n} \\left[ Y \\log(A_2) + (1 - Y) \\log(1 - A_2) \\right] \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial A_2}\\)</li> </ul> \\[ \\displaystyle \\frac{\\partial L}{\\partial A_2} =  \\frac{1}{n} \\left[\\frac{A_2-Y}{A_2(1-A_2)}\\right] \\in \\mathbb{R}^{h_2 \\times n} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial Z_2} = \\displaystyle \\frac{\\partial L}{\\partial A_2} \\times \\displaystyle \\frac{\\partial A_2}{\\partial Z_2}\\)</li> </ul> \\[  \\displaystyle \\frac{\\partial A_2}{\\partial Z_2} = \\sigma(Z_2)(1-\\sigma(Z_2))= A_2(1-A_2) \\in \\mathbb{R}^{h_2 \\times n} \\quad \\text{ since }\\sigma(Z_2) = A_2\\\\ \\] <p>Therefore we have,</p> \\[ \\begin{align*} \\displaystyle \\frac{\\partial L}{\\partial Z_2} &amp;= \\displaystyle \\frac{\\partial L}{\\partial A_2} \\times \\displaystyle \\frac{\\partial A_2}{\\partial Z_2}\\\\                                               &amp;=  \\frac{1}{n} \\left[\\frac{A_2-Y}{A_2(1-A_2)}\\right] \\times A_2(1-A_2)\\\\                                               &amp;= \\frac{1}{n} \\left[A_2-Y\\right] \\in \\mathbb{R}^{h_2\\times n} \\end{align*} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial W_2} = \\displaystyle \\frac{\\partial L}{\\partial Z_2} \\times \\displaystyle \\frac{\\partial Z_2}{\\partial W_2}\\)</li> </ul> \\[  \\frac{\\partial Z_2}{\\partial W_2} = A_1^\\top \\in \\mathbb{R}^{h_2\\times h_1} \\] <p>Therefore we have,</p> \\[ \\displaystyle \\frac{\\partial L}{\\partial W_2}  = \\frac{1}{n} \\left[A_2-Y\\right]A_1^\\top \\in \\mathbb{R}^{h_2 \\times h_1} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial b_2} = \\displaystyle \\frac{\\partial L}{\\partial Z_2} \\times \\displaystyle \\frac{\\partial Z_2}{\\partial b_2}\\)</li> </ul> \\[  \\frac{\\partial Z_2}{\\partial b_2} = I \\quad \\text{Identity} \\] <p>Therefore we have,</p> \\[ \\displaystyle \\frac{\\partial L}{\\partial b_2}  = \\frac{1}{n} \\left[A_2-Y\\right] \\in \\mathbb{R}^{h_2 \\times n} \\quad \\text{ but } \\displaystyle \\frac{\\partial L}{\\partial b_2} \\in \\mathbb{R}^{h_2 \\times 1} \\quad \\text{So, we will sum over the second dimension.} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial A_1} = \\displaystyle \\frac{\\partial L}{\\partial Z_2} \\times \\displaystyle \\frac{\\partial Z_2}{\\partial A_1}\\)</li> </ul> \\[  \\displaystyle \\frac{\\partial Z_2}{\\partial A_1} =  W_2^\\top  \\in \\mathbb{R}^{h_1\\times h_2} \\] <p>Therefore we have,</p> \\[ \\displaystyle \\frac{\\partial L}{\\partial A_1} =  \\frac{1}{n} \\left[A_2-Y\\right] W_2^\\top \\in \\mathbb{R}^{h_1\\times n} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial Z_1} = \\displaystyle \\frac{\\partial L}{\\partial A_1} \\times \\displaystyle \\frac{\\partial A_1}{\\partial Z_1}\\)</li> </ul> \\[  \\displaystyle \\frac{\\partial A_1}{\\partial Z_1} = \\underbrace{\\sigma(Z_1)(1-\\sigma(Z_1))}_{\\text{element-wise multip.}} \\in \\mathbb{R}^{h_1\\times n} \\] <p>Therefore we have,</p> \\[ \\begin{align*} \\displaystyle \\frac{\\partial L}{\\partial Z_1} &amp;=  \\frac{1}{n} \\left[A_2-Y\\right] W_2^\\top (\\sigma(Z_1)(1-\\sigma(Z_1))) \\quad \\text{ Due to dimentionality we change to}\\\\                                               &amp;=   \\frac{1}{n} \\underbrace{\\underbrace{W_2^\\top \\left[A_2-Y\\right]}_{\\text{matrix multip.}} (\\sigma(Z_1)(1-\\sigma(Z_1)))}_{\\text{element-wise multip.}} \\in \\mathbb{R}^{h_1\\times n} \\end{align*} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial W_1} = \\displaystyle \\frac{\\partial L}{\\partial Z_1} \\times \\displaystyle \\frac{\\partial Z_1}{\\partial W_1}\\)</li> </ul> \\[ \\displaystyle \\frac{\\partial Z_1}{\\partial W_1} =  X^\\top \\in \\mathbb{R}^{n\\times d} \\] <p>Therefore we have,</p> \\[ \\displaystyle \\frac{\\partial L}{\\partial W_1} = \\frac{1}{n} W_2^\\top \\left[A_2-Y\\right] (\\sigma(Z_1)(1-\\sigma(Z_1))) X^\\top \\in \\mathbb{R}^{h_1\\times d} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial b_1} = \\displaystyle \\frac{\\partial L}{\\partial Z_1} \\times \\displaystyle \\frac{\\partial Z_1}{\\partial b_1}\\)</li> </ul> \\[ \\displaystyle \\frac{\\partial Z_1}{\\partial b_1} =  I \\quad \\text{Identity} \\] <p>Therefore we have,</p> \\[ \\displaystyle \\frac{\\partial L}{\\partial b_1} = \\frac{1}{n} W_2^\\top \\left[A_2-Y\\right] (\\sigma(Z_1)(1-\\sigma(Z_1))) \\in \\mathbb{R}^{h_1\\times n} \\quad \\text{So, we will sum over the second dimension to get } \\mathbb{R}^{h_1\\times 1} \\]"},{"location":"#gradient-descent-and-optimization","title":"Gradient Descent and Optimization","text":"<p>Using gradient descent, we update parameters in the direction that reduces the loss.</p> <p>Let \\(\\eta\\) be the learning rate. The update rules:</p> <ul> <li>\\(W_1 \\leftarrow W_1 - \\eta \\cdot \\frac{\\partial L}{\\partial W_1}\\)</li> <li>\\(b_1 \\leftarrow b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\\)</li> <li>\\(W_2 \\leftarrow W_2 - \\eta \\cdot \\frac{\\partial L}{\\partial W_2}\\)</li> <li>\\(b_2 \\leftarrow b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\\)</li> </ul> <p>Repeat this process for multiple epochs until the loss converges.</p>"},{"location":"#vanishing-and-exploding-gradient-problems-in-deep-neural-networks","title":"Vanishing and Exploding Gradient Problems in Deep Neural Networks","text":"<p>In a deep neural network, at layer \\(\\mathcal{l}\\), we define the pre-activation and activation as follows:</p> \\[ h^{\\mathcal{l}} = \\phi(Z^{\\mathcal{l}}), \\quad \\text{where} \\quad Z^{\\mathcal{l}} = W^{\\mathcal{l}} h^{\\mathcal{l}-1} \\] <p>Here, \\(\\phi\\) is the activation function.</p>"},{"location":"#gradient-behavior-in-backpropagation","title":"Gradient Behavior in Backpropagation","text":"<p>During backpropagation, the gradient of the loss \\(L\\) with respect to \\(Z^{\\mathcal{l}}\\) can be approximated as:</p> \\[ \\left\\| \\frac{\\partial L}{\\partial Z^{\\mathcal{l}}} \\right\\| \\approx \\prod_{k = 1}^{\\mathcal{l}} \\|W^{\\mathcal{k}}\\| \\cdot \\|\\phi'(Z^{\\mathcal{k}-1})\\| \\] <p>This product can either explode or vanish, depending on the magnitudes of the weights and activation derivatives.</p>"},{"location":"#case-1-exploding-gradients","title":"Case 1: Exploding Gradients","text":"<p>Occurs when:</p> \\[ \\|W^{\\mathcal{k}}\\| &gt; 1 \\] <p>Let</p> \\[ C = \\max_k \\|W^{\\mathcal{k}}\\| \\Rightarrow \\prod_{k} \\|W^{\\mathcal{k}}\\|  \\propto C^{\\mathcal{l}} \\] <p>This exponential growth leads to exploding gradients, destabilizing training.</p>"},{"location":"#solutions","title":"Solutions:","text":"<ol> <li>Reduce depth using residual/skip connections.</li> <li>Regularization (e.g., \\(L_1\\), \\(L_2\\), or spectral norm regularization).</li> <li>Gradient clipping to limit gradient magnitude.</li> <li>Normalization techniques, such as BatchNorm or LayerNorm.</li> </ol>"},{"location":"#case-2-vanishing-gradients","title":"Case 2: Vanishing Gradients","text":"<p>Occurs when:</p> \\[ \\|W^{\\mathcal{k}}\\| &lt; 1 \\quad \\text{or} \\quad \\|\\phi'(Z^{\\mathcal{k}-1})\\| &lt; 1 \\] <p>This leads to gradients approaching zero, making it difficult for earlier layers to learn.</p>"},{"location":"#solutions_1","title":"Solutions:","text":"<ol> <li>Reduce depth via residual connections.</li> <li> <p>Use non-saturating activation functions:</p> </li> <li> <p>Prefer ReLU, Leaky ReLU, ELU, or Swish over sigmoid or tanh.</p> </li> <li>Proper weight initialization (e.g., He or Xavier initialization).</li> </ol>"},{"location":"#problem-of-dying-neurons","title":"Problem of Dying Neurons","text":"<p>With ReLU, neurons can \u201cdie\u201d (i.e., output zero for all inputs), especially when gradients become zero.</p>"},{"location":"#solutions_2","title":"Solutions:","text":"<ul> <li>Use Leaky ReLU, PReLU, or ELU to maintain non-zero gradients.</li> </ul>"},{"location":"#depth-and-skip-connections","title":"Depth and Skip Connections","text":"<p>Depth refers to the number of layers or the length of the computational path from input to output. Skip connections help by providing alternate shorter paths for gradient flow, effectively reducing the network\u2019s depth from a gradient propagation perspective.</p>"},{"location":"#summary-table","title":"Summary Table","text":"Problem Solutions Vanishing Gradient - Use residual connections (reduce effective depth)   - Use non-saturating activations  - Use proper initialization Exploding Gradient - Use residual connections  - Regularization (e.g., spectral norm)  - Gradient clipping  - Normalization techniques Dying Neurons - Use Leaky ReLU, ELU, or PReLU"},{"location":"about/","title":"About the Author","text":"<p>Hello! I\u2019m Emmanuel Kirui Barkacha, a student in the African Master\u2019s in Machine Intelligence (AMMI) program, pursuing a Master\u2019s degree in Machine Intelligence.</p> <p>This site was inspired by the coursework I\u2019m undertaking as part of the AMMI program particularly the Foundations to Machine Learning and Deep Learning module. I\u2019m especially grateful to my tutors and to Prof. Moustapha Ciss\u00e9, whose mentorship and teaching have deeply influenced my learning journey.</p>"},{"location":"about/#contact-information","title":"Contact Information:","text":"<ul> <li>Email: ebarkacha@aimsammi.org </li> <li>LinkedIn: www.linkedin.com/in/emmanuel-kirui-barkacha-493807294 </li> <li>GitHub: https://github.com/ekbarkacha </li> </ul>"},{"location":"about/#interests","title":"Interests:","text":"<ul> <li>Neural networks and deep learning  </li> <li>Applied machine learning  </li> <li>Responsible AI and fairness in machine learning  </li> </ul> <p>This site serves as a platform for sharing my notes, tutorials and practical insights from my studies with the hope it can help others learning ML as well.</p>"},{"location":"about/#projects","title":"Projects","text":"<p>Python Packages: - ek-data-structures   Implements key data structures such as arrays, stacks, queues, linked lists, trees, and graphs. - ek-ml-package   Collection of core machine learning algorithms implemented from scratch with NumPy.</p> <p>Android Mobile Applications: - Campus Aide   Brings all essential campus services together \u2014 from social features and memos to shopping and advertisements. - ViewPoint Status Saver   Download WhatsApp image and video statuses and share your own with the ViewPoint community. - Status Saver   Save and manage WhatsApp statuses (photos &amp; videos) easily in one place, with share and create functionality.</p>"},{"location":"activation_function/","title":"Activation Functions","text":""},{"location":"activation_function/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Step Function</li> <li>Linear Activation</li> <li>Sigmoid Function</li> <li>Tanh Function</li> <li>ReLU Function</li> <li>Leaky ReLU</li> <li>The Risk of Vanishing or Exploding Gradients</li> <li>Impact of Activation Functions on Model Performance</li> <li>Conclusion</li> </ol>"},{"location":"activation_function/#introduction","title":"Introduction","text":"<p>Activation functions in neural networks are mathematical functions that determine the output of a neuron based on its input. They introduce non-linearity, allowing neural networks to learn complex patterns in data. These functions decide whether a neuron should be activated or not, and how strongly its signal is passed to the next layer.</p> <p>Given an input vector \\(x \\in \\mathbb{R}^d\\) and weights \\(w \\in \\mathbb{R}^d\\), the neuron output before activation is:</p> \\[ z = w^\\top x + b \\] <p>Then the activation function \\(f(z)\\) is applied:</p> \\[ a = f(z) \\]"},{"location":"activation_function/#types-of-activation-functions","title":"Types of Activation Functions","text":"<p>Several different types of activation functions are used in Deep Learning. Some of them are explained below:</p> <ol> <li>Step Function</li> <li>Linear Activation</li> <li>Sigmoid Function</li> <li>Tanh Function</li> <li>ReLU Function</li> <li>Leaky ReLU</li> </ol>"},{"location":"activation_function/#step-function","title":"Step Function","text":"<p>The Step Function is one of the earliest activation functions, used in perceptrons to make binary decisions.</p>"},{"location":"activation_function/#definition","title":"Definition","text":"\\[ f(x) = \\begin{cases} 1 &amp; \\text{if } x \\geq 0 \\\\ 0 &amp; \\text{if } x &lt; 0 \\end{cases} \\] <p>This is also known as the Heaviside Step Function.</p> <p></p>"},{"location":"activation_function/#properties","title":"Properties","text":"<ul> <li>Output Range: \\(\\{0, 1\\}\\)</li> <li>Non-linear</li> <li>Not differentiable at \\(x = 0\\)</li> <li>No gradient, so not used in training modern neural networks</li> <li>Used in early binary classifiers (e.g., Perceptrons)</li> </ul> <p>Its graph jumps from 0 to 1 at \\(x = 0\\), with no smooth transition.</p>"},{"location":"activation_function/#linear-activation","title":"Linear Activation","text":"<p>The Linear Activation Function (identity function) is defined as:</p>"},{"location":"activation_function/#definition_1","title":"Definition","text":"\\[ f(x) = x \\]"},{"location":"activation_function/#derivative","title":"Derivative","text":"\\[ \\frac{df}{dx} = 1 \\]"},{"location":"activation_function/#properties_1","title":"Properties","text":"<ul> <li>Output Range: \\((-\\infty, \\infty)\\)</li> <li>No non-linearity as it just passes input as output</li> <li>Differentiable</li> <li>Used in the output layer of regression models</li> <li>Not suitable for hidden layers (makes stacked layers equivalent to a single layer)</li> </ul>"},{"location":"activation_function/#sigmoid-function","title":"Sigmoid Function","text":"<p>The sigmoid (logistic) function maps any real value to the (0, 1) interval.</p>"},{"location":"activation_function/#function-definition","title":"Function Definition","text":"\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]"},{"location":"activation_function/#properties_2","title":"Properties","text":"<ul> <li>Output Range: \\((0, 1)\\) i.e convenient to generate probabilities as output.</li> <li>Non-linear</li> <li>The function is differentiable and the gradient is smooth, i.e. no jumps in the ouput values.</li> <li>Used in binary classification</li> </ul> <p>Cons</p> <ul> <li>The sigmoid\u2019s derivative vanishes at its extreme input values (\\(z \\rightarrow - \\infty\\) and \\(z \\rightarrow \\infty\\)) and is thus proned to the issue called Vanishing Gradient problem.</li> </ul>"},{"location":"activation_function/#derivative_1","title":"Derivative","text":"<p>We need to compute:</p> \\[ \\frac{d}{dz} \\sigma(z) \\] <p>Let:</p> \\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] <p>Differentiate:</p> \\[ \\begin{align*} \\frac{d}{dz} \\sigma(z) &amp;= \\frac{d}{dz} \\left(1 + e^{-z}\\right)^{-1} \\\\ &amp;= -1 \\cdot (1 + e^{-z})^{-2} \\cdot (-e^{-z}) \\\\ &amp;= \\frac{e^{-z}}{(1 + e^{-z})^2} \\end{align*} \\] <p>Now rewrite in terms of \\(\\sigma(z)\\):</p> \\[ \\boxed{ \\frac{d}{dz} \\sigma(z) = \\sigma(z)(1 - \\sigma(z)) } \\]"},{"location":"activation_function/#tanh-function","title":"Tanh Function","text":"<p>The hyperbolic tangent maps real values to the range \\((-1, 1)\\). It\u2019s a scaled and shifted version of the sigmoid.</p>"},{"location":"activation_function/#function-definition_1","title":"Function Definition","text":"\\[ \\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \\]"},{"location":"activation_function/#properties_3","title":"Properties","text":"<ul> <li>Output Range: \\((-1, 1)\\)</li> <li>Zero-centered output</li> <li>Smooth and differentiable</li> </ul> <p>Cons</p> <ul> <li> <p>The gradient is much steeper than for the sigmoid (risk of jumps while descending)</p> </li> <li> <p>There is also a Vanishing Gradient problem due to the derivative cancelling for \\(z \\rightarrow - \\infty\\) and \\(z \\rightarrow \\infty\\).</p> </li> </ul>"},{"location":"activation_function/#derivative_2","title":"Derivative","text":"<p>We use:</p> \\[ \\frac{d}{dz} \\tanh(z) = 1 - \\tanh^2(z) \\]"},{"location":"activation_function/#proof","title":"Proof","text":"<p>Let \\(t = \\tanh(z)\\):</p> \\[ \\frac{d}{dz} \\tanh(z) = \\frac{d}{dz} \\left( \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\right) \\] <p>Using quotient rule:</p> <p>Let:</p> <ul> <li>\\(u = e^z - e^{-z}\\)</li> <li>\\(v = e^z + e^{-z}\\)</li> </ul> <p>Then:</p> \\[ \\frac{d}{dz} \\tanh(z) = \\frac{u'v - uv'}{v^2} \\] <p>Compute:</p> <ul> <li>\\(u' = e^z + e^{-z}\\)</li> <li>\\(v' = e^z - e^{-z}\\)</li> </ul> <p>So:</p> \\[ \\frac{d}{dz} \\tanh(z) = \\frac{(e^z + e^{-z})(e^z + e^{-z}) - (e^z - e^{-z})(e^z - e^{-z})}{(e^z + e^{-z})^2} \\] <p>Simplify numerator:</p> \\[ (a + b)^2 - (a - b)^2 = 4ab \\Rightarrow \\text{So numerator becomes: } 4 \\] <p>So:</p> \\[ \\frac{d}{dz} \\tanh(z) = \\frac{4}{(e^z + e^{-z})^2} \\] <p>But:</p> \\[ \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\Rightarrow \\tanh^2(z) = \\left( \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\right)^2 \\] <p>And:</p> \\[ 1 - \\tanh^2(z) = \\frac{(e^z + e^{-z})^2 - (e^z - e^{-z})^2}{(e^z + e^{-z})^2} = \\frac{4}{(e^z + e^{-z})^2} \\] <p>Therefore:</p> \\[ \\boxed{ \\frac{d}{dz} \\tanh(z) = 1 - \\tanh^2(z) } \\]"},{"location":"activation_function/#relu-function","title":"ReLU Function","text":"<p>Rectified Linear Unit is widely used in deep learning due to its simplicity and efficiency.</p>"},{"location":"activation_function/#function-definition_2","title":"Function Definition","text":"\\[ \\text{ReLU}(z) = \\max(0, z) \\]"},{"location":"activation_function/#properties_4","title":"Properties","text":"<ul> <li>Output Range: \\([0, \\infty)\\)</li> <li>Not differentiable at \\(z = 0\\)</li> <li>Computationally efficient</li> <li>Sparse activations</li> <li>Better gradient descent as the function does not saturate in both directions like the sigmoid and tanh. In other words, the Vanishing Gradient problem is half reduced.</li> </ul> <p>Cons</p> <ul> <li>Unlike the hyperbolic tangent, it is not zero-centered</li> <li>The range is infinite for positive input value (not bounded)</li> <li>ReLU is not differentiable at zero (but this can be solved by choosing arbitrarily a value for the derivative of either 0 or 1 for \\(z=0\\))</li> <li>The \u201cDying ReLU problem\u201d</li> </ul>"},{"location":"activation_function/#derivative_3","title":"Derivative","text":"\\[ \\frac{d}{dz} \\text{ReLU}(z) = \\begin{cases} 1 &amp; \\text{if } z &gt; 0 \\\\ 0 &amp; \\text{if } z &lt; 0 \\\\ \\text{undefined (or subgradient = 0 or 1)} &amp; \\text{if } z = 0 \\end{cases} \\] \\[ \\boxed{ \\frac{d}{dz} \\text{ReLU}(z) = \\mathbb{1}_{z &gt; 0} } \\] <p>What is the Dying ReLU problem? When we look at the derivative, we see the gradient on the negative side is zero. During the backpropagation algorithm, the weights and biases are not updated and the neuron becomes stuck in an inactive state. We refer to it as \u2018dead neuron.\u2019 If a large number of nodes are stuck in dead states, the model capacity to fit the data is decreased.</p> <p>To solve this serious issue, rectifier variants of the ReLU have been proposed:</p>"},{"location":"activation_function/#leaky-relu","title":"Leaky ReLU","text":"<p>Attempts to fix the dying ReLU problem (neurons output 0 and never recover).</p>"},{"location":"activation_function/#function-definition_3","title":"Function Definition","text":"\\[ \\text{Leaky ReLU}(z) = \\begin{cases} z &amp; \\text{if } z &gt; 0 \\\\ \\alpha z &amp; \\text{if } z \\leq 0 \\end{cases} \\] <p>where \\(\\alpha\\) is a small constant (e.g. 0.01)</p> <p></p> <p>Cons</p> <ul> <li>There is an extra parameter to tweak in the network, the slope value \\(\\alpha\\), which is not trivial to get as its optimized value is different depending on the data to fit.</li> </ul>"},{"location":"activation_function/#derivative_4","title":"Derivative","text":"\\[ \\frac{d}{dz} \\text{Leaky ReLU}(z) = \\begin{cases} 1 &amp; \\text{if } z &gt; 0 \\\\ \\alpha &amp; \\text{if } z \\leq 0 \\end{cases} \\] <p>Other activation functions include:</p>"},{"location":"activation_function/#exponential-linear-units-elus","title":"Exponential Linear Units (ELUs)","text":"<p>It does not have Rectifier in the name but the Exponential Linear Unit is another variant of ReLU.</p>"},{"location":"activation_function/#function-definition_4","title":"Function Definition","text":"\\[ \\text{ELU}(z) = \\begin{cases} \\alpha (e^z-1) &amp; \\text{if } z &lt; 0 \\\\ \\alpha z &amp; \\text{if } z \\geq 0 \\end{cases} \\] <p>where \\(z,\\alpha \\in \\mathbb{R}\\) with \\(\\alpha\\) a hyper-parameter to be tuned.</p> <p></p> <p>Pros</p> <ul> <li>From high to low input values, the ELU smoothly decreases until it outputs the negative value \\(-\\alpha\\). There is no more a \u2018kick\u2019 like in ReLU</li> <li>ELU functions have shown to converge cost to zero faster and produce more accurate results</li> </ul> <p>Cons</p> <ul> <li>The parameter  \\(\\alpha\\) needs to be tuned; it is not learnt</li> <li>For positive inputs, there is a risk of experiencing the Exploding Gradient problem (explanations further below in The risk of vanishing or exploding gradients)</li> </ul>"},{"location":"activation_function/#scaled-exponential-linear-unit-selu","title":"Scaled Exponential Linear Unit (SELU)","text":""},{"location":"activation_function/#function-definition_5","title":"Function Definition","text":"<p>$$ \\text{SELU}(z) = \\lambda \\begin{cases} \\alpha (e^z-1) &amp; \\text{if } z &lt; 0 \\ \\alpha z &amp; \\text{if } z \\geq 0 \\end{cases} $$ where \\(z,\\alpha \\in \\mathbb{R}\\)</p> <p>where \\(\\lambda = 1.0507 \\text{ and } \\alpha = 1.67326\\). Why these specific values? The values come from a normalization procedure; the SELU activation introduces self-normalizing properties. It takes care of internal normalization which means each layer preserves the mean and variance from the previous layers. SELU enables this normalization by adjusting the mean and variance. It can be shown that, for self-normalizing neural networks (SNNs), neuron activations are pushed towards zero mean and unit variance when propagated through the network.</p> <p></p> <p>Pros</p> <ul> <li>All the rectifier\u2019s advantages are at play</li> <li>Thanks to internal normalization, the network converges faster.</li> </ul> <p>Cons</p> <ul> <li>Not really a caveat in itself, but the SELU is outperforming other activation functions only for very deep networks.</li> </ul>"},{"location":"activation_function/#gaussian-error-linear-unit-gelu","title":"Gaussian Error Linear Unit (GELU)","text":"<p>Another modification of ReLU is the Gaussian Error Linear Unit. It can be thought of as a smoother ReLU. The definition is:</p> \\[ \\text{GELU}(z) = z \\cdot \\Phi(z) = z \\cdot \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right] \\] <p>Where:</p> <ul> <li>\\(\\Phi(z)\\) is the cumulative distribution function (CDF) of the standard normal distribution.</li> <li>\\(\\text{erf}(\\cdot)\\) is the Gauss error function.</li> </ul> <p>GELU is the state-of-the-art activation function used in particular in models called Transformers. Transformer model was introduced by Google Brain in 2017 to help in the multidisciplinary field of Natural Language Processing (NLP) that deals, among others, with tasks such as text translation or text summarization.</p> <p></p> <p>Pros</p> <ul> <li>Differentiable for all input values </li> <li>Avoids the Vanishing Gradient problem</li> <li>The function is non-convex, non-monotonic and not linear in the positive domain: it has thus curvature at all points. This actually allowed GELUs to approximate better complicated functions that ReLUs or ELUs can as it weights inputs by their value and not their sign (like ReLu and ELU do)</li> <li>The GELU, by construction, has a probabilistic interpretation (it is the expectaction of a stochastic regularizer)</li> </ul> <p>Cons</p> <ul> <li>GELU is time-consuming to compute</li> </ul>"},{"location":"activation_function/#sigmoid-linear-unit-silu-and-swish","title":"Sigmoid Linear Unit (SiLU) and Swish","text":"<p>The SiLU and Swish are the same function, just introduced by different authors (the Swish authors are from Google Brain). It is a state-of-the-art function aiming at superceeding the hegemonic ReLU. The Swish is defined as a sigmoid multiplied with the identity: $$ f(z) = \\frac{z}{1+e^{-z}} $$ The Swish function exhibits increased classification accuracy and consistently matches or outperforms ReLU activation function on deep networks (especially on image classification).</p> <p></p> <p>Pros</p> <ul> <li>It is differentiable on the whole range.</li> <li> <p>The function is smooth and non-monotonic (like GELU), which is an advantage to enhance input data during learning</p> </li> <li> <p>Unlike the ReLU function, small negative values are not zeroed, allowing for a better modeling of the data. And large negative values are zeroed out (in other words, the node will die only if it needs to die)</p> </li> </ul> <p>Note: Swish function is only relevant if it is used in neural networks having a depth greater than 40 layers.</p>"},{"location":"activation_function/#the-risk-of-vanishing-or-exploding-gradients","title":"The risk of vanishing or exploding gradients","text":"<p>Training a neural network with a gradient-based learning method (the gradient descent is one) can lead to issues. The culprit, or rather cause, lies in the choice of the activation function:</p>"},{"location":"activation_function/#vanishing-gradient-problem","title":"Vanishing Gradient problem","text":"<p>As seen with the sigmoid and hyperbolic tangent, certain activation functions converge asymptotically towards the bounded range. Thus, at the extremities (large negative or large positive input values), a large change in the input will cause a very small modification of the output: there is a saturation. As a consequence the gradient will be also very small and the learning gain after one iteration very minimal, tending towards zero. This is to be avoid if we want the algorithm to learn a decent amount at each step.</p>"},{"location":"activation_function/#exploding-gradient-problem","title":"Exploding Gradient problem","text":"<p>If significant errors accumulate and the neural network updates the weights with larger and larger values, the difference between the prediction and observed values will increase further and further, leading to exploding gradients. It\u2019s no more a descent but a failure to converge. Pragmatically, it is possible to see it when weights are so large that they overflow and return a NaN value (meaning Not A Number).</p>"},{"location":"activation_function/#impact-of-activation-functions-on-model-performance","title":"Impact of Activation Functions on Model Performance","text":"<p>The choice of activation function has a direct impact on the performance of a neural network in several ways:</p> <ol> <li> <p>Convergence Speed: Functions like ReLU allow faster training by avoiding the vanishing gradient problem while Sigmoid and Tanh can slow down convergence in deep networks.</p> </li> <li> <p>Gradient Flow: Activation functions like ReLU ensure better gradient flow, helping deeper layers learn effectively. In contrast Sigmoid can lead to small gradients, hindering learning in deep layers.</p> </li> <li> <p>Model Complexity: Activation functions like Softmax allow the model to handle complex multi-class problems, whereas simpler functions like ReLU or Leaky ReLU are used for basic layers.</p> </li> </ol>"},{"location":"activation_function/#conclusion","title":"Conclusion","text":"<p>Activation functions are essential to neural networks, enabling them to capture complex, non-linear relationships in data. We examined the mathematical formulations and derivatives of key activation functions, including Sigmoid, Tanh, ReLU, and Leaky ReLU\u2014each offering unique strengths and trade-offs. From traditional functions to modern variants, the choice of activation function plays a critical role in a model\u2019s performance and should be made based on the specific problem and network depth.</p>"},{"location":"gaussian_discriminant_analysis/","title":"Gaussian Discriminant Analysis (GDA) \u2014 Multiclass","text":""},{"location":"gaussian_discriminant_analysis/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Generative Assumptions</li> <li>Model Derivation</li> <li>Linear Discriminant Analysis (LDA)</li> <li>Quadratic Discriminant Analysis (QDA)</li> <li>Comparison: LDA vs QDA</li> <li>Conclusion</li> </ol>"},{"location":"gaussian_discriminant_analysis/#introduction","title":"Introduction","text":"<p>GDA is a generative classifier as it models the joint distribution \\(P(x, y)\\), and uses Bayes\u2019 Rule to derive \\(P(y|x)\\) for classification.</p> <p>GDA assumes that the feature distribution conditioned on the class is Gaussian:</p> \\[ P(x \\mid y = k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k) \\] <p>This generalizes easily to the multiclass setting, where:</p> <ul> <li>\\(x \\in \\mathbb{R}^d\\) is the feature vector</li> <li>\\(y \\in \\{0, 1, ..., K-1\\}\\) is the class label</li> <li>\\(\\phi_k = P(y = k)\\) is the prior for class \\(k\\)</li> <li>\\(\\mu_k \\in \\mathbb{R}^d\\), \\(\\Sigma_k \\in \\mathbb{R}^{d \\times d}\\)</li> </ul>"},{"location":"gaussian_discriminant_analysis/#generative-assumptions","title":"Generative Assumptions","text":"<p>Let the training data be:</p> \\[ \\mathcal{D} = \\left\\{ (x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \\right\\} \\] <p>For each class \\(k \\in \\{0, ..., K-1\\}\\):</p> <ul> <li>\\(x \\mid y = k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)\\)</li> <li>\\(y \\sim \\text{Categorical}(\\phi_0, ..., \\phi_{K-1})\\)</li> </ul>"},{"location":"gaussian_discriminant_analysis/#model-derivation","title":"Model Derivation","text":""},{"location":"gaussian_discriminant_analysis/#posterior-using-bayes-theorem","title":"Posterior using Bayes\u2019 Theorem:","text":"\\[ P(y = k \\mid x) = \\frac{P(x \\mid y = k) P(y = k)}{\\sum_{j=0}^{K-1} P(x \\mid y = j) P(y = j)} \\] <p>We predict:</p> \\[ \\boxed{h(x) = \\arg\\max_k \\left[ \\log P(x \\mid y = k) + \\log \\phi_k \\right]} \\]"},{"location":"gaussian_discriminant_analysis/#linear-discriminant-analysis-lda","title":"Linear Discriminant Analysis (LDA)","text":""},{"location":"gaussian_discriminant_analysis/#assumptions","title":"Assumptions","text":"<ul> <li>All class-conditional distributions share the same covariance matrix: \\(\\Sigma_k = \\Sigma\\)</li> <li>Only the class means \\(\\mu_k\\) differ.</li> </ul>"},{"location":"gaussian_discriminant_analysis/#decision-function","title":"Decision Function","text":"<p>Each class has a discriminant function:</p> \\[ g_k(x) = x^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log \\phi_k \\] <p>Predict:</p> \\[ \\boxed{h(x) = \\arg\\max_k \\, g_k(x)} \\]"},{"location":"gaussian_discriminant_analysis/#parameter-estimation-mle","title":"Parameter Estimation (MLE)","text":"<p>From data, estimate:</p> <ul> <li>Class priors:</li> </ul> <p>$$   \\phi_k = \\frac{n_k}{n}, \\quad \\text{where } n_k = \\sum_{i=1}^n \\mathbb{1}{y_i = k}   $$</p> <ul> <li>Class means:</li> </ul> <p>$$   \\mu_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i   $$</p> <ul> <li>Shared covariance matrix:</li> </ul> <p>$$   \\Sigma = \\frac{1}{n} \\sum_{k=0}^{K-1} \\sum_{i: y_i = k} (x_i - \\mu_k)(x_i - \\mu_k)^\\top   $$</p>"},{"location":"gaussian_discriminant_analysis/#quadratic-discriminant-analysis-qda","title":"Quadratic Discriminant Analysis (QDA)","text":""},{"location":"gaussian_discriminant_analysis/#assumptions_1","title":"Assumptions","text":"<ul> <li>Each class has its own covariance matrix: \\(\\Sigma_k\\)</li> <li>This yields quadratic decision boundaries.</li> </ul>"},{"location":"gaussian_discriminant_analysis/#decision-function_1","title":"Decision Function","text":"<p>Each class has a quadratic discriminant:</p> \\[ g_k(x) = -\\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2}(x - \\mu_k)^\\top \\Sigma_k^{-1}(x - \\mu_k) + \\log \\phi_k \\] <p>Predict:</p> \\[ \\boxed{h(x) = \\arg\\max_k \\, g_k(x)} \\]"},{"location":"gaussian_discriminant_analysis/#parameter-estimation-mle_1","title":"Parameter Estimation (MLE)","text":"<p>For each class \\(k\\):</p> <ul> <li>Class prior:</li> </ul> <p>$$   \\phi_k = \\frac{n_k}{n}   $$</p> <ul> <li>Class mean:</li> </ul> <p>$$   \\mu_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i   $$</p> <ul> <li>Class-specific covariance:</li> </ul> <p>$$   \\Sigma_k = \\frac{1}{n_k} \\sum_{i: y_i = k} (x_i - \\mu_k)(x_i - \\mu_k)^\\top   $$</p>"},{"location":"gaussian_discriminant_analysis/#comparison-lda-vs-qda","title":"Comparison: LDA vs QDA","text":"Feature LDA QD Covariance Assumption Same for all classes Unique per class Decision Boundary Linear Quadratic Parameters Estimated \\(\\mu_k, \\phi_k, \\Sigma\\) \\(\\mu_k, \\phi_k, \\Sigma_k\\) Complexity Simpler, lower variance More flexible, but can overfit on small data Ideal Use Case Classes share similar spread (covariance) Classes have distinct covariance structures"},{"location":"gaussian_discriminant_analysis/#conclusion","title":"Conclusion","text":"<p>Multiclass GDA extends the binary case by modeling each class \\(k \\in \\{0, ..., K-1\\}\\) with its own mean and covariance (for QDA) or a shared covariance (for LDA). The model remains clear and interpretable:</p> <ul> <li>LDA gives a linear classifier, computationally efficient and robust.</li> <li>QDA is more expressive, better when covariances differ.</li> </ul> <p>Both compute closed-form MLE solutions, making them fast and useful even in high dimensions.</p>"},{"location":"kmeans/","title":"K-Means Clustering","text":""},{"location":"kmeans/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Problem Setup</li> <li>Objective Function</li> <li>K-Means Algorithm</li> <li>Mathematical Derivation</li> <li>Convergence and Complexity</li> <li>Limitations and Considerations</li> <li>Conclusion</li> </ol>"},{"location":"kmeans/#introduction","title":"Introduction","text":"<p>K-Means is an unsupervised learning algorithm that partitions a dataset into K clusters, where each point belongs to the cluster with the nearest mean. It aims to minimize intra-cluster variance while maximizing inter-cluster separation. The algorithm is simple yet powerful and it\u2019s widely used in data mining and pattern recognition.</p>"},{"location":"kmeans/#problem-setup","title":"Problem Setup","text":"<p>Let:</p> <ul> <li>\\(n\\): number of data points</li> <li>\\(d\\): number of dimensions/features</li> <li>\\(k\\): number of clusters</li> </ul> <p>We define:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\): data matrix where each row \\(x_i \\in \\mathbb{R}^d\\)</li> <li>\\(\\mu_1, \\mu_2, \\dots, \\mu_k \\in \\mathbb{R}^d\\): centroids of clusters</li> <li>\\(C_i\\): the set of data points assigned to cluster \\(i\\) for \\(i = 1, 2, ..., k\\)</li> </ul>"},{"location":"kmeans/#objective-function","title":"Objective Function","text":"<p>K-means aims to minimize the within-cluster sum of squares (WCSS), also called the distortion function:</p> \\[ J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 \\] <p>The goal is to find the cluster assignments \\(C_i\\) and the centroids \\(\\mu\\_i\\) that minimize \\(J\\).</p>"},{"location":"kmeans/#k-means-algorithm","title":"K-Means Algorithm","text":"<p>The algorithm follows an iterative refinement approach:</p> <p>Input: Dataset \\(\\mathbf{X}\\), number of clusters \\(k\\) Output: Set of centroids \\(\\mu_1, \\dots, \\mu_k\\), cluster assignments</p> <p>Steps:</p> <ol> <li>Initialization: Randomly choose \\(k\\) points from the dataset as initial centroids.</li> <li>Assignment Step: Assign each data point to the cluster whose centroid is closest:</li> </ol> <p>$$ C_i = \\{ x_j : |x_j - \\mu_i|^2 \\le |x_j - \\mu_l|^2 \\quad \\forall l = 1, \\dots, k \\} $$ 3. Update Step: Recompute the centroids as the mean of the assigned points:</p> <p>$$    \\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x    $$ 4. Repeat steps 2 and 3 until convergence (no change in assignments or centroids).</p>"},{"location":"kmeans/#mathematical-derivation","title":"Mathematical Derivation","text":""},{"location":"kmeans/#1-assignment-step-minimizes-distance","title":"1. Assignment Step Minimizes Distance","text":"<p>We fix the centroids \\(\\mu_1, \\dots, \\mu_k\\) and assign each \\(x_j\\) to the nearest centroid. For each point, we solve:</p> \\[ \\min_{i \\in \\{1,\\dots,k\\}} \\|x_j - \\mu_i\\|^2 \\] <p>This step ensures that, for a fixed set of centroids, the loss \\(J\\) is minimized with respect to cluster assignments.</p>"},{"location":"kmeans/#2-update-step-minimizes-wcss-for-fixed-assignments","title":"2. Update Step Minimizes WCSS for Fixed Assignments","text":"<p>Fixing the assignments \\(C_1, \\dots, C_k\\), the best centroid \\(\\mu_i\\) that minimizes:</p> \\[ \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 \\] <p>is obtained by solving:</p> \\[ \\frac{d}{d\\mu_i} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 = 0 \\] <p>Using vector calculus:</p> \\[ \\begin{aligned} \\|x - \\mu_i\\|^2 &amp;= (x - \\mu_i)^\\top(x - \\mu_i) \\\\ \\frac{d}{d\\mu_i} \\|x - \\mu_i\\|^2 &amp;= -2(x - \\mu_i) \\\\ \\Rightarrow \\frac{d}{d\\mu_i} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2 &amp;= -2 \\sum_{x \\in C_i} (x - \\mu_i) \\end{aligned} \\] <p>Setting the derivative to 0:</p> \\[ \\sum_{x \\in C_i} (x - \\mu_i) = 0 \\Rightarrow \\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x \\]"},{"location":"kmeans/#convergence-and-complexity","title":"Convergence and Complexity","text":"<ul> <li>Convergence: K-Means is guaranteed to converge to a local minimum of the objective \\(J\\), because each step (assignment and update) does not increase the objective.</li> <li> <p>Time Complexity:</p> </li> <li> <p>One iteration: \\(\\mathcal{O}(nkd)\\)</p> </li> <li>If the algorithm converges in \\(T\\) iterations, total: \\(\\mathcal{O}(nkdT)\\)</li> </ul>"},{"location":"kmeans/#limitations-and-considerations","title":"Limitations and Considerations","text":"Limitation Description Local Minimum May converge to different solutions depending on initialization Number of Clusters (\\(k\\)) Must be specified beforehand Spherical Clusters Assumed Works best when clusters are isotropic (round, same size) Sensitive to Outliers Outliers can distort the position of centroids Empty Clusters A cluster may become empty during iterations, requiring reassignment"},{"location":"kmeans/#initialization-strategies","title":"Initialization Strategies","text":"<p>To address random initialization issues, methods like K-Means++ are used, which spread out initial centroids:</p> \\[ P(x) \\propto \\min_{i} \\|x - \\mu_i\\|^2 \\]"},{"location":"kmeans/#k-means-initialization-with-softmax-probabilities","title":"K-Means++ Initialization (with Softmax Probabilities)","text":"<p>K-Means++ improves the standard K-Means algorithm by carefully choosing initial centroids to be well-separated, which often leads to improved convergence and clustering quality. This version uses the softmax function to transform distances into selection probabilities.</p> <p>Initialize Centroids using K-Means++:</p> <ol> <li> <p>Choose the first centroid randomly from the dataset \\(\\mathcal{X} = {x_1, x_2, \\dots, x_n}\\).</p> </li> <li> <p>For each data point \\(x_i \\in \\mathcal{X}\\) that is not yet selected as a centroid, compute the distance \\(d(x_i)\\) to the nearest already chosen centroid:</p> <p>\\(   d(x_i) = \\min_{\\mu \\in \\{\\mu_1, \\dots, \\mu_t\\}} \\|x_i - \\mu\\|   \\)</p> </li> <li> <p>Convert distances to probabilities using the softmax function:</p> <p>\\(   p(x_i) = \\frac{e^{d(x_i)}}{\\sum_{j=1}^{n} e^{d(x_j)}}   \\)</p> <p>This assigns a higher probability to points that are farther from existing centroids, but in a smooth, differentiable manner.</p> </li> <li> <p>Select the next centroid randomly from the dataset using the probability distribution \\(p(x_i)\\).</p> </li> <li> <p>Repeat steps 2\u20134 until \\(k\\) centroids have been initialized.</p> </li> <li> <p>Continue with the standard K-Means algorithm using these initial centroids.</p> </li> </ol>"},{"location":"kmeans/#conclusion","title":"Conclusion","text":"<p>K-Means is an important clustering algorithm that minimizes intra-cluster variance using an iterative algorithm. The two main steps\u2014assignment and update\u2014each reduce the objective, ensuring convergence. While simple and efficient, it assumes spherical clusters and requires choosing the number of clusters beforehand.</p>"},{"location":"knn/","title":"K-Nearest Neighbors (KNN)","text":""},{"location":"knn/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Data Representation</li> <li>Model Hypothesis</li> <li>Loss Function</li> <li>Classification Algorithm</li> <li>Distance Metrics</li> <li>Advantages and Disadvantages</li> <li>Conclusion</li> </ol>"},{"location":"knn/#introduction","title":"Introduction","text":"<p>K-Nearest Neighbors (KNN) is a non-parametric and lazy learning algorithm used for classification and regression tasks. It does not learn an explicit model during training and instead it stores the training dataset and uses it during the prediction time.</p> <ul> <li>Non-Parametric: KNN does not assume any underlying distribution for the data.</li> <li>Lazy Learning: The algorithm does not train on the dataset immediately but instead performs all computation during prediction.</li> </ul>"},{"location":"knn/#basic-idea","title":"Basic Idea:","text":"<p>For a given test point, KNN looks at the K closest training points (neighbors) in the feature space and outputs the most common class (for classification) or average (for regression) of these neighbors.</p>"},{"location":"knn/#data-representation","title":"Data Representation","text":"<p>Let:</p> <ul> <li>\\(n\\): number of training examples</li> <li>\\(d\\): number of features</li> </ul> <p>We define:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\): feature matrix (training data)</li> <li>\\(\\mathbf{y} \\in {0, 1}^n\\): target vector (labels) for classification tasks (can be continuous for regression)</li> <li>\\(\\mathbf{x\\_{\\text{test}}} \\in \\mathbb{R}^{1 \\times d}\\): feature vector for the test point</li> </ul> <p>In classification tasks, we assume that each data point is labeled as either \\(y_i \\in {0, 1}\\) (binary classification), or can be from a set of multiple classes for multiclass classification.</p>"},{"location":"knn/#model-hypothesis","title":"Model Hypothesis","text":"<p>In KNN, the hypothesis is simply based on local neighborhoods. Given a new test point \\(\\mathbf{x\\_{\\text{test}}}\\), the model classifies it by looking at the K nearest neighbors from the training set \\(\\mathbf{X}\\). The output is the most common class among these neighbors.</p> <ul> <li>Classification: The label for the test point is predicted based on the majority vote of the K nearest neighbors:</li> </ul> \\[ \\hat{y}_{\\text{test}} = \\text{mode}(y_{\\text{neighbors}}) \\quad \\text{where } y_{\\text{neighbors}} \\text{ is the label of the K nearest neighbors}. \\] <ul> <li>Regression: The predicted value is the average of the K nearest neighbors\u2019 values:</li> </ul> \\[ \\hat{y}_{\\text{test}} = \\frac{1}{K} \\sum_{i=1}^{K} y_i \\]"},{"location":"knn/#key-concept","title":"Key Concept:","text":"<p>KNN relies on the assumption that similar data points (i.e., those close in the feature space) share similar labels.</p>"},{"location":"knn/#loss-function","title":"Loss Function","text":"<p>KNN is not trained with a traditional loss function like in supervised models such as Logistic Regression. Instead, the \u201closs\u201d comes from the accuracy of the model during prediction. However, for understanding and performance evaluation, the accuracy or error rate of the predictions can be considered the equivalent of a loss function.</p>"},{"location":"knn/#for-classification","title":"For Classification:","text":"<ul> <li>Accuracy (as a performance measure):</li> </ul> <p>$$   \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{n_{\\text{test}}}   $$</p> <ul> <li>Where \\(n\\_{\\text{test}}\\) is the number of test examples.</li> <li>Misclassification Rate (as the loss function):</li> </ul> <p>$$   \\text{Loss} = \\frac{\\text{Number of Incorrect Predictions}}{n_{\\text{test}}}   $$</p>"},{"location":"knn/#for-regression","title":"For Regression:","text":"<ul> <li>Mean Squared Error (MSE) (Loss Function):</li> </ul> <p>$$   \\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{true}}^{(i)} - \\hat{y}_{\\text{test}}^{(i)})^2   $$</p> <ul> <li>Where \\(y_{\\text{true}}\\) is the true value, and \\(\\hat{y}_{\\text{test}}\\) is the predicted value for test point \\(i\\).</li> </ul>"},{"location":"knn/#classification-algorithm","title":"Classification Algorithm","text":"<ol> <li> <p>Training Phase:</p> <ul> <li>The KNN algorithm does not perform explicit training. All data points are stored in memory.</li> </ul> </li> <li> <p>Prediction Phase:</p> <ul> <li>Given a test point \\(\\mathbf{x_{\\text{test}}}\\), find the K nearest neighbors from the training set.</li> <li>The distance between \\(\\mathbf{x_{\\text{test}}}\\) and every point in the training set is calculated.</li> <li>Sort the training points by distance.</li> <li>Select the top K nearest neighbors.</li> <li>For classification, perform a majority vote among the K neighbors to determine the predicted class.</li> </ul> </li> </ol>"},{"location":"knn/#distance-metrics","title":"Distance Metrics","text":"<p>The distance between points is very important in KNN, as it directly affects the algorithm\u2019s performance. The most commonly used distance metrics are:</p> <ol> <li> <p>Euclidean Distance (or L2 Distance):</p> <p>\\( d(\\mathbf{x}_i, \\mathbf{x}_j) = \\sqrt{\\sum_{k=1}^{d} (x_{i,k} - x_{j,k})^2} \\)</p> </li> <li> <p>Manhattan Distance (or L1 Distance):</p> <p>\\(   d(\\mathbf{x}_i, \\mathbf{x}_j) = \\sum_{k=1}^{d} |x_{i,k} - x_{j,k}|   \\)</p> </li> <li> <p>Cosine Similarity (often used for text data):</p> <p>\\(   d(\\mathbf{x}_i, \\mathbf{x}_j) = 1 - \\frac{\\mathbf{x}_i \\cdot \\mathbf{x}_j}{\\|\\mathbf{x}_i\\| \\|\\mathbf{x}_j\\|}   \\)</p> </li> </ol>"},{"location":"knn/#choosing-k","title":"Choosing K:","text":"<ul> <li>Small K (e.g., K = 1): More sensitive to noise but can model highly localized patterns.</li> <li>Large K: Less sensitive to noise, but may smooth out important patterns (underfitting).</li> </ul>"},{"location":"knn/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":""},{"location":"knn/#advantages","title":"Advantages:","text":"<ul> <li>Simplicity: KNN is easy to understand and implement.</li> <li>No Training Phase: KNN is a lazy learner, so it doesn\u2019t require training data to build an explicit model.</li> <li>Versatile: KNN can be used for both classification and regression tasks.</li> </ul>"},{"location":"knn/#disadvantages","title":"Disadvantages:","text":"<ul> <li>Computationally Expensive: KNN can be slow, especially for large datasets, because it needs to compute the distance between the test point and all points in the training set.</li> <li>Sensitive to Irrelevant Features: The performance of KNN can degrade if the data has many irrelevant features, as distances become less meaningful.</li> <li>Storage: KNN requires storing all the training data, which may be infeasible for very large datasets.</li> </ul>"},{"location":"knn/#conclusion","title":"Conclusion","text":"<p>The K-Nearest Neighbors (KNN) algorithm is a simple, instance-based learning method that makes predictions based on the proximity of data points. It is widely used due to its simplicity and effectiveness for both classification and regression problems. Despite its simplicity, KNN can be computationally expensive and is sensitive to irrelevant or noisy features. Proper distance metrics and selecting an appropriate K value are essential for achieving good performance with this algorithm.</p>"},{"location":"linear_regression/","title":"Linear Regression And Gradient Descent","text":""},{"location":"linear_regression/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Data Representation</li> <li>Model Hypothesis</li> <li>Loss Function (L2 Norm)</li> <li>Gradient Of Loss Function</li> <li>Gradient Descent Algorithms<ul> <li>Batch Gradient Descent</li> <li>Stochastic Gradient Descent</li> <li>Stochastic Gradient Descent with Momentum</li> <li>Mini-batch Gradient Descent</li> </ul> </li> <li>Conclusion</li> </ol>"},{"location":"linear_regression/#introduction","title":"Introduction","text":"<p>Linear regression models a linear relationship between input features X and a target vector y. Using matrix operations simplifies computation and scales better for large datasets.</p>"},{"location":"linear_regression/#data-representation","title":"Data Representation","text":"<p>Let:</p> <ul> <li>\\(n\\): number of training examples  </li> <li>\\(d\\): number of features  </li> </ul> <p>We define:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\): feature matrix  </li> <li>\\(\\mathbf{y} \\in \\mathbb{R}^{n \\times 1}\\): target vector  </li> <li>\\(\\beta \\in \\mathbb{R}^{d \\times 1}\\): weight vector</li> </ul> \\[ \\mathbf{X} = \\begin{bmatrix} x_1^{(1)} &amp; x_2^{(1)} &amp; \\dots &amp; x_d^{(1)} \\\\ x_1^{(2)} &amp; x_2^{(2)} &amp; \\dots &amp; x_d^{(2)} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_1^{(n)} &amp; x_2^{(n)} &amp; \\dots &amp; x_d^{(n)} \\end{bmatrix} \\]"},{"location":"linear_regression/#model-hypothesis","title":"Model Hypothesis","text":"<p>Given the dataset in the form of \\((x_i, y_i)\\) for \\(i = 1, 2, \\dots, n\\), and under the following assumptions:</p> <ul> <li>\\(y_i = \\beta x_i + \\epsilon_i\\) (Linearity assumption) </li> <li>\\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\) (Noise is normally distributed with zero mean)</li> </ul> <p>Then the expected value of \\(y_i\\) conditioned on \\(x_i\\) is:</p> \\[ \\begin{aligned} \\hat{y}_i &amp;= \\mathbb{E}[y_i \\mid x_i] \\\\          &amp;= \\mathbb{E}[\\beta x_i + \\epsilon_i] \\\\          &amp;= \\beta x_i + \\mathbb{E}[\\epsilon_i] \\\\          &amp;= \\beta x_i \\end{aligned} \\] <p>Hence, the predicted output is a linear function of the input.</p>"},{"location":"linear_regression/#matrix-form","title":"Matrix Form","text":"<p>The model hypothesis in matrix form is:</p> \\[ \\hat{\\mathbf{y}} = \\mathbf{X} \\beta \\] <p>Where:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) is the feature matrix  </li> <li>\\(\\beta \\in \\mathbb{R}^{d \\times 1}\\) is the weight vector  </li> <li>\\(\\hat{\\mathbf{y}} \\in \\mathbb{R}^{n \\times 1}\\) is the vector of predictions</li> </ul>"},{"location":"linear_regression/#loss-function","title":"Loss Function","text":"<p>To derive the loss function for linear regression, we start by modeling the data generation process probabilistically.</p>"},{"location":"linear_regression/#assumption-gaussian-noise-model","title":"Assumption: Gaussian Noise Model","text":"<p>We assume that the observed outputs \\(y_i\\) are generated from a linear model with additive Gaussian noise:</p> \\[ \\begin{aligned} y_i &amp;= \\mathbf{x}_i \\beta + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\\\ \\hat{y}_i&amp;= \\mathbf{x}_i \\beta \\end{aligned} \\] <p>Then, the conditional probability of observing \\(y_i\\) given \\(\\mathbf{x}_i\\) and the parameters \\(\\beta\\) is:</p> \\[ p(y_i \\mid \\mathbf{x}_i; \\beta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i \\beta)^2}{2\\sigma^2} \\right) \\]"},{"location":"linear_regression/#likelihood","title":"Likelihood","text":"\\[ \\mathcal{L}(\\beta) = \\prod_{i=1}^n p(y_i \\mid \\mathbf{x}_i; \\beta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i \\beta)^2}{2\\sigma^2} \\right) \\]"},{"location":"linear_regression/#log-likelihood","title":"Log-Likelihood","text":"<p>To simplify, take the log-likelihood:</p> \\[ \\log \\mathcal{L}(\\beta) = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_i\\beta)^2 \\]"},{"location":"linear_regression/#maximum-log-likelihood-estimator-mle","title":"Maximum Log-likelihood Estimator (MLE)","text":"\\[\\hat{\\beta} = \\arg\\max_{\\beta} \\left( -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_i \\beta)^2 \\right)\\] <p>Since \\(\\sigma^2\\) is constant and we want to maximize the log-likelihood, this is equivalent to minimizing the negative log-likelihood (NLL):</p> \\[ \\begin{aligned} \\hat{\\beta} &amp;= \\arg\\min_{\\beta} \\left(\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_i \\beta)^2\\right)\\\\ &amp;= \\arg\\min_{\\beta} \\left(\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\right) \\text{ since }\\hat{y}_i&amp;= \\mathbf{x}_i \\beta \\end{aligned} \\] <p>Ignoring constants, minimizing NLL is equivalent to minimizing the Mean Squared Error (MSE):</p> \\[ \\hat{\\beta} = \\arg\\min_{\\beta} \\left(\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\right) \\]"},{"location":"linear_regression/#final-loss-function-mse","title":"Final Loss Function (MSE)","text":"<p>In matrix form:</p> \\[ \\mathcal{L}(\\beta) = \\frac{1}{n} \\| \\mathbf{y} - \\mathbf{X} \\beta\\|_2^2 \\] <p>Where:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\)</li> <li>\\(\\mathbf{y} \\in \\mathbb{R}^{n \\times 1}\\)</li> <li>\\(\\beta \\in \\mathbb{R}^{d \\times 1}\\)</li> </ul>"},{"location":"linear_regression/#gradient-of-the-loss-function","title":"Gradient of the Loss Function","text":"<p>We begin with the Mean Squared Error (MSE) loss function in matrix form as shown above.</p> \\[ \\begin{aligned} \\mathcal{L}(\\beta) &amp;= \\frac{1}{n} \\| \\mathbf{y} - \\mathbf{X} \\beta\\|_2^2\\\\                    &amp;= \\frac{1}{n} (\\mathbf{y} - \\mathbf{X} \\beta)^\\top (\\mathbf{y} - \\mathbf{X} \\beta) \\end{aligned} \\] <p>Let\u2019s expand the inner product:</p> \\[ \\begin{aligned} \\mathcal{L}(\\beta) &amp;= \\frac{1}{n} \\left( \\mathbf{y}^\\top \\mathbf{y} - 2 \\mathbf{y}^\\top \\mathbf{X} \\beta + \\beta^\\top \\mathbf{X}^\\top \\mathbf{X} \\beta \\right) \\end{aligned} \\] <p>Now, we compute the gradient of each term with respect to \\(\\beta\\):</p> <ul> <li>\\(\\nabla_\\beta(\\mathbf{y}^\\top \\mathbf{y}) = 0\\) (no \\(\\beta\\) involved)  </li> <li>\\(\\nabla_\\beta(-2 \\mathbf{y}^\\top \\mathbf{X} \\beta) = -2 \\mathbf{X}^\\top \\mathbf{y}\\) </li> <li>\\(\\nabla_\\beta(\\beta^\\top \\mathbf{X}^\\top \\mathbf{X} \\beta) = 2 \\mathbf{X}^\\top \\mathbf{X} \\beta\\)</li> </ul> <p>Putting it all together:</p> \\[ \\nabla_\\beta \\mathcal{L}(\\beta) = \\frac{1}{n} \\left( -2 \\mathbf{X}^\\top \\mathbf{y} + 2 \\mathbf{X}^\\top \\mathbf{X} \\beta \\right) \\] <p>To simplify the expression we factor out the 2 which gives the gradient of the MSE loss with respect to \\(\\beta\\):</p> \\[ \\nabla_\\beta \\mathcal{L}(\\beta) = \\frac{2}{n} \\left( \\mathbf{X}^\\top \\mathbf{X} \\beta - \\mathbf{X}^\\top \\mathbf{y} \\right) \\] <p>This gradient is used to perform parameter updates in Gradient Descent:</p> \\[ \\beta_1 = \\beta_0 - \\alpha \\cdot \\nabla_\\beta \\mathcal{L}(\\beta_0) \\] <p>Where:</p> <ul> <li>\\(\\alpha\\) is the learning rate</li> </ul>"},{"location":"linear_regression/#solving-for-beta-normal-equation","title":"Solving for \\(\\beta\\) (Normal Equation)","text":"<p>We can find the optimal \\(\\beta\\) by setting the gradient of the loss function to zero:</p> \\[ \\nabla_\\beta \\mathcal{L}(\\beta) = \\frac{2}{n} \\left( \\mathbf{X}^\\top \\mathbf{X} \\beta - \\mathbf{X}^\\top \\mathbf{y} \\right) = 0 \\] <p>Multiplying both sides by \\(\\frac{n}{2}\\):</p> \\[ \\mathbf{X}^\\top \\mathbf{X} \\beta = \\mathbf{X}^\\top \\mathbf{y} \\] <p>Solving for \\(\\beta\\) gives the closed-form solution (also called the normal equation):</p> \\[ \\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\]"},{"location":"linear_regression/#issue-non-invertibility-of-mathbfxtop-mathbfx","title":"Issue: Non-Invertibility of \\(\\mathbf{X}^\\top \\mathbf{X}\\)","text":"<p>In practice, the matrix \\(\\mathbf{X}^\\top \\mathbf{X}\\) may not be invertible if:</p> <ul> <li>Features are linearly dependent (i.e., multicollinearity)</li> <li>The number of features \\(d\\) is greater than the number of samples \\(n\\) (underdetermined system)</li> <li>Numerical precision issues due to ill-conditioning</li> </ul> <p>In these cases, we cannot compute \\( \\hat{\\beta} \\) directly using the normal equation.</p>"},{"location":"linear_regression/#solution-add-regularization-ridge-regression","title":"Solution: Add Regularization (Ridge Regression)","text":"<p>To address non-invertibility and reduce overfitting, we add an L2 regularization term (also known as Ridge Regression):</p> <p>Modified loss function:</p> \\[ \\mathcal{L}_{\\text{ridge}}(\\beta) = \\frac{1}{n} \\| \\mathbf{y} - \\mathbf{X} \\beta \\|_2^2 + \\lambda \\| \\beta \\|_2^2 \\] <p>Where:</p> <ul> <li>\\(\\lambda &gt; 0\\) is the regularization strength</li> <li>\\(\\| \\beta \\|_2^2 = \\beta^\\top \\beta\\)</li> </ul>"},{"location":"linear_regression/#closed-form-solution-with-regularization","title":"Closed-Form Solution with Regularization","text":"<p>Taking the gradient and setting it to zero:</p> \\[ \\nabla_\\beta \\mathcal{L}_{\\text{ridge}}(\\beta) = \\frac{2}{n} \\mathbf{X}^\\top (\\mathbf{X} \\beta - \\mathbf{y}) + 2 \\lambda \\beta = 0 \\] <p>Leads to the regularized normal equation:</p> \\[ \\hat{\\beta}_{\\text{ridge}} = \\frac{1}{n} \\left( (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\right) \\] <p>This ensures invertibility even when \\(\\mathbf{X}^\\top \\mathbf{X}\\) is singular, due to the identity matrix \\(\\mathbf{I}\\) added to it.</p>"},{"location":"linear_regression/#gradient-descent-algorithms","title":"Gradient Descent Algorithms","text":"<p>Gradient Descent is an iterative optimization algorithm used to minimize the loss function by updating the parameters in the direction of the negative gradient.</p> <p>We update the parameter vector \\(\\beta\\) using the rule:</p> \\[ \\beta = \\beta - \\alpha \\cdot \\nabla_\\beta \\mathcal{L}(\\beta) \\] <p>Where: - \\(\\alpha\\) is the learning rate - \\(\\nabla_\\beta \\mathcal{L}(\\beta)\\) is the gradient of the loss function with respect to \\(\\beta\\)</p> <p>In this section, we describe three variations of the Gradient Descent (GD) algorithm used for training a linear regression model: Batch Gradient Descent (BGD), Stochastic Gradient Descent (SGD), and Mini-Batch Gradient Descent (MBGD).</p>"},{"location":"linear_regression/#1-batch-gradient-descent","title":"1. Batch Gradient Descent","text":"<p>Batch Gradient Descent computes the gradient using the entire dataset and updates the parameters at the end of each epoch.</p>"},{"location":"linear_regression/#input","title":"Input:","text":"<ul> <li>Dataset: \\(D = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^{n}\\) </li> <li>Learning rate: \\(\\epsilon\\) </li> <li>Number of epochs  </li> <li>Tolerance: <code>tol</code></li> </ul>"},{"location":"linear_regression/#output","title":"Output:","text":"<ul> <li>Optimized weight vector \\(\\beta\\)</li> </ul>"},{"location":"linear_regression/#algorithm","title":"Algorithm:","text":"<pre><code>1. Initialize \u03b2\u2080 = 0 \u2208 \u211d\u1d48\n2. for epoch in number of epochs do:\n    3. Shuffle the dataset D\n    4. Compute predictions: \u0177 = X \u03b2\u2080\n    5. Compute loss: L = (1/n) \u2016\u0177 \u2212 y\u2016\u00b2\n    6. Compute gradient: \u2207\u03b2f(\u03b2\u2080) = (2/n)  (X\u1d40X\u03b2\u2080 \u2212 X\u1d40y)\n    7. Update weights: \u03b2\u2081 = \u03b2\u2080 \u2212 \u03b5 \u2207\u03b2f(\u03b2\u2080)\n    8. if \u2016y \u2212 X \u03b2\u2081\u2016\u00b2 &lt; tol then:\n        9. break\n    10. \u03b2\u2080 = \u03b2\u2081\n</code></pre>"},{"location":"linear_regression/#2-stochastic-gradient-descent","title":"2. Stochastic Gradient Descent","text":"<p>Stochastic Gradient Descent updates the weights after processing each individual data point. This approach tends to converge faster, but the updates are noisy.</p>"},{"location":"linear_regression/#input_1","title":"Input:","text":"<ul> <li>Dataset: \\(D = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^{n}\\) </li> <li>Learning rate: \\(\\epsilon\\) </li> <li>Number of epochs  </li> <li>Tolerance: <code>tol</code></li> </ul>"},{"location":"linear_regression/#output_1","title":"Output:","text":"<ul> <li>Optimized weight vector \\(\\beta\\)</li> </ul>"},{"location":"linear_regression/#algorithm_1","title":"Algorithm:","text":"<pre><code>1. Initialize \u03b2\u2080 = 0 \u2208 \u211d\u1d48\n2. for epoch in number of epochs do:\n    3. Shuffle the dataset D\n    4. for i = 1 to n do:\n        5. Compute prediction for observation i: \u0177\u1d62 = x\u1d62\u1d40 \u03b2\u2080\n        6. Compute loss: L = (\u0177\u1d62 \u2212 y\u1d62)\u00b2\n        7. Compute gradient: \u2207\u03b2f(\u03b2\u2080) = 2 ( x\u1d62x\u1d62\u1d40 \u03b2\u2080 \u2212 x\u1d62y\u1d62 ) \n        8. Update weights: \u03b2\u2081 = \u03b2\u2080 \u2212 \u03b5 \u2207\u03b2f(\u03b2\u2080)\n        9. if (\u0177\u1d62 \u2212 y\u1d62)\u00b2 &lt; tol then:\n            10. break\n    11. \u03b2\u2080 = \u03b2\u2081\n</code></pre>"},{"location":"linear_regression/#3-stochastic-gradient-descent-with-momentum","title":"3. Stochastic Gradient Descent with Momentum","text":"<p>Momentum is an enhancement to standard SGD that helps accelerate gradient descent in the relevant direction and dampens oscillations. It is particularly useful in scenarios where the optimization landscape has high curvature, small but consistent gradients, or noisy updates.</p>"},{"location":"linear_regression/#motivation","title":"Motivation:","text":"<ul> <li>Variance in SGD: Helps smooth out noisy gradients by using an exponentially decaying moving average.</li> <li>Poor conditioning: Helps accelerate through narrow valleys and avoids getting stuck.</li> </ul>"},{"location":"linear_regression/#concept","title":"Concept:","text":"<p>We introduce a velocity vector \\(v\\) that accumulates the exponentially decaying average of past gradients:</p> <ul> <li>\\(v\\) is initialized to zero</li> <li>\\(\\beta\\) is the momentum coefficient (typically \\(\\beta \\in [0.9, 0.99]\\))</li> <li>\\(\\epsilon\\) is the learning rate</li> </ul>"},{"location":"linear_regression/#update-rule","title":"Update Rule:","text":"\\[ v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla_\\beta \\mathcal{L}(\\beta_{t-1}) \\] \\[ \\beta_t = \\beta_{t-1} - \\epsilon v_t \\] <p>This adds \u201cinertia\u201d to the parameter updates, similar to how momentum works in physics.</p>"},{"location":"linear_regression/#algorithm_2","title":"Algorithm:","text":"<p>Input:</p> <ul> <li>Dataset: \\(D = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^{n}\\) </li> <li>Learning rate: \\(\\epsilon\\) </li> <li>Momentum coefficient: \\(\\beta\\) </li> <li>Number of epochs  </li> <li>Tolerance: <code>tol</code></li> </ul> <p>Output:</p> <ul> <li>Optimized weight vector \\(\\beta\\)</li> </ul> <pre><code>1. Initialize \u03b2\u2080 = 0 \u2208 \u211d\u1d48\n2. Initialize velocity vector v = 0 \u2208 \u211d\u1d48\n3. for epoch in number of epochs do:\n    4. Shuffle the dataset D\n    5. for i = 1 to n do:\n        6. Compute prediction for observation i: \u0177\u1d62 = x\u1d62\u1d40 \u03b2\u2080\n        7. Compute gradient: \u2207\u03b2f(\u03b2\u2080) = 2 ( x\u1d62x\u1d62\u1d40 \u03b2\u2080 \u2212 x\u1d62y\u1d62 ) \n        8. Update velocity: v = \u03b2 \u22c5 v + (1 - \u03b2) \u22c5 \u2207\u03b2f(\u03b2\u2080)\n        9. Update weights: \u03b2\u2081 = \u03b2\u2080 \u2212 \u03b5 \u22c5 v\n        10. if (\u0177\u1d62 \u2212 y\u1d62)\u00b2 &lt; tol then:\n            11. break\n    12. \u03b2\u2080 = \u03b2\u2081\n</code></pre>"},{"location":"linear_regression/#4-mini-batch-gradient-descent","title":"4. Mini-Batch Gradient Descent","text":"<p>Mini-Batch Gradient Descent updates the weights after processing small batches of data, offering a balance between the computational efficiency of BGD and the faster convergence of SGD.</p>"},{"location":"linear_regression/#input_2","title":"Input:","text":"<ul> <li>Dataset: \\(D = \\{ (\\mathbf{x}_i, y_i) \\}_{i=1}^{n}\\) </li> <li>Learning rate: \\(\\epsilon\\) </li> <li>Number of epochs  </li> <li>Batch size: \\(B\\)</li> <li>Tolerance: <code>tol</code></li> </ul>"},{"location":"linear_regression/#output_2","title":"Output:","text":"<ul> <li>Optimized weight vector \\(\\beta\\)</li> </ul>"},{"location":"linear_regression/#algorithm_3","title":"Algorithm:","text":"<pre><code>1. Initialize \u03b2\u2080 = 0 \u2208 \u211d\u1d48\n2. for epoch in number of epochs do:\n    3. Shuffle the dataset D\n    4. Partition data into n_batches = ceil(n / B)\n    5. for j = 1 to n_batches do:\n        6. Get mini-batch (X\u2c7c, y\u2c7c)\n        7. Compute prediction: \u0177\u2c7c = X\u2c7c \u03b2\u2080\n        8. Compute loss: L = (1/B) \u2016\u0177\u2c7c \u2212 y\u2c7c\u2016\u00b2\n        9. Compute gradient: \u2207\u03b2f(\u03b2\u2080) = (2/B)  (X\u2c7c\u1d40X\u2c7c \u03b2\u2080 \u2212 X\u2c7c\u1d40y\u2c7c)\n        10. Update weights: \u03b2\u2081 = \u03b2\u2080 \u2212 \u03b5 \u2207\u03b2f(\u03b2\u2080)\n        11. if \u2016y\u2c7c \u2212 X\u2c7c \u03b2\u2081\u2016\u00b2 &lt; tol then:\n            12. break\n    13. \u03b2\u2080 = \u03b2\u2081\n</code></pre>"},{"location":"linear_regression/#conclusion","title":"Conclusion","text":"<p>Linear Regression is foundational in machine learning and serves as the basis for more complex models. By understanding how to train it using different optimization methods like GD, SGD, and mini-batch GD, one gains deep insights into how models learn from data.</p>"},{"location":"logistic_regression/","title":"Logistic Regression","text":""},{"location":"logistic_regression/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Data Representation</li> <li>Model Hypothesis</li> <li>Loss Function</li> <li>Gradient Of Loss Function</li> <li>Method 2: When \\(y \\in \\{-1, 1\\}\\)</li> <li>Conclusion</li> </ol>"},{"location":"logistic_regression/#introduction","title":"Introduction","text":"<p>Logistic regression models a linear relationship between input features \\(X\\) and a binary target vector \\(y\\). It applies the sigmoid function to the linear combination of inputs to produce an output between 0 and 1, which is then thresholded to make a binary class prediction. Matrix operations enable efficient computation and scalability to large datasets.</p>"},{"location":"logistic_regression/#data-representation","title":"Data Representation","text":"<p>Let:</p> <ul> <li>\\(n\\): number of training examples  </li> <li>\\(d\\): number of features  </li> </ul> <p>We define:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\): feature matrix  </li> <li>\\(\\mathbf{y} \\in \\{0,1\\}^n\\): target vector  </li> <li>\\(\\beta \\in \\mathbb{R}^{d \\times 1}\\): weight vector</li> </ul> \\[ \\mathbf{X} = \\begin{bmatrix} x_1^{(1)} &amp; x_2^{(1)} &amp; \\dots &amp; x_d^{(1)} \\\\ x_1^{(2)} &amp; x_2^{(2)} &amp; \\dots &amp; x_d^{(2)} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_1^{(n)} &amp; x_2^{(n)} &amp; \\dots &amp; x_d^{(n)} \\end{bmatrix} \\]"},{"location":"logistic_regression/#model-hypothesis","title":"Model Hypothesis","text":"<p>In logistic regression, we compute a linear combination of input features and model parameters, and then apply the sigmoid function to make the result into the range \\([0, 1]\\). The hypothesis function is:</p> \\[ \\hat{\\mathbf{y}} = \\sigma(\\mathbf{X} \\beta) \\] <p>where the sigmoid function \\(\\sigma(z)\\) is defined as:</p> \\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]"},{"location":"logistic_regression/#loss-function","title":"Loss Function","text":"<p>To derive the loss function for logistic regression, we use maximum likelihood estimation (MLE), assuming that each target label is drawn from a Bernoulli distribution.</p>"},{"location":"logistic_regression/#assumption-bernoulli-targets","title":"Assumption: Bernoulli Targets","text":"<p>We assume that the training dataset \\(\\{(x_i, y_i)\\}_{i=1}^n\\) are independent and identically distributed (iid), and that the target \\(y_{i} \\in \\{0, 1\\}\\) follows a Bernoulli distribution parameterized by \\(\\theta_i\\):</p> \\[ y_{i} \\sim \\text{Bernoulli}(\\theta_i) \\] <p>where \\(\\theta_i = \\hat{y}_{i} = \\sigma(\\beta^\\top x_{i})\\). The probability mass function of the Bernoulli distribution is:</p> \\[ P(y_{i}\\;\\theta_i) = \\theta_{i}^{y_{i}} (1 - \\theta_{i})^{1 - y_{i}} \\quad \\text{ where } 0\\le\\theta_i \\le 1, y_{i} = 0,1 \\] <p>Now computing the likelihood \\(L(\\beta)\\):</p> \\[ \\begin{align*} L(\\beta) &amp;= \\prod_{i=1}^n P(y_{i}; \\theta_i) \\quad \\text{ because of iid}\\\\            &amp;= \\prod_{i=1}^n \\theta_{i}^{y_{i}} (1 - \\theta_{i})^{1 - y_{i}}\\\\           &amp;= \\theta_{i}^{\\sum_{i=1}^n y_{i} } (1 - \\theta_{i})^{\\sum_{i=1}^n (1 - y_{i})} \\end{align*} \\] <p>Next we computing the Log-Likelihood (\\(\\log L(\\beta)\\)),</p> \\[ \\begin{align*} \\log L(\\beta) &amp;=\\log \\left(\\theta_{i}^{\\sum_{i=1}^n y_{i} } (1 - \\theta_{i})^{\\sum_{i=1}^n (1 - y_{i})}\\right)\\\\                &amp;= \\sum_{i=1}^n y_{i} \\log \\theta_{i} + \\sum_{i=1}^n (1 - y_{i}) \\log (1 - \\theta_{i})\\\\                &amp;= \\sum_{i=1}^n  \\left(y_{i} \\log \\theta_{i} + (1 - y_{i}) \\log (1 - \\theta_{i}) \\right) \\end{align*} \\] <p>Finding the negative of Log-Likelihood \\(-\\log L(\\beta)\\) we  have our final loss function as Negative-Log-Likelihood \\(NLL\\) given as:</p> \\[ NLL = - \\sum_{i=1}^n \\left( y_{i} \\log \\theta_{i} + (1 - y_{i}) \\log (1 - \\theta_{i})\\right) \\] <p>Since \\(0\\le\\theta_{i} \\le 1\\) then we can use a sigmoid function to generate it hence we have \\(\\theta_{i} = \\hat{y}_{i} = \\sigma(\\beta^\\top x_{i})\\). And finally we have a loss function known as binary cross entropy given as </p> \\[ NLL = - \\sum_{i=1}^n \\left( y_{i} \\log \\sigma(z_{i}) + (1 - y_{i}) \\log (1 - \\sigma(z_{i}))\\right) \\quad \\text{ where } \\sigma(z_{i}) = \\frac{1}{1 + e^{-z_{i}}}, z_{i} = \\beta^\\top x_{i} \\] \\[ \\boxed{\\text{binary cross entropy} = - \\sum_{i=1}^n\\left(y_{i} \\log \\sigma(z_{i}) + (1 - y_{i}) \\log (1 - \\sigma(z_{i}))\\right)} \\]"},{"location":"logistic_regression/#gradient-of-the-loss-function","title":"Gradient of the Loss Function","text":"<p>Now we want to find the gradient of our loss function Negative-Log-Likelihood \\(NLL\\) using chain rule i.e. $$ L = - \\sum_{i=1}^n\\left(y_{i} \\log \\sigma(z_{i}) + (1 - y_{i}) \\log (1 - \\sigma(z_{i}))\\right) $$ $$ \\frac{d L}{d \\beta} = \\frac{d L}{dz_{i}} \\times \\frac{d z_{i}}{d \\beta} $$</p>"},{"location":"logistic_regression/#note","title":"NOTE","text":"<p>$$ \\sigma(z_{i}) = \\frac{1}{1 + e^{-z_{i}}}, \\quad \\text{where } z_{i} = \\beta^\\top x_{i} $$ To find the derivative of \\(\\sigma(z_{i})\\) with respect to \\(z_{i}\\), we have:</p> \\[ \\boxed{ \\begin{aligned} \\frac{d\\sigma}{dz_{i}} &amp;= \\frac{d}{dz_{i}} \\left( \\frac{1}{1 + e^{-z_{i}}} \\right) \\\\ &amp;= \\frac{d}{dz_{i}} \\left( (1 + e^{-z_{i}})^{-1} \\right) \\\\ &amp;= -1 \\cdot (1 + e^{-z_{i}})^{-2} \\cdot \\frac{d}{dz_{i}}(1 + e^{-z_{i}}) \\\\ &amp;= - (1 + e^{-z_{i}})^{-2} \\cdot (-e^{-z_{i}}) \\\\ &amp;= \\frac{e^{-z_{i}}}{(1 + e^{-z_{i}})^2} \\\\ \\\\ \\text{Now recall:} \\quad \\sigma(z_{i}) &amp;= \\frac{1}{1 + e^{-z_{i}}} \\quad \\Rightarrow \\quad 1 - \\sigma(z_{i}) = \\frac{e^{-z_{i}}}{1 + e^{-z_{i}}} \\\\ \\\\ \\text{So:} \\quad \\frac{d\\sigma}{dz_{i}} &amp;= \\sigma(z_{i})(1 - \\sigma(z_{i})) \\end{aligned} } \\] <p>Now continueing with the derivative of our binary cross entropy loss \\(L\\) w.r.t \\(z_{i}\\) we have</p> \\[ \\begin{aligned} \\frac{dL}{dz_{i}} &amp;= -\\left(y_{i}\\cdot \\frac{1}{\\sigma(z_{i})} \\cdot \\sigma(z_{i})(1 - \\sigma(z_{i}))  + (1 - y_{i}) \\cdot \\frac{1}{(1 - \\sigma(z_{i}))} \\cdot -\\sigma(z_{i})(1 - \\sigma(z_{i}))\\right)\\\\ &amp;= - \\left[y_{i}(1 - \\sigma(z_{i}))  - (1 - y_{i}) \\sigma(z_{i})\\right]\\\\ &amp;= - \\left[y_{i} - y_{i}\\sigma(z_{i})  - \\sigma(z_{i}) + y_{i}\\sigma(z_{i})\\right]\\\\ &amp;= - \\left[y_{i}  - \\sigma(z_{i})\\right]\\\\ &amp;= \\sigma(z_{i})-y_{i} \\end{aligned} \\] <p>Next we solve for \\(\\frac{d z_{i}}{d \\beta}\\)</p> <p>\\(z_{i} = \\beta^\\top x_{i}\\)</p> <p>\\(\\frac{d z_{i}}{d \\beta} = x_{i}\\)</p> <p>Therefore we have our final gradient as</p> \\[ \\begin{align*} \\frac{d L}{d \\beta} &amp;= \\frac{d L}{dz_{i}} \\times \\frac{d z_{i}}{d \\beta}\\\\                     &amp;= \\sum_{i=1}^n\\left(\\sigma(z_{i})-y_{i}\\right) x_{i} \\end{align*} \\] <p>The gradient in vectorized form is:</p> \\[ \\boxed{ \\frac{d L}{d \\beta} = \\sum_{i=1}^n \\left( \\sigma(z_i) - y_i \\right) x_i } \\] <p>When $ X $ is a data matrix, the gradient becomes:</p> \\[ \\boxed{ \\frac{d L}{d \\beta} = X^\\top \\left( \\sigma(X\\beta) - y \\right) } \\]"},{"location":"logistic_regression/#method-2-when-y-in-1-1","title":"Method 2: When \\(y \\in \\{-1, 1\\}\\)","text":"<p>In some variants, target labels are represented using \\(-1\\) and \\(+1\\). The logistic regression model and loss function can be adjusted accordingly.</p> <p>Now we are going to derive the hypothesis, loss function, and the gradient.</p>"},{"location":"logistic_regression/#hypothesis","title":"Hypothesis","text":"<p>Let</p> \\[ \\mathcal{D} = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^{n} \\quad \\text{where } x_i \\in \\mathbb{R}^{d},\\quad y_i \\in \\{-1,1\\},\\quad w \\in \\mathbb{R}^{d} \\] <p>and \\(\\hat{y} = w^\\top x\\)</p> <p>Assuming \\(P(Y=-1\\mid X)\\) and \\(P(Y=1\\mid X)\\) are defined, then</p> \\[ \\frac{P(Y=1\\mid X)}{P(Y=-1\\mid X)} \\in (0,\\infty) \\] <p>Now introducing the logarithm, we have</p> \\[ \\log \\left(\\frac{P(Y=1\\mid X)}{P(Y=-1\\mid X)} \\right) = w^\\top x \\in \\mathbb{R} \\] <p>This is equivalent to</p> \\[ \\begin{align*} &amp;\\frac{P(Y=1\\mid X)}{P(Y=-1\\mid X)} = e^{w^\\top x}\\\\ &amp; P(Y=1\\mid X) = \\left[1-P(Y=1\\mid X)\\right]e^{w^\\top x} \\quad \\text{since } P(Y=1\\mid X) + P(Y=-1\\mid X) =1\\\\ &amp; P(Y=1\\mid X)\\left[1+e^{w^\\top x}\\right] = e^{w^\\top x}\\\\ &amp; P(Y=1\\mid X) = \\frac{e^{w^\\top x}}{1+e^{w^\\top x}} =  \\frac{1}{1+e^{-w^\\top x}} \\end{align*} \\] <p>Therefore we have</p> \\[ P(Y=1\\mid X) = \\frac{1}{1+e^{-w^\\top x}} = \\sigma(w^\\top x) \\] <p>And</p> \\[ P(Y=-1\\mid X) = \\frac{1}{1+e^{w^\\top x}} = \\sigma(-w^\\top x) \\] <p>Therefore, we can conclude our Hypothesis as</p> \\[ \\boxed{P(Y=y\\mid X) = \\frac{1}{1+e^{-yw^\\top x}} = \\sigma(yw^\\top x)} \\] <p>where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function.</p>"},{"location":"logistic_regression/#loss-function_1","title":"Loss Function","text":"<p>From Maximum Likelihood Estimation (MLE), the negative log-likelihood is given as</p> \\[ NLL(w) = -\\sum_{i=1}^{n} \\log P(y_i \\mid x_i; w) \\] <p>From the hypothesis derived earlier, we have</p> \\[ P(y_i \\mid x_i; w) = \\frac{1}{1 + e^{-y_i (w^\\top x_i)}} \\] <p>Now solving for \\(NLL(w)\\):</p> \\[ \\begin{align*} NLL(w) &amp;= -\\sum_{i=1}^{n} \\log \\left(\\frac{1}{1 + e^{-y_i (w^\\top x_i)}}\\right)\\\\        &amp;= -\\sum_{i=1}^{n} \\log \\left(1 + e^{-y_i (w^\\top x_i)}\\right)^{-1}\\\\        &amp;= \\sum_{i=1}^{n} \\log \\left(1 + e^{-y_i (w^\\top x_i)}\\right) \\end{align*} \\] <p>Therefore, our Loss Function is given as</p> \\[ \\boxed{\\mathcal{L}(w) = \\sum_{i=1}^{n} \\log \\left(1 + e^{-y_i (w^\\top x_i)}\\right)} \\]"},{"location":"logistic_regression/#gradient","title":"Gradient","text":"<p>Now we find the gradient of our loss function \\(\\mathcal{L}(w)\\) with respect to \\(w\\):</p> \\[ \\begin{align*} \\frac{d \\mathcal{L}(w)}{d w} &amp;= \\sum_{i=1}^{n} \\frac{d}{d w} \\log \\left(1 + e^{-y_i (w^\\top x_i)}\\right) \\\\ &amp;= \\sum_{i=1}^{n} \\frac{1}{1 + e^{-y_i (w^\\top x_i)}} \\cdot \\left(-e^{-y_i (w^\\top x_i)} \\cdot y_i x_i\\right) \\\\ &amp;= -\\sum_{i=1}^{n} \\left( \\frac{e^{-y_i (w^\\top x_i)}}{1 + e^{-y_i (w^\\top x_i)}} \\right) y_i x_i \\\\ &amp;= -\\sum_{i=1}^{n} \\left(1 - \\frac{1}{1 + e^{-y_i (w^\\top x_i)}} \\right) y_i x_i \\\\ &amp;= -\\sum_{i=1}^{n} \\left(1 - \\sigma(y_i w^\\top x_i)\\right) y_i x_i \\end{align*} \\] <p>where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function.</p> <p>Therefore our gradient \\(\\nabla_w L\\)  is given as; $$ \\boxed{ \\nabla_w L = -\\sum_{i=1}^{n} \\left(1 - \\sigma(y_i w^\\top x_i)\\right) y_i x_i } $$</p>"},{"location":"logistic_regression/#conclusion","title":"Conclusion","text":"<p>Here\u2019s a table comparing the two formulations of logistic regression based on the label encoding: \\(y \\in {0, 1}\\) vs. \\(y \\in {-1, 1}\\).</p> Feature \\(y \\in {0, 1}\\) \\(y \\in {-1, 1}\\) Label Encoding 0 for negative class, 1 for positive class -1 for negative class, +1 for positive class Hypothesis Function \\(\\hat{y} = \\sigma(\\beta^\\top x)\\) \\(P(y \\mid x) = \\sigma(y \\cdot w^\\top x)\\) Sigmoid Argument \\(\\sigma(z) = \\frac{1}{1 + e^{-\\beta^\\top x}}\\) \\(\\sigma(y \\cdot w^\\top x)\\) Loss Function \\(-\\sum_{i=1}^n \\left[y_i \\log \\sigma(z_i) + (1-y_i) \\log(1-\\sigma(z_i))\\right]\\) \\(\\sum_{i=1}^n \\log(1 + e^{-y_i (w^\\top x_i)})\\) Loss Name Binary Cross-Entropy Log-Loss (equivalent in nature, different form) Gradient of Loss Function \\(\\nabla_\\beta = X^\\top(\\sigma(X\\beta) - y)\\) \\(\\nabla_w = -\\sum_{i=1}^n (1 - \\sigma(y_i w^\\top x_i)) y_i x_i\\) <p>Logistic regression models binary outcomes using a sigmoid function over a linear combination of features. We derived the binary cross-entropy loss using maximum likelihood and computed its gradient using the chain rule. The model can be expressed for both \\(y \\in \\{0, 1\\}\\) and \\(y \\in \\{-1, 1\\}\\), with slightly different forms of the loss function and gradient. The final vectorized gradient enables efficient optimization using gradient descent.</p>"},{"location":"loss_function/","title":"Loss Functions","text":""},{"location":"loss_function/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Mean Squared Error (MSE)</li> <li>Mean Absolute Error (MAE)</li> <li>Huber Loss</li> <li>Binary Cross-Entropy Loss</li> <li>Categorical Cross-Entropy Loss</li> <li>Kullback-Leibler Divergence (KL Divergence)</li> <li>Hinge Loss</li> <li>Contrastive Loss</li> <li>Triplet Loss</li> <li>Conclusion</li> </ol>"},{"location":"loss_function/#introduction","title":"Introduction","text":"<p>Loss functions (also called cost functions or objective functions) quantify the difference between predicted outputs and true targets. During training, neural networks use loss functions to guide weight updates using optimization algorithms like Gradient Descent. A good choice of loss function ensures meaningful learning and faster convergence.</p> <p>Given:</p> <ul> <li>Predictions: \\(\\hat{y}\\)</li> <li>True values: \\(y\\)</li> <li>Loss: \\(L(y, \\hat{y})\\)</li> </ul> <p>The loss function is minimized during training.</p>"},{"location":"loss_function/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Used in regression tasks.</p>"},{"location":"loss_function/#definition","title":"Definition","text":"\\[ L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\]"},{"location":"loss_function/#properties","title":"Properties","text":"<ul> <li>Penalizes large errors more heavily than small errors.</li> <li>Sensitive to outliers.</li> <li>Smooth and differentiable.</li> </ul>"},{"location":"loss_function/#derivative","title":"Derivative","text":"\\[ \\frac{\\partial L}{\\partial \\hat{y}_i} = -2(y_i - \\hat{y}_i) \\]"},{"location":"loss_function/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Also for regression tasks.</p>"},{"location":"loss_function/#definition_1","title":"Definition","text":"\\[ L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| \\]"},{"location":"loss_function/#properties_1","title":"Properties","text":"<ul> <li>More robust to outliers than MSE.</li> <li>Not differentiable at \\(y = \\hat{y}\\) (but subgradients are used).</li> <li>Slower convergence compared to MSE.</li> </ul>"},{"location":"loss_function/#derivative-subgradient","title":"Derivative (subgradient)","text":"\\[ \\frac{\\partial L}{\\partial \\hat{y}_i} = \\begin{cases} -1 &amp; \\text{if } y_i - \\hat{y}_i &gt; 0 \\\\ 1 &amp; \\text{if } y_i - \\hat{y}_i &lt; 0 \\end{cases} \\]"},{"location":"loss_function/#huber-loss","title":"Huber Loss","text":"<p>Combines MSE and MAE to be robust and smooth.</p>"},{"location":"loss_function/#definition_2","title":"Definition","text":"\\[ L_\\delta(y, \\hat{y}) = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 &amp; \\text{if } |y - \\hat{y}| \\leq \\delta \\\\ \\delta \\left( |y - \\hat{y}| - \\frac{1}{2}\\delta \\right) &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"loss_function/#properties_2","title":"Properties","text":"<ul> <li>Quadratic for small errors, linear for large errors.</li> <li>Reduces sensitivity to outliers.</li> </ul>"},{"location":"loss_function/#binary-cross-entropy-loss","title":"Binary Cross-Entropy Loss","text":"<p>Used in binary classification.</p>"},{"location":"loss_function/#definition_3","title":"Definition","text":"\\[ L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y)\\log(1 - \\hat{y})] \\] <p>Where \\(y \\in {0, 1}\\) and \\(\\hat{y} \\in (0, 1)\\).</p>"},{"location":"loss_function/#properties_3","title":"Properties","text":"<ul> <li>Penalizes confident incorrect predictions.</li> <li>Assumes outputs are probabilities (usually after Sigmoid).</li> </ul>"},{"location":"loss_function/#derivative_1","title":"Derivative","text":"\\[ \\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} \\]"},{"location":"loss_function/#categorical-cross-entropy-loss","title":"Categorical Cross-Entropy Loss","text":"<p>Used in multi-class classification with one-hot encoded targets.</p>"},{"location":"loss_function/#definition_4","title":"Definition","text":"<p>Given:</p> <ul> <li>True label vector \\(y \\in \\{0, 1\\}^C\\) (one-hot encoded)</li> <li>Predicted probabilities \\(\\hat{y} = \\text{softmax}(z)\\)</li> </ul> \\[ L(y, \\hat{y}) = -\\sum_{i=1}^C y_i \\log(\\hat{y}_i) \\] <p>Where:</p> <ul> <li>\\(C\\) = number of classes</li> <li>\\(y_i = 1\\) for the correct class, $0$ otherwise</li> </ul>"},{"location":"loss_function/#with-softmax-output","title":"With Softmax Output:","text":"<p>Let:</p> \\[ \\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}} \\quad \\text{(Softmax)} \\]"},{"location":"loss_function/#properties_4","title":"Properties","text":"<ul> <li>Used with Softmax outputs.</li> <li>Measures log loss over multiple categories.</li> </ul>"},{"location":"loss_function/#derivative-of-loss-wrt-input-z_i","title":"Derivative of Loss w.r.t. Input \\(z_i\\)","text":"<p>We want to compute the derivative of the loss with respect to the logits \\(z_k\\), where \\(\\hat{y}_k = \\text{softmax}(z_k)\\).</p>"},{"location":"loss_function/#step-by-step","title":"Step-by-Step:","text":"\\[ \\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^C \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_k} \\] <p>We already know:</p> \\[ \\frac{\\partial L}{\\partial \\hat{y}_i} = -\\frac{y_i}{\\hat{y}_i} \\] <p>Now, for softmax:</p> \\[ \\frac{\\partial \\hat{y}_i}{\\partial z_k} = \\begin{cases} \\hat{y}_i(1 - \\hat{y}_i) &amp; \\text{if } i = k \\\\ -\\hat{y}_i \\hat{y}_k &amp; \\text{if } i \\ne k \\end{cases} \\] <p>Putting this together:</p> \\[ \\frac{\\partial L}{\\partial z_k} = \\hat{y}_k - y_k \\] <p>This is more common and efficient: when using categorical cross-entropy with softmax, the gradient simplifies greatly, which is why they are often combined in frameworks like TensorFlow (<code>SoftmaxCrossEntropyWithLogits</code>) and PyTorch (<code>CrossEntropyLoss</code>).</p>"},{"location":"loss_function/#kullback-leibler-divergence-kl-divergence","title":"Kullback-Leibler Divergence (KL Divergence)","text":"<p>Measures how one probability distribution diverges from a second expected distribution.</p>"},{"location":"loss_function/#definition_5","title":"Definition","text":"\\[ D_{KL}(P \\parallel Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)} \\] <p>Where:</p> <ul> <li>\\(P\\): true distribution</li> <li>\\(Q\\): predicted distribution</li> </ul>"},{"location":"loss_function/#properties_5","title":"Properties","text":"<ul> <li>Asymmetric: \\(D\\_{KL}(P \\parallel Q) \\neq D\\_{KL}(Q \\parallel P)\\)</li> <li>Used in VAEs (Variational Autoencoders), NLP models</li> </ul>"},{"location":"loss_function/#hinge-loss","title":"Hinge Loss","text":"<p>Used in SVMs and \u201cmaximum-margin\u201d classifiers.</p>"},{"location":"loss_function/#definition_6","title":"Definition","text":"\\[ L(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y}) \\] <p>Where:</p> <ul> <li>\\(y \\in {-1, +1}\\)</li> </ul>"},{"location":"loss_function/#properties_6","title":"Properties","text":"<ul> <li>Encourages correct classification with a margin.</li> <li>Only penalizes incorrect or borderline predictions.</li> </ul>"},{"location":"loss_function/#contrastive-loss","title":"Contrastive Loss","text":"<p>Used in Siamese Networks (e.g., face recognition).</p>"},{"location":"loss_function/#definition_7","title":"Definition","text":"\\[ L = (1 - y) \\cdot \\frac{1}{2} D^2 + y \\cdot \\frac{1}{2} \\max(0, m - D)^2 \\] <p>Where:</p> <ul> <li>\\(D\\): Euclidean distance between feature embeddings</li> <li>\\(y = 0\\) for similar pairs, \\(1\\) for dissimilar</li> <li>\\(m\\): margin</li> </ul>"},{"location":"loss_function/#triplet-loss","title":"Triplet Loss","text":"<p>Used in ranking tasks (e.g., face verification).</p>"},{"location":"loss_function/#definition_8","title":"Definition","text":"\\[ L = \\max(0, d(a, p) - d(a, n) + \\alpha) \\] <p>Where:</p> <ul> <li>\\(a\\) = anchor</li> <li>\\(p\\) = positive example</li> <li>\\(n\\) = negative example</li> <li>\\(\\alpha\\) = margin</li> </ul>"},{"location":"loss_function/#properties_7","title":"Properties","text":"<ul> <li>Encourages anchor-positive pairs to be closer than anchor-negative pairs by a margin \\(\\alpha\\).</li> </ul>"},{"location":"loss_function/#conclusion","title":"Conclusion","text":"<p>Loss functions guide learning by quantifying prediction errors. Choosing the right loss function depends on the problem type:</p> Task Type Common Loss Functions Regression MSE, MAE, Huber Binary Classification Binary Cross-Entropy Multi-class Classification Categorical Cross-Entropy, Sparse Cross-Entropy Embedding / Similarity Contrastive, Triplet Structured / Probabilistic KL Divergence, Hinge <p>Proper loss function choice can significantly improve convergence, performance, and generalization.</p>"},{"location":"neural_network/","title":"Neural Networks","text":""},{"location":"neural_network/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Data Representation</li> <li>Neural Network Architecture</li> <li>Forward Propagation</li> <li>Loss Function</li> <li>Backward Propagation</li> <li>Gradient Descent and Optimization</li> <li>Vanishing and Exploding Gradient </li> <li>Conclusion</li> </ol>"},{"location":"neural_network/#introduction","title":"Introduction","text":"<p>A neural network is a machine learning model inspired by the human brain that maps input data to output predictions by learning weights through layers of interconnected nodes (neurons). Neural networks are capable of modeling complex non-linear decision boundaries by composing multiple linear transformations and non-linear activation functions.</p> <p>A feedforward neural network is made up of:</p> <ul> <li>An input layer that takes the features.</li> <li>One or more hidden layers that learn intermediate representations.</li> <li>An output layer that produces predictions.</li> </ul> <p>Each layer applies a linear transformation followed by a non-linear activation function.</p>"},{"location":"neural_network/#data-representation","title":"Data Representation","text":"<p>Let:</p> <ul> <li>\\(n\\): number of training examples</li> <li>\\(d\\): number of features</li> <li>\\(X \\in \\mathbb{R}^{d \\times n}\\): input matrix</li> <li>\\(y \\in \\mathbb{R}^{1 \\times n}\\): target vector for regression or \\(y \\in {0, 1}^{1\\times n}\\) for binary classification</li> </ul> <p>For a 2-layer neural network (i.e., one hidden layer), we define:</p> <ul> <li>\\(W^{[1]} \\in \\mathbb{R}^{h \\times d}\\): weights for hidden layer</li> <li>\\(b^{[1]} \\in \\mathbb{R}^{h \\times 1}\\): bias for hidden layer</li> <li>\\(W^{[2]} \\in \\mathbb{R}^{1 \\times h}\\): weights for output layer</li> <li>\\(b^{[2]} \\in \\mathbb{R}\\): bias for output layer</li> </ul>"},{"location":"neural_network/#neural-network-architecture","title":"Neural Network Architecture","text":"<p>A neural network is made of interconnected neurons. Each of them is characterized by its weight, bias and activation function.</p> <p>Here are other elements of this network.</p> <p>Input Layer</p> <p>The input layer takes raw input from the domain. No computation is performed at this layer. Nodes here just pass on the information (features) to the hidden layer. </p> <p>Hidden Layer</p> <p>As the name suggests, the nodes of this layer are not exposed. They provide an abstraction to the neural network. </p> <p>The hidden layer performs all kinds of computation on the features entered through the input layer and transfers the result to the output layer.</p> <p>Output Layer</p> <p>It\u2019s the final layer of the network that brings the information learned through the hidden layer and delivers the final value as a result. </p> <p>Here we are going to use an example of a 2-layer feedforward neural network with:</p> <ul> <li>Input layer (\\(d\\) neurons)</li> <li>Hidden layer (\\(h_1\\) neurons)</li> <li>Output layer (\\(h_2\\) neurons)</li> </ul> <p>Assuming our task is binary clasification and we use \\(\\sigma\\) (sigmoid) activation function on both layers.</p> <p>Now let\u2019s see how our data and parameters are represented.</p> <ul> <li> <p>Data:</p> <ul> <li>Features: \\(X \\in \\mathbb{R}^{d\\times n}\\) which is the transpose of our original input data.</li> <li>Targets: \\(Y \\in \\mathbb{R}^{h_2\\times n}\\) which is the transpose of our original target data.</li> </ul> </li> <li> <p>Layer 1:</p> <ul> <li>Weight: \\(W_1 \\in \\mathbb{R}^{h_1\\times d}\\)</li> <li>Bias: \\(b_1 \\in \\mathbb{R}^{h_1\\times 1}\\)</li> </ul> </li> <li> <p>Layer 2:</p> <ul> <li>Weight: \\(W_2 \\in \\mathbb{R}^{h_2\\times h_1}\\)</li> <li>Bias: \\(b_2 \\in \\mathbb{R}^{h_2\\times 1}\\)</li> </ul> </li> <li> <p>Loss Function: Since this is a binary classification problem, we use binary cross entrpy. $$ \\mathcal{L} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$</p> </li> </ul>"},{"location":"neural_network/#forward-propagation","title":"Forward Propagation","text":"<p>The Feedforward Propagation, also called Forward Pass, is the process consisting of computing all network nodes\u2019 output values, starting with the first hidden layer until the last output layer, using at start either a subset or the entire dataset samples.</p> <p>Layer 1</p> <ul> <li>linear transformation  $$ Z_1 = W_1X+b_1 \\quad \\text{where } Z_1 \\in \\mathbb{R}^{h_1\\times n} $$</li> <li>non-linear transformation with \\(\\sigma\\) activation function $$ A_1 = \\sigma (Z_1) \\quad \\text{where } A_1 \\in \\mathbb{R}^{h_1\\times n} $$</li> <li>Number of parameters in layer 1 is given as: \\((h_1 \\times d) + h_1 = h_1(d + 1)\\).</li> </ul> <p>Layer 2</p> <ul> <li>linear transformation  $$ Z_2 = W_2A_1+b_2 \\quad \\text{where } Z_2 \\in \\mathbb{R}^{h_2\\times n} $$</li> <li>non-linear transformation with \\(\\sigma\\) activation function $$ A_2 = \\sigma (Z_2) \\quad \\text{where } A_2 \\in \\mathbb{R}^{h_2\\times n} $$</li> <li>Number of parameters in layer 2 is given as: \\((h_2 \\times h_1) + h_2 = h_2(h_1+1)\\).</li> </ul> <p>Output Layer</p> <ul> <li>Output    $$    A_2 = \\hat{Y} \\in \\mathbb{R}^{h_2\\times n}    $$</li> </ul>"},{"location":"neural_network/#loss-function","title":"Loss Function","text":"<p>For binary classification, we use binary cross-entropy loss:</p> \\[ \\mathcal{L} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\] <p>Matrix form:</p> \\[ \\mathcal{L} = - \\frac{1}{n} \\left[ Y \\log(A_2) + (1 - Y) \\log(1 - A_2) \\right] \\]"},{"location":"neural_network/#backward-propagation","title":"Backward Propagation","text":"<p>Backpropagation, or backward propagation of errors, is an algorithm working from the output nodes to the input nodes of a neural network using the chain rule to compute how much each activation unit contributed to the overall error.</p> <p>Backpropagation automatically computes error gradients to then repeatedly adjust all weights and biases to reduce the overall error.</p> <p>From our example our aim is to find </p> \\[ \\displaystyle \\frac{\\partial L}{\\partial A_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial Z_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial W_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial b_2}, \\quad \\displaystyle \\frac{\\partial L}{\\partial A_1}, \\quad \\displaystyle \\frac{\\partial L}{\\partial Z_1}, \\quad \\displaystyle \\frac{\\partial L}{\\partial W_1} \\quad \\text{ and } \\displaystyle \\frac{\\partial L}{\\partial b_1} \\] <p>Note that we are using our loss and binary cross entropy. But since we want to find its partial derivative w.r.t \\(A_2\\) we will modify it to be $$ L =  - \\frac{1}{n} \\left[ Y \\log(A_2) + (1 - Y) \\log(1 - A_2) \\right] $$</p> <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial A_2}\\) $$ \\displaystyle \\frac{\\partial L}{\\partial A_2} =  \\frac{1}{n} \\left[\\frac{A_2-Y}{A_2(1-A_2)}\\right] \\in \\mathbb{R}^{h_2 \\times n} $$</li> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial Z_2} = \\displaystyle \\frac{\\partial L}{\\partial A_2} \\times \\displaystyle \\frac{\\partial A_2}{\\partial Z_2}\\) $$  \\displaystyle \\frac{\\partial A_2}{\\partial Z_2} = \\sigma(Z_2)(1-\\sigma(Z_2))= A_2(1-A_2) \\in \\mathbb{R}^{h_2 \\times n} \\quad \\text{ since }\\sigma(Z_2) = A_2\\ $$ Therefore we have,</li> </ul> \\[ \\begin{align*} \\displaystyle \\frac{\\partial L}{\\partial Z_2} &amp;= \\displaystyle \\frac{\\partial L}{\\partial A_2} \\times \\displaystyle \\frac{\\partial A_2}{\\partial Z_2}\\\\                                               &amp;=  \\frac{1}{n} \\left[\\frac{A_2-Y}{A_2(1-A_2)}\\right] \\times A_2(1-A_2)\\\\                                               &amp;= \\frac{1}{n} \\left[A_2-Y\\right] \\in \\mathbb{R}^{h_2\\times n} \\end{align*} \\] <ul> <li> <p>\\(\\displaystyle \\frac{\\partial L}{\\partial W_2} = \\displaystyle \\frac{\\partial L}{\\partial Z_2} \\times \\displaystyle \\frac{\\partial Z_2}{\\partial W_2}\\) $$  \\frac{\\partial Z_2}{\\partial W_2} = A_1^\\top \\in \\mathbb{R}^{h_2\\times h_1} $$ Therefore we have, $$ \\displaystyle \\frac{\\partial L}{\\partial W_2}  = \\frac{1}{n} \\left[A_2-Y\\right]A_1^\\top \\in \\mathbb{R}^{h_2 \\times h_1} $$</p> </li> <li> <p>\\(\\displaystyle \\frac{\\partial L}{\\partial b_2} = \\displaystyle \\frac{\\partial L}{\\partial Z_2} \\times \\displaystyle \\frac{\\partial Z_2}{\\partial b_2}\\) $$  \\frac{\\partial Z_2}{\\partial b_2} = I \\quad \\text{Identity} $$ Therefore we have,</p> </li> </ul> \\[ \\displaystyle \\frac{\\partial L}{\\partial b_2}  = \\frac{1}{n} \\left[A_2-Y\\right] \\in \\mathbb{R}^{h_2 \\times n} \\quad \\text{ but } \\displaystyle \\frac{\\partial L}{\\partial b_2} \\in \\mathbb{R}^{h_2 \\times 1} \\quad \\text{So, we will sum over the second dimension.} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial A_1} = \\displaystyle \\frac{\\partial L}{\\partial Z_2} \\times \\displaystyle \\frac{\\partial Z_2}{\\partial A_1}\\) $$  \\displaystyle \\frac{\\partial Z_2}{\\partial A_1} =  W_2^\\top  \\in \\mathbb{R}^{h_1\\times h_2} $$ Therefore we have,</li> </ul> \\[ \\displaystyle \\frac{\\partial L}{\\partial A_1} =  \\frac{1}{n} \\left[A_2-Y\\right] W_2^\\top \\in \\mathbb{R}^{h_1\\times n} \\] <ul> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial Z_1} = \\displaystyle \\frac{\\partial L}{\\partial A_1} \\times \\displaystyle \\frac{\\partial A_1}{\\partial Z_1}\\) $$  \\displaystyle \\frac{\\partial A_1}{\\partial Z_1} = \\underbrace{\\sigma(Z_1)(1-\\sigma(Z_1))}_{\\text{element-wise multip.}} \\in \\mathbb{R}^{h_1\\times n} $$ Therefore we have,</li> </ul> \\[ \\begin{align*} \\displaystyle \\frac{\\partial L}{\\partial Z_1} &amp;=  \\frac{1}{n} \\left[A_2-Y\\right] W_2^\\top (\\sigma(Z_1)(1-\\sigma(Z_1))) \\quad \\text{ Due to dimentionality we change to}\\\\                                               &amp;=   \\frac{1}{n} \\underbrace{\\underbrace{W_2^\\top \\left[A_2-Y\\right]}_{\\text{matrix multip.}} (\\sigma(Z_1)(1-\\sigma(Z_1)))}_{\\text{element-wise multip.}} \\in \\mathbb{R}^{h_1\\times n} \\end{align*} \\] <ul> <li> <p>\\(\\displaystyle \\frac{\\partial L}{\\partial W_1} = \\displaystyle \\frac{\\partial L}{\\partial Z_1} \\times \\displaystyle \\frac{\\partial Z_1}{\\partial W_1}\\) $$ \\displaystyle \\frac{\\partial Z_1}{\\partial W_1} =  X^\\top \\in \\mathbb{R}^{n\\times d} $$ Therefore we have, $$ \\displaystyle \\frac{\\partial L}{\\partial W_1} = \\frac{1}{n} W_2^\\top \\left[A_2-Y\\right] (\\sigma(Z_1)(1-\\sigma(Z_1))) X^\\top \\in \\mathbb{R}^{h_1\\times d} $$</p> </li> <li> <p>\\(\\displaystyle \\frac{\\partial L}{\\partial b_1} = \\displaystyle \\frac{\\partial L}{\\partial Z_1} \\times \\displaystyle \\frac{\\partial Z_1}{\\partial b_1}\\) $$ \\displaystyle \\frac{\\partial Z_1}{\\partial b_1} =  I \\quad \\text{Identity} $$ Therefore we have, $$ \\displaystyle \\frac{\\partial L}{\\partial b_1} = \\frac{1}{n} W_2^\\top \\left[A_2-Y\\right] (\\sigma(Z_1)(1-\\sigma(Z_1))) \\in \\mathbb{R}^{h_1\\times n} \\quad \\text{So, we will sum over the second dimension to get } \\mathbb{R}^{h_1\\times 1} $$</p> </li> </ul>"},{"location":"neural_network/#gradient-descent-and-optimization","title":"Gradient Descent and Optimization","text":"<p>Using gradient descent, we update parameters in the direction that reduces the loss.</p> <p>Let \\(\\eta\\) be the learning rate. The update rules:</p> <ul> <li>\\(W_1 \\leftarrow W_1 - \\eta \\cdot \\frac{\\partial L}{\\partial W_1}\\)</li> <li>\\(b_1 \\leftarrow b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\\)</li> <li>\\(W_2 \\leftarrow W_2 - \\eta \\cdot \\frac{\\partial L}{\\partial W_2}\\)</li> <li>\\(b_2 \\leftarrow b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\\) </li> </ul> <p>Repeat this process for multiple epochs until the loss converges.</p>"},{"location":"neural_network/#vanishing-and-exploding-gradient","title":"Vanishing and Exploding Gradient","text":"<p>In a deep neural network, at layer \\(\\mathcal{l}\\), we define the pre-activation and activation as follows:</p> \\[ h^{\\mathcal{l}} = \\phi(Z^{\\mathcal{l}}), \\quad \\text{where} \\quad Z^{\\mathcal{l}} = W^{\\mathcal{l}} h^{\\mathcal{l}-1} \\] <p>Here, \\(\\phi\\) is the activation function.</p>"},{"location":"neural_network/#gradient-behavior-in-backpropagation","title":"Gradient Behavior in Backpropagation","text":"<p>During backpropagation, the gradient of the loss \\(L\\) with respect to \\(Z^{\\mathcal{l}}\\) can be approximated as:</p> \\[ \\left\\| \\frac{\\partial L}{\\partial Z^{\\mathcal{l}}} \\right\\| \\approx \\prod_{k = 1}^{\\mathcal{l}} \\|W^{\\mathcal{k}}\\| \\cdot \\|\\phi'(Z^{\\mathcal{k}-1})\\| \\] <p>This product can either explode or vanish, depending on the magnitudes of the weights and activation derivatives.</p>"},{"location":"neural_network/#case-1-exploding-gradients","title":"Case 1: Exploding Gradients","text":"<p>Occurs when:</p> \\[ \\|W^{\\mathcal{k}}\\| &gt; 1 \\] <p>Let</p> \\[ C = \\max_k \\|W^{\\mathcal{k}}\\| \\Rightarrow \\prod_{k} \\|W^{\\mathcal{k}}\\|  \\propto C^{\\mathcal{l}} \\] <p>This exponential growth leads to exploding gradients, destabilizing training.</p>"},{"location":"neural_network/#solutions","title":"Solutions:","text":"<ol> <li>Reduce depth using residual/skip connections.</li> <li>Regularization (e.g., \\(L_1\\), \\(L_2\\), or spectral norm regularization).</li> <li>Gradient clipping to limit gradient magnitude.</li> <li>Normalization techniques, such as BatchNorm or LayerNorm.</li> </ol>"},{"location":"neural_network/#case-2-vanishing-gradients","title":"Case 2: Vanishing Gradients","text":"<p>Occurs when:</p> \\[ \\|W^{\\mathcal{k}}\\| &lt; 1 \\quad \\text{or} \\quad \\|\\phi'(Z^{\\mathcal{k}-1})\\| &lt; 1 \\] <p>This leads to gradients approaching zero, making it difficult for earlier layers to learn.</p>"},{"location":"neural_network/#solutions_1","title":"Solutions:","text":"<ol> <li>Reduce depth via residual connections.</li> <li> <p>Use non-saturating activation functions:</p> </li> <li> <p>Prefer ReLU, Leaky ReLU, ELU, or Swish over sigmoid or tanh.</p> </li> <li>Proper weight initialization (e.g., He or Xavier initialization).</li> </ol>"},{"location":"neural_network/#problem-of-dying-neurons","title":"Problem of Dying Neurons","text":"<p>With ReLU, neurons can \u201cdie\u201d (i.e., output zero for all inputs), especially when gradients become zero.</p>"},{"location":"neural_network/#solutions_2","title":"Solutions:","text":"<ul> <li>Use Leaky ReLU, PReLU, or ELU to maintain non-zero gradients.</li> </ul>"},{"location":"neural_network/#depth-and-skip-connections","title":"Depth and Skip Connections","text":"<p>Depth refers to the number of layers or the length of the computational path from input to output. Skip connections help by providing alternate shorter paths for gradient flow, effectively reducing the network\u2019s depth from a gradient propagation perspective.</p>"},{"location":"neural_network/#summary-table","title":"Summary Table","text":"Problem Solutions Vanishing Gradient - Use residual connections (reduce effective depth)   - Use non-saturating activations  - Use proper initialization Exploding Gradient - Use residual connections  - Regularization (e.g., spectral norm)  - Gradient clipping  - Normalization techniques Dying Neurons - Use Leaky ReLU, ELU, or PReLU"},{"location":"neural_network/#conclusion","title":"Conclusion","text":"<p>Neural networks learn by combining linear transformations with non-linear activations, using forward and backward propagation to update their weights. We have explored a 2-layer network and detailed how gradients flow through the model during training.</p> <p>We also discussed key training challenges which includes: vanishing and exploding gradients and how they can be addressed using techniques like residual connections, proper initialization, and non-saturating activations.</p> <p>A solid understanding of these core concepts is essential for building deeper, more stable, and effective neural networks.</p>"},{"location":"notebooks/","title":"Notebook Gallery","text":"<p>Explore practical implementations of the machine learning concepts covered in this site. Each notebook includes hands-on examples, visualizations, and explanations to reinforce the theory.</p>"},{"location":"notebooks/#supervised-learning","title":"Supervised Learning","text":"<ul> <li> <p>Linear Regression \ud83d\udcc4 View on GitHub </p> </li> <li> <p>Logistic Regression \ud83d\udcc4 View on GitHub </p> </li> <li> <p>Gaussian Discriminant Analysis \ud83d\udcc4 View on GitHub </p> </li> <li> <p>K-Nearest Neighbors (KNN) \ud83d\udcc4 View on GitHub </p> </li> <li> <p>Perceptron Algorithm \ud83d\udcc4 View on GitHub </p> </li> <li> <p>Neural Networks \ud83d\udcc4 View on GitHub </p> </li> </ul>"},{"location":"notebooks/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li> <p>K-Means Clustering \ud83d\udcc4 View on GitHub </p> </li> <li> <p>Principal Component Analysis (PCA) \ud83d\udcc4 View on GitHub </p> </li> </ul>"},{"location":"notebooks/#supporting-concepts","title":"Supporting Concepts","text":"<ul> <li>Activation Functions \ud83d\udcc4 View on GitHub </li> </ul> <p>Tip: Click the \u201cOpen in Colab\u201d badge to launch and interact with each notebook directly in your browser \u2014 no setup required! After opening a notebook in Colab, make sure to run: <pre><code>!pip install ek_ml_package\n</code></pre> This installs the necessary package to enable all examples and code to run correctly.</p>"},{"location":"pca/","title":"Principal Component Analysis (PCA)","text":""},{"location":"pca/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Standardization</li> <li>Covariance Matrix</li> <li>Eigen Decomposition</li> <li>Dimensionality Reduction</li> <li>Algorithm</li> <li>Reconstruction</li> <li>Conclusion</li> </ol>"},{"location":"pca/#introduction","title":"Introduction","text":"<p>Principal Component Analysis (PCA) is a linear dimensionality reduction technique that finds the directions (principal components) that capture the most variance in the data. PCA transforms the data into a new coordinate system where the axes (principal components) are ordered by the amount of variance they explain.</p>"},{"location":"pca/#standardization","title":"Standardization","text":"<p>Before applying PCA, we standardize the data so that each feature contributes equally, especially when features are on different scales.</p> <p>Let:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the original data matrix</li> <li>\\(x_{ij}\\) is the value of the \\(j\\)-th feature for the \\(i\\)-th sample</li> </ul>"},{"location":"pca/#compute-mean-and-standard-deviation","title":"Compute Mean and Standard Deviation","text":"<p>Let:</p> <ul> <li>\\(\\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}\\)</li> <li>\\(\\sigma_j = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (x_{ij} - \\mu_j)^2 }\\)</li> </ul>"},{"location":"pca/#standardize-each-feature","title":"Standardize Each Feature","text":"\\[ x_{ij}^{\\text{(std)}} = \\frac{x_{ij} - \\mu_j}{\\sigma_j} \\] <p>Let:</p> <ul> <li>\\(\\mathbf{X}\\_{\\text{std}}\\) be the standardized data matrix</li> </ul> <p>So now:</p> \\[ \\mathbf{X}_{\\text{std}} = \\frac{\\mathbf{X} - \\mu}{\\sigma} \\] <p>where \\(\\mu\\) and \\(\\sigma\\) are broadcast row-wise over all samples</p> <p>Now \\(\\mathbf{X}_{\\text{std}}\\) has zero mean and unit variance per feature.</p>"},{"location":"pca/#covariance-matrix","title":"Covariance Matrix","text":"<p>The covariance matrix measures how features vary with respect to each other:</p> \\[ \\mathbf{\\Sigma} = \\frac{1}{n} \\mathbf{X}_{\\text{std}}^\\top \\mathbf{X}_{\\text{std}} \\in \\mathbb{R}^{d \\times d} \\] <ul> <li>Diagonal elements represent feature variances</li> <li>Off-diagonal elements represent covariances</li> </ul>"},{"location":"pca/#eigen-decomposition","title":"Eigen Decomposition","text":"<p>To find the principal components, we compute the eigenvectors and eigenvalues of \\(\\mathbf{\\Sigma}\\):</p> \\[ \\mathbf{\\Sigma} v_i = \\lambda_i v_i \\] <p>Where:</p> <ul> <li>\\(v_i\\) = \\(i\\)-th eigenvector (principal direction)</li> <li>\\(\\lambda_i\\) = variance explained along \\(v_i\\)</li> </ul> <p>Sort eigenvalues: \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d\\)</p> <p>Let:</p> <ul> <li>\\(\\mathbf{V} = [v_1, v_2, \\dots, v_d] \\in \\mathbb{R}^{d \\times d}\\)</li> <li>\\(\\mathbf{\\Lambda} = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)\\)</li> </ul> <p>Then:</p> \\[ \\mathbf{\\Sigma} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top \\]"},{"location":"pca/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>To reduce the dataset from \\(d\\) dimensions to \\(k\\) dimensions (\\(k &lt; d\\)), we take the top \\(k\\) eigenvectors:</p> <p>Let:</p> <ul> <li>\\(\\mathbf{V} = [v_1, \\dots, v_k] \\in \\mathbb{R}^{d \\times k}\\)</li> </ul> <p>Then the projected data is:</p> \\[ \\mathbf{Z} = \\mathbf{X}_{\\text{std}} \\mathbf{V} \\in \\mathbb{R}^{n \\times k} \\] <p>Each row in \\(\\mathbf{Z}\\) is the lower-dimensional representation of the original sample.</p>"},{"location":"pca/#algorithm","title":"Algorithm","text":"<p>Input:</p> <ul> <li>Dataset: \\(D = \\{ \\mathbf{x}_i \\}_{i=1}^n\\), where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\)</li> <li>Desired number of components: \\(k\\) such that \\(k &lt; d\\)</li> <li>Standardization flag: <code>standardize = True/False</code></li> </ul> <p>Output:</p> <ul> <li>Projected data \\(\\mathbf{Z} \\in \\mathbb{R}^{n \\times k}\\)</li> <li>Projection matrix \\(\\mathbf{V} \\in \\mathbb{R}^{d \\times k}\\)</li> </ul> <pre><code>1. Compute the mean vector \u03bc = (1/n) \u2211 x\u1d62 \u2208 \u211d\u1d48\n2. Center the data: X_centered = X \u2212 1\u22c5\u03bc\u1d40  \u2208 \u211d\u207f\u02e3\u1d48\n\n3. If standardize is True then:\n    4. Compute std deviation vector \u03c3 = sqrt((1/n) \u2211 (x\u1d62 \u2212 \u03bc)\u00b2)\n    5. Standardize: X_std = X_centered / \u03c3  (element-wise)\n    6. X \u2190 X_std\n   else:\n    7. X \u2190 X_centered\n\n8. Compute covariance matrix: \u03a3 = (1/n) \u22c5 X\u1d40X \u2208 \u211d\u1d48\u02e3\u1d48\n\n9. Compute eigenvalues \u03bb\u2081,...,\u03bb_d and eigenvectors v\u2081,...,v_d of \u03a3\n10. Sort eigenvectors V = [v\u2081, ..., v_d] by descending eigenvalues\n\n11. Form projection matrix: V = [v\u2081, ..., v_k] \u2208 \u211d\u1d48\u02e3\u1d4f\n\n12. Project data onto top-k components: Z = X \u22c5 V \u2208 \u211d\u207f\u02e3\u1d4f\n\n13. Return Z, V\n</code></pre>"},{"location":"pca/#reconstruction","title":"Reconstruction","text":"<p>To reconstruct the approximation of the original standardized data from the reduced representation:</p> \\[ \\hat{\\mathbf{X}}_{\\text{std}} = \\mathbf{Z} \\mathbf{V}^\\top \\] <p>To return to the original scale, we unstandardize:</p> \\[ \\hat{\\mathbf{X}} = \\hat{\\mathbf{X}}_{\\text{std}} \\cdot \\sigma + \\mu \\] <p>where \\(\\sigma\\) and \\(\\mu\\) are broadcast appropriately across dimensions</p>"},{"location":"pca/#conclusion","title":"Conclusion","text":"<p>PCA is a powerful unsupervised technique to:</p> <ul> <li>Reduce dimensionality</li> <li>Remove noise</li> <li>Visualize high-dimensional data</li> </ul>"},{"location":"pca/#summary-table","title":"Summary Table","text":"Step Description Standardization Ensure each feature has zero mean and unit variance Covariance Matrix Compute feature-wise covariance after standardization Eigen Decomposition Extract eigenvectors and eigenvalues of the covariance matrix Projection Project original data onto top \\(k\\) eigenvectors Reconstruction Approximate the original data using the reduced components"},{"location":"perceptron/","title":"Perceptron Algorithm","text":""},{"location":"perceptron/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Algorithm</li> <li>Intuition Behind the Update Rule</li> <li>Perceptron Convergence Theorem</li> <li>Advantages &amp; Disadvantages</li> </ol>"},{"location":"perceptron/#the-algorithm","title":"The Algorithm","text":"<p>The Perceptron is an algorithm for learning a binary classifier. The goal is to find a hyperplane that separates the data linearly. It predicts labels \\(\\hat{y} \\in \\{-1, +1\\}\\) given an input \\(x \\in \\mathbb{R}^n\\).</p>"},{"location":"perceptron/#algorithm-1-perceptron-algorithm","title":"Algorithm 1: Perceptron Algorithm","text":"<p>Input: Sequence of training examples \\((x_1, y_1), (x_2, y_2), \\ldots\\), where \\(x_i \\in \\mathbb{R}^n\\), \\(y_i \\in \\{-1, +1\\}\\)</p> <p>Output: Final weight vector \\(w \\in \\mathbb{R}^n\\)</p> <pre><code>1. Initialize:      w\u2081 = 0 \u2208 \u211d\u207f\n2. For t = 1, 2, ...\n3.     Sample (x\u1d62, y\u1d62)\n4.     Predict:      \u0177 = sign(w\u209c\u1d40 x\u1d62)\n5.     If \u0177 \u2260 y\u1d62 then\n6.         w\u209c\u208a\u2081 = w\u209c + y\u1d62 x\u1d62\n7.     Else\n8.         w\u209c\u208a\u2081 = w\u209c\n</code></pre>"},{"location":"perceptron/#intuition-behind-the-update-rule","title":"Intuition Behind the Update Rule","text":""},{"location":"perceptron/#why-is-the-update-rule","title":"Why is the update rule:","text":"\\[ w_{t+1} = w_t + y_i x_i\\ ? \\] <p>We can analyze this by multiplying the transpose of the weight update with the feature vector \\(x_i\\):</p> \\[ \\begin{aligned} w_{t+1}^\\top x_i &amp;= (w_t + y_i x_i)^\\top x_i \\\\                  &amp;= w_t^\\top x_i + y_i x_i^\\top x_i \\end{aligned} \\] <p>Let\u2019s consider two cases:</p>"},{"location":"perceptron/#case-1-y_i-1-positive-example","title":"Case 1: \\(y_i = +1\\) (positive example)","text":"<p>If the prediction was wrong, then \\(w_t^\\top x_i &lt; 0\\), and we have:</p> \\[ w_{t+1}^\\top x_i = \\underbrace{w_t^\\top x_i}_{&lt; 0} + \\underbrace{x_i^\\top x_i}_{\\|x_i\\|^2 &gt; 0} \\] <p>This moves the dot product closer to being positive, which corrects the classification.</p>"},{"location":"perceptron/#case-2-y_i-1-negative-example","title":"Case 2: \\(y_i = -1\\) (negative example)","text":"<p>If the prediction was wrong, then \\(w_t^\\top x_i &gt; 0\\), and we have:</p> \\[ w_{t+1}^\\top x_i = \\underbrace{w_t^\\top x_i}_{&gt; 0} - \\underbrace{x_i^\\top x_i}_{\\|x_i\\|^2 &gt; 0} \\] <p>This moves the dot product closer to being negative, again correcting the classification.</p>"},{"location":"perceptron/#perceptron-convergence-theorem","title":"Perceptron Convergence Theorem","text":""},{"location":"perceptron/#theorem-perceptron-mistake-bound","title":"Theorem (Perceptron Mistake Bound)","text":"<p>Assume:</p> <ul> <li>The data is linearly separable if there exists a unit vector \\(w^* \\in \\mathbb{R}^n\\) and margin \\(\\gamma &gt; 0\\) such that</li> </ul> <p>$$   \\forall i,\\quad y_i (w^{*T} x_i) \\geq \\gamma   $$ * Each example is bounded: \\(\\|x_i\\| \\leq R\\)</p> <p>Then, the Perceptron algorithm makes at most \\(\\frac{R^2}{\\gamma^2}\\) mistakes.</p>"},{"location":"perceptron/#proof","title":"Proof:","text":"<p>When mistake \\(k\\) occurs at example \\(t\\), we update:</p> \\[ w_{k+1} = w_k + y_t x_t \\] <p>Multiply both sides by \\(w^*\\):</p> \\[ w_{k+1}^\\top w^* = w_k^\\top w^* + y_t x_t^\\top w^* \\geq w_k^\\top w^* + \\gamma \\] <p>By induction (starting from \\(w_1 = 0\\)):</p> \\[ w_{k+1}^\\top w^* \\geq k \\gamma \\tag{1} \\] <p>Now compute the squared norm of \\(w_{k+1}\\):</p> \\[ \\begin{align*} \\|w_{k+1}\\|^2 &amp;= \\|w_k + y_t x_t\\|^2 \\\\               &amp;= \\|w_k\\|^2 + 2 y_t w_k^\\top x_t + \\|x_t\\|^2 \\\\               &amp;\\leq \\|w_k\\|^2 + R^2 \\\\               &amp;\\leq k R^2 \\tag{2} \\end{align*} \\] <p>Using Cauchy-Schwarz and equations (1) and (2):</p> \\[ \\begin{aligned} \\|w_{k+1}\\| \\cdot \\|w^*\\| &amp;\\geq w_{k+1}^\\top w^* \\\\ \\Rightarrow \\|w_{k+1}\\| &amp;\\geq k \\gamma \\quad \\text{(since \\( \\|w^*\\| = 1 \\))} \\\\ \\Rightarrow (k\\gamma)^2 &amp;\\leq k R^2 \\Rightarrow k \\leq \\frac{R^2}{\\gamma^2} \\end{aligned} \\] <p>Hence the Perceptron algorithm makes at most \\(\\frac{R^2}{\\gamma^2}\\) mistakes..</p>"},{"location":"perceptron/#advantages-disadvantages","title":"Advantages &amp; Disadvantages","text":""},{"location":"perceptron/#advantages","title":"Advantages","text":"<ol> <li>Cheap and easy to implement.</li> <li>Online algorithm: processes one example at a time. It can scale to large datasets.</li> <li>Can use any features easily.</li> </ol>"},{"location":"perceptron/#disadvantages","title":"Disadvantages","text":"<ol> <li>All classifiers that separate the data are equivalent.</li> <li>Convergence depends on the margin \\(\\gamma\\). If classes are not well separated, slow convergence.</li> </ol>"}]}