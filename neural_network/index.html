<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=../perceptron/ rel=prev><link href=../kmeans/ rel=next><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>Neural Networks - Machine Learning</title><link rel=stylesheet href=../assets/stylesheets/main.342714a4.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#neural-networks class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Machine Learning" class="md-header__button md-logo" aria-label="Machine Learning" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Machine Learning </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Neural Networks </span> </div> </div> </div> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Introduction </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../linear_regression/ class=md-tabs__link> Supervised Learning </a> </li> <li class=md-tabs__item> <a href=../kmeans/ class=md-tabs__link> Unsupervised Learning </a> </li> <li class=md-tabs__item> <a href=../activation_function/ class=md-tabs__link> Supporting Concepts </a> </li> <li class=md-tabs__item> <a href=../notebooks/ class=md-tabs__link> Notebook Gallery </a> </li> <li class=md-tabs__item> <a href=../about/ class=md-tabs__link> About Author </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Machine Learning" class="md-nav__button md-logo" aria-label="Machine Learning" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Machine Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Supervised Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Supervised Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../linear_regression/ class=md-nav__link> <span class=md-ellipsis> Linear Regression </span> </a> </li> <li class=md-nav__item> <a href=../logistic_regression/ class=md-nav__link> <span class=md-ellipsis> Logistic Regression </span> </a> </li> <li class=md-nav__item> <a href=../gaussian_discriminant_analysis/ class=md-nav__link> <span class=md-ellipsis> Gaussian Discriminant Analysis </span> </a> </li> <li class=md-nav__item> <a href=../knn/ class=md-nav__link> <span class=md-ellipsis> K-Nearest Neighbors (KNN) </span> </a> </li> <li class=md-nav__item> <a href=../perceptron/ class=md-nav__link> <span class=md-ellipsis> Perceptron Algorithm </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Neural Networks </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Unsupervised Learning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Unsupervised Learning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../kmeans/ class=md-nav__link> <span class=md-ellipsis> K-Means Clustering </span> </a> </li> <li class=md-nav__item> <a href=../pca/ class=md-nav__link> <span class=md-ellipsis> Principal Component Analysis (PCA) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Supporting Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Supporting Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../activation_function/ class=md-nav__link> <span class=md-ellipsis> Activation Functions </span> </a> </li> <li class=md-nav__item> <a href=../loss_function/ class=md-nav__link> <span class=md-ellipsis> Loss Functions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../notebooks/ class=md-nav__link> <span class=md-ellipsis> Notebook Gallery </span> </a> </li> <li class=md-nav__item> <a href=../about/ class=md-nav__link> <span class=md-ellipsis> About Author </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=neural-networks>Neural Networks</h1> <h2 id=table-of-contents>Table of Contents</h2> <ol> <li><a href=#introduction>Introduction</a></li> <li><a href=#data-representation>Data Representation</a></li> <li><a href=#neural-network-architecture>Neural Network Architecture</a></li> <li><a href=#forward-propagation>Forward Propagation</a></li> <li><a href=#loss-function>Loss Function</a></li> <li><a href=#backward-propagation>Backward Propagation</a></li> <li><a href=#gradient-descent-and-optimization>Gradient Descent and Optimization</a></li> <li><a href=#vanishing-and-exploding-gradient>Vanishing and Exploding Gradient</a> </li> <li><a href=#conclusion>Conclusion</a></li> </ol> <h2 id=introduction>Introduction</h2> <p>A neural network is a machine learning model inspired by the human brain that maps input data to output predictions by learning weights through layers of interconnected nodes (neurons). Neural networks are capable of modeling complex non-linear decision boundaries by composing multiple linear transformations and non-linear activation functions.</p> <p>A feedforward neural network is made up of:</p> <ul> <li>An input layer that takes the features.</li> <li>One or more hidden layers that learn intermediate representations.</li> <li>An output layer that produces predictions.</li> </ul> <p>Each layer applies a linear transformation followed by a non-linear activation function.</p> <h2 id=data-representation>Data Representation</h2> <p>Let:</p> <ul> <li><span class=arithmatex>\(n\)</span>: number of training examples</li> <li><span class=arithmatex>\(d\)</span>: number of features</li> <li><span class=arithmatex>\(X \in \mathbb{R}^{d \times n}\)</span>: input matrix</li> <li><span class=arithmatex>\(y \in \mathbb{R}^{1 \times n}\)</span>: target vector for regression or <span class=arithmatex>\(y \in {0, 1}^{1\times n}\)</span> for binary classification</li> </ul> <p>For a 2-layer neural network (i.e., one hidden layer), we define:</p> <ul> <li><span class=arithmatex>\(W^{[1]} \in \mathbb{R}^{h \times d}\)</span>: weights for hidden layer</li> <li><span class=arithmatex>\(b^{[1]} \in \mathbb{R}^{h \times 1}\)</span>: bias for hidden layer</li> <li><span class=arithmatex>\(W^{[2]} \in \mathbb{R}^{1 \times h}\)</span>: weights for output layer</li> <li><span class=arithmatex>\(b^{[2]} \in \mathbb{R}\)</span>: bias for output layer</li> </ul> <h2 id=neural-network-architecture>Neural Network Architecture</h2> <p>A neural network is made of interconnected neurons. Each of them is characterized by its <strong>weight</strong>, <strong>bias</strong> and <strong>activation function</strong>.</p> <p>Here are other elements of this network.</p> <p><strong>Input Layer</strong></p> <p>The input layer takes raw input from the domain. No computation is performed at this layer. Nodes here just pass on the information (features) to the hidden layer. </p> <p><strong>Hidden Layer</strong></p> <p>As the name suggests, the nodes of this layer are not exposed. They provide an abstraction to the neural network. </p> <p>The hidden layer performs all kinds of computation on the features entered through the input layer and transfers the result to the output layer.</p> <p><strong>Output Layer</strong></p> <p>It’s the final layer of the network that brings the information learned through the hidden layer and delivers the final value as a result. </p> <p>Here we are going to use an example of a 2-layer feedforward neural network with:</p> <ul> <li>Input layer (<span class=arithmatex>\(d\)</span> neurons)</li> <li>Hidden layer (<span class=arithmatex>\(h_1\)</span> neurons)</li> <li>Output layer (<span class=arithmatex>\(h_2\)</span> neurons)</li> </ul> <p>Assuming our task is binary clasification and we use <span class=arithmatex>\(\sigma\)</span> (sigmoid) activation function on both layers.</p> <p>Now let&rsquo;s see how our data and parameters are represented.</p> <ul> <li> <p><strong>Data</strong>:</p> <ul> <li><strong>Features</strong>: <span class=arithmatex>\(X \in \mathbb{R}^{d\times n}\)</span> which is the transpose of our original input data.</li> <li><strong>Targets</strong>: <span class=arithmatex>\(Y \in \mathbb{R}^{h_2\times n}\)</span> which is the transpose of our original target data.</li> </ul> </li> <li> <p><strong>Layer 1</strong>:</p> <ul> <li><strong>Weight</strong>: <span class=arithmatex>\(W_1 \in \mathbb{R}^{h_1\times d}\)</span></li> <li><strong>Bias</strong>: <span class=arithmatex>\(b_1 \in \mathbb{R}^{h_1\times 1}\)</span></li> </ul> </li> <li> <p><strong>Layer 2</strong>:</p> <ul> <li><strong>Weight</strong>: <span class=arithmatex>\(W_2 \in \mathbb{R}^{h_2\times h_1}\)</span></li> <li><strong>Bias</strong>: <span class=arithmatex>\(b_2 \in \mathbb{R}^{h_2\times 1}\)</span></li> </ul> </li> <li> <p><strong>Loss Function</strong>: Since this is a binary classification problem, we use binary cross entrpy. $$ \mathcal{L} = - \frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] $$</p> </li> </ul> <h3 id=forward-propagation>Forward Propagation</h3> <p>The Feedforward Propagation, also called Forward Pass, is the process consisting of computing all network nodes’ output values, starting with the first hidden layer until the last output layer, using at start either a subset or the entire dataset samples.</p> <p><strong>Layer 1</strong></p> <ul> <li>linear transformation $$ Z_1 = W_1X+b_1 \quad \text{where } Z_1 \in \mathbb{R}^{h_1\times n} $$</li> <li>non-linear transformation with <span class=arithmatex>\(\sigma\)</span> activation function $$ A_1 = \sigma (Z_1) \quad \text{where } A_1 \in \mathbb{R}^{h_1\times n} $$</li> <li>Number of parameters in layer 1 is given as: <span class=arithmatex>\((h_1 \times d) + h_1 = h_1(d + 1)\)</span>.</li> </ul> <p><strong>Layer 2</strong></p> <ul> <li>linear transformation $$ Z_2 = W_2A_1+b_2 \quad \text{where } Z_2 \in \mathbb{R}^{h_2\times n} $$</li> <li>non-linear transformation with <span class=arithmatex>\(\sigma\)</span> activation function $$ A_2 = \sigma (Z_2) \quad \text{where } A_2 \in \mathbb{R}^{h_2\times n} $$</li> <li>Number of parameters in layer 2 is given as: <span class=arithmatex>\((h_2 \times h_1) + h_2 = h_2(h_1+1)\)</span>.</li> </ul> <p><strong>Output Layer</strong></p> <ul> <li>Output $$ A_2 = \hat{Y} \in \mathbb{R}^{h_2\times n} $$</li> </ul> <h2 id=loss-function>Loss Function</h2> <p>For binary classification, we use binary cross-entropy loss:</p> <div class=arithmatex>\[ \mathcal{L} = - \frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] \]</div> <p>Matrix form:</p> <div class=arithmatex>\[ \mathcal{L} = - \frac{1}{n} \left[ Y \log(A_2) + (1 - Y) \log(1 - A_2) \right] \]</div> <h3 id=backward-propagation>Backward Propagation</h3> <p>Backpropagation, or backward propagation of errors, is an algorithm working from the output nodes to the input nodes of a neural network using the chain rule to compute how much each activation unit contributed to the overall error.</p> <p>Backpropagation automatically computes error gradients to then repeatedly adjust all weights and biases to reduce the overall error.</p> <p>From our example our aim is to find </p> <div class=arithmatex>\[ \displaystyle \frac{\partial L}{\partial A_2}, \quad \displaystyle \frac{\partial L}{\partial Z_2}, \quad \displaystyle \frac{\partial L}{\partial W_2}, \quad \displaystyle \frac{\partial L}{\partial b_2}, \quad \displaystyle \frac{\partial L}{\partial A_1}, \quad \displaystyle \frac{\partial L}{\partial Z_1}, \quad \displaystyle \frac{\partial L}{\partial W_1} \quad \text{ and } \displaystyle \frac{\partial L}{\partial b_1} \]</div> <p>Note that we are using our loss and binary cross entropy. But since we want to find its partial derivative w.r.t <span class=arithmatex>\(A_2\)</span> we will modify it to be $$ L = - \frac{1}{n} \left[ Y \log(A_2) + (1 - Y) \log(1 - A_2) \right] $$</p> <ul> <li><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial A_2}\)</span> $$ \displaystyle \frac{\partial L}{\partial A_2} = \frac{1}{n} \left[\frac{A_2-Y}{A_2(1-A_2)}\right] \in \mathbb{R}^{h_2 \times n} $$</li> <li><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial Z_2} = \displaystyle \frac{\partial L}{\partial A_2} \times \displaystyle \frac{\partial A_2}{\partial Z_2}\)</span> $$ \displaystyle \frac{\partial A_2}{\partial Z_2} = \sigma(Z_2)(1-\sigma(Z_2))= A_2(1-A_2) \in \mathbb{R}^{h_2 \times n} \quad \text{ since }\sigma(Z_2) = A_2\ $$ Therefore we have,</li> </ul> <div class=arithmatex>\[ \begin{align*} \displaystyle \frac{\partial L}{\partial Z_2} &amp;= \displaystyle \frac{\partial L}{\partial A_2} \times \displaystyle \frac{\partial A_2}{\partial Z_2}\\ &amp;= \frac{1}{n} \left[\frac{A_2-Y}{A_2(1-A_2)}\right] \times A_2(1-A_2)\\ &amp;= \frac{1}{n} \left[A_2-Y\right] \in \mathbb{R}^{h_2\times n} \end{align*} \]</div> <ul> <li> <p><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial W_2} = \displaystyle \frac{\partial L}{\partial Z_2} \times \displaystyle \frac{\partial Z_2}{\partial W_2}\)</span> $$ \frac{\partial Z_2}{\partial W_2} = A_1^\top \in \mathbb{R}^{h_2\times h_1} $$ Therefore we have, $$ \displaystyle \frac{\partial L}{\partial W_2} = \frac{1}{n} \left[A_2-Y\right]A_1^\top \in \mathbb{R}^{h_2 \times h_1} $$</p> </li> <li> <p><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial b_2} = \displaystyle \frac{\partial L}{\partial Z_2} \times \displaystyle \frac{\partial Z_2}{\partial b_2}\)</span> $$ \frac{\partial Z_2}{\partial b_2} = I \quad \text{Identity} $$ Therefore we have,</p> </li> </ul> <div class=arithmatex>\[ \displaystyle \frac{\partial L}{\partial b_2} = \frac{1}{n} \left[A_2-Y\right] \in \mathbb{R}^{h_2 \times n} \quad \text{ but } \displaystyle \frac{\partial L}{\partial b_2} \in \mathbb{R}^{h_2 \times 1} \quad \text{So, we will sum over the second dimension.} \]</div> <ul> <li><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial A_1} = \displaystyle \frac{\partial L}{\partial Z_2} \times \displaystyle \frac{\partial Z_2}{\partial A_1}\)</span> $$ \displaystyle \frac{\partial Z_2}{\partial A_1} = W_2^\top \in \mathbb{R}^{h_1\times h_2} $$ Therefore we have,</li> </ul> <div class=arithmatex>\[ \displaystyle \frac{\partial L}{\partial A_1} = \frac{1}{n} \left[A_2-Y\right] W_2^\top \in \mathbb{R}^{h_1\times n} \]</div> <ul> <li><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial Z_1} = \displaystyle \frac{\partial L}{\partial A_1} \times \displaystyle \frac{\partial A_1}{\partial Z_1}\)</span> $$ \displaystyle \frac{\partial A_1}{\partial Z_1} = \underbrace{\sigma(Z_1)(1-\sigma(Z_1))}_{\text{element-wise multip.}} \in \mathbb{R}^{h_1\times n} $$ Therefore we have,</li> </ul> <div class=arithmatex>\[ \begin{align*} \displaystyle \frac{\partial L}{\partial Z_1} &amp;= \frac{1}{n} \left[A_2-Y\right] W_2^\top (\sigma(Z_1)(1-\sigma(Z_1))) \quad \text{ Due to dimentionality we change to}\\ &amp;= \frac{1}{n} \underbrace{\underbrace{W_2^\top \left[A_2-Y\right]}_{\text{matrix multip.}} (\sigma(Z_1)(1-\sigma(Z_1)))}_{\text{element-wise multip.}} \in \mathbb{R}^{h_1\times n} \end{align*} \]</div> <ul> <li> <p><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial W_1} = \displaystyle \frac{\partial L}{\partial Z_1} \times \displaystyle \frac{\partial Z_1}{\partial W_1}\)</span> $$ \displaystyle \frac{\partial Z_1}{\partial W_1} = X^\top \in \mathbb{R}^{n\times d} $$ Therefore we have, $$ \displaystyle \frac{\partial L}{\partial W_1} = \frac{1}{n} W_2^\top \left[A_2-Y\right] (\sigma(Z_1)(1-\sigma(Z_1))) X^\top \in \mathbb{R}^{h_1\times d} $$</p> </li> <li> <p><span class=arithmatex>\(\displaystyle \frac{\partial L}{\partial b_1} = \displaystyle \frac{\partial L}{\partial Z_1} \times \displaystyle \frac{\partial Z_1}{\partial b_1}\)</span> $$ \displaystyle \frac{\partial Z_1}{\partial b_1} = I \quad \text{Identity} $$ Therefore we have, $$ \displaystyle \frac{\partial L}{\partial b_1} = \frac{1}{n} W_2^\top \left[A_2-Y\right] (\sigma(Z_1)(1-\sigma(Z_1))) \in \mathbb{R}^{h_1\times n} \quad \text{So, we will sum over the second dimension to get } \mathbb{R}^{h_1\times 1} $$</p> </li> </ul> <h2 id=gradient-descent-and-optimization>Gradient Descent and Optimization</h2> <p>Using gradient descent, we update parameters in the direction that reduces the loss.</p> <p>Let <span class=arithmatex>\(\eta\)</span> be the learning rate. The update rules:</p> <ul> <li><span class=arithmatex>\(W_1 \leftarrow W_1 - \eta \cdot \frac{\partial L}{\partial W_1}\)</span></li> <li><span class=arithmatex>\(b_1 \leftarrow b_1 - \eta \cdot \frac{\partial L}{\partial b_1}\)</span></li> <li><span class=arithmatex>\(W_2 \leftarrow W_2 - \eta \cdot \frac{\partial L}{\partial W_2}\)</span></li> <li><span class=arithmatex>\(b_2 \leftarrow b_2 - \eta \cdot \frac{\partial L}{\partial b_2}\)</span> </li> </ul> <p>Repeat this process for multiple epochs until the loss converges.</p> <h2 id=vanishing-and-exploding-gradient>Vanishing and Exploding Gradient</h2> <p>In a deep neural network, at layer <span class=arithmatex>\(\mathcal{l}\)</span>, we define the pre-activation and activation as follows:</p> <div class=arithmatex>\[ h^{\mathcal{l}} = \phi(Z^{\mathcal{l}}), \quad \text{where} \quad Z^{\mathcal{l}} = W^{\mathcal{l}} h^{\mathcal{l}-1} \]</div> <p>Here, <span class=arithmatex>\(\phi\)</span> is the activation function.</p> <h3 id=gradient-behavior-in-backpropagation><strong>Gradient Behavior in Backpropagation</strong></h3> <p>During backpropagation, the gradient of the loss <span class=arithmatex>\(L\)</span> with respect to <span class=arithmatex>\(Z^{\mathcal{l}}\)</span> can be approximated as:</p> <div class=arithmatex>\[ \left\| \frac{\partial L}{\partial Z^{\mathcal{l}}} \right\| \approx \prod_{k = 1}^{\mathcal{l}} \|W^{\mathcal{k}}\| \cdot \|\phi'(Z^{\mathcal{k}-1})\| \]</div> <p>This product can either <strong>explode</strong> or <strong>vanish</strong>, depending on the magnitudes of the weights and activation derivatives.</p> <h3 id=case-1-exploding-gradients><strong>Case 1: Exploding Gradients</strong></h3> <p>Occurs when:</p> <div class=arithmatex>\[ \|W^{\mathcal{k}}\| &gt; 1 \]</div> <p>Let</p> <div class=arithmatex>\[ C = \max_k \|W^{\mathcal{k}}\| \Rightarrow \prod_{k} \|W^{\mathcal{k}}\| \propto C^{\mathcal{l}} \]</div> <p>This exponential growth leads to <strong>exploding gradients</strong>, destabilizing training.</p> <h4 id=solutions><strong>Solutions:</strong></h4> <ol> <li><strong>Reduce depth</strong> using residual/skip connections.</li> <li><strong>Regularization</strong> (e.g., <span class=arithmatex>\(L_1\)</span>, <span class=arithmatex>\(L_2\)</span>, or spectral norm regularization).</li> <li><strong>Gradient clipping</strong> to limit gradient magnitude.</li> <li><strong>Normalization techniques</strong>, such as BatchNorm or LayerNorm.</li> </ol> <h3 id=case-2-vanishing-gradients><strong>Case 2: Vanishing Gradients</strong></h3> <p>Occurs when:</p> <div class=arithmatex>\[ \|W^{\mathcal{k}}\| &lt; 1 \quad \text{or} \quad \|\phi'(Z^{\mathcal{k}-1})\| &lt; 1 \]</div> <p>This leads to gradients approaching zero, making it difficult for earlier layers to learn.</p> <h4 id=solutions_1><strong>Solutions:</strong></h4> <ol> <li><strong>Reduce depth</strong> via residual connections.</li> <li> <p><strong>Use non-saturating activation functions</strong>:</p> </li> <li> <p>Prefer <strong>ReLU</strong>, <strong>Leaky ReLU</strong>, <strong>ELU</strong>, or <strong>Swish</strong> over <strong>sigmoid</strong> or <strong>tanh</strong>.</p> </li> <li><strong>Proper weight initialization</strong> (e.g., He or Xavier initialization).</li> </ol> <h3 id=problem-of-dying-neurons><strong>Problem of Dying Neurons</strong></h3> <p>With ReLU, neurons can &ldquo;die&rdquo; (i.e., output zero for all inputs), especially when gradients become zero.</p> <h4 id=solutions_2><strong>Solutions:</strong></h4> <ul> <li>Use <strong>Leaky ReLU</strong>, <strong>PReLU</strong>, or <strong>ELU</strong> to maintain non-zero gradients.</li> </ul> <h3 id=depth-and-skip-connections><strong>Depth and Skip Connections</strong></h3> <p>Depth refers to the number of layers or the length of the computational path from input to output. <a href=https://arxiv.org/pdf/1512.03385v1><strong>Skip connections</strong></a> help by providing alternate shorter paths for gradient flow, effectively reducing the network&rsquo;s depth from a gradient propagation perspective.</p> <h3 id=summary-table><strong>Summary Table</strong></h3> <table> <thead> <tr> <th>Problem</th> <th>Solutions</th> </tr> </thead> <tbody> <tr> <td><strong>Vanishing Gradient</strong></td> <td>- Use residual connections (reduce effective depth) <br> - Use non-saturating activations <br> - Use proper initialization</td> </tr> <tr> <td><strong>Exploding Gradient</strong></td> <td>- Use residual connections <br> - Regularization (e.g., spectral norm) <br> - Gradient clipping <br> - Normalization techniques</td> </tr> <tr> <td><strong>Dying Neurons</strong></td> <td>- Use Leaky ReLU, ELU, or PReLU</td> </tr> </tbody> </table> <h2 id=conclusion>Conclusion</h2> <p>Neural networks learn by combining linear transformations with non-linear activations, using forward and backward propagation to update their weights. We have explored a 2-layer network and detailed how gradients flow through the model during training.</p> <p>We also discussed key training challenges which includes: <strong>vanishing</strong> and <strong>exploding gradients</strong> and how they can be addressed using techniques like <strong>residual connections</strong>, <strong>proper initialization</strong>, and <strong>non-saturating activations</strong>.</p> <p>A solid understanding of these core concepts is essential for building deeper, more stable, and effective neural networks.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../perceptron/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Perceptron Algorithm"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Perceptron Algorithm </div> </div> </a> <a href=../kmeans/ class="md-footer__link md-footer__link--next" aria-label="Next: K-Means Clustering"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> K-Means Clustering </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.copy", "content.math", "navigation.footer", "navigation.tabs", "toc.integrate", "header.autohide"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../assets/mathjax/es5/tex-mml-chtml.js></script> </body> </html>