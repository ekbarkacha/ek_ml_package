
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../pca/">
      
      
        <link rel="next" href="../loss_function/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>Activation Functions - Machine Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#activation-functions" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Machine Learning" class="md-header__button md-logo" aria-label="Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Activation Functions
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Introduction

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../linear_regression/" class="md-tabs__link">
          
  
  
  Supervised Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../kmeans/" class="md-tabs__link">
          
  
  
  Unsupervised Learning

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  
  Supporting Concepts

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../notebooks/" class="md-tabs__link">
        
  
  
    
  
  Notebook Gallery

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../about/" class="md-tabs__link">
        
  
  
    
  
  About Author

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Machine Learning" class="md-nav__button md-logo" aria-label="Machine Learning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supervised Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Supervised Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linear_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logistic_regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Logistic Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gaussian_discriminant_analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gaussian Discriminant Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../knn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Nearest Neighbors (KNN)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perceptron/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Perceptron Algorithm
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../neural_network/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unsupervised Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kmeans/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    K-Means Clustering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pca/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Principal Component Analysis (PCA)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Supporting Concepts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Supporting Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Activation Functions
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loss_function/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Loss Functions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Notebook Gallery
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About Author
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="activation-functions">Activation Functions</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#step-function">Step Function</a></li>
<li><a href="#linear-activation">Linear Activation</a></li>
<li><a href="#sigmoid-function">Sigmoid Function</a></li>
<li><a href="#tanh-function">Tanh Function</a></li>
<li><a href="#relu-function">ReLU Function</a></li>
<li><a href="#leaky-relu">Leaky ReLU</a></li>
<li><a href="#the-risk-of-vanishing-or-exploding-gradients">The Risk of Vanishing or Exploding Gradients</a></li>
<li><a href="#impact-of-activation-functions-on-model-performance">Impact of Activation Functions on Model Performance</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
<h2 id="introduction">Introduction</h2>
<p>Activation functions in neural networks are mathematical functions that determine the output of a neuron based on its input. They introduce non-linearity, allowing neural networks to learn complex patterns in data. These functions decide whether a neuron should be activated or not, and how strongly its signal is passed to the next layer.</p>
<p>Given an input vector <span class="arithmatex">\(x \in \mathbb{R}^d\)</span> and weights <span class="arithmatex">\(w \in \mathbb{R}^d\)</span>, the neuron output before activation is:</p>
<div class="arithmatex">\[
z = w^\top x + b
\]</div>
<p>Then the activation function <span class="arithmatex">\(f(z)\)</span> is applied:</p>
<div class="arithmatex">\[
a = f(z)
\]</div>
<h3 id="types-of-activation-functions">Types of Activation Functions</h3>
<p>Several different types of activation functions are used in Deep Learning. Some of them are explained below:</p>
<ol>
<li>Step Function</li>
<li>Linear Activation</li>
<li>Sigmoid Function</li>
<li>Tanh Function</li>
<li>ReLU Function</li>
<li>Leaky ReLU</li>
</ol>
<h2 id="step-function">Step Function</h2>
<p>The Step Function is one of the earliest activation functions, used in perceptrons to make binary decisions.</p>
<h3 id="definition">Definition</h3>
<div class="arithmatex">\[
f(x) =
\begin{cases}
1 &amp; \text{if } x \geq 0 \\
0 &amp; \text{if } x &lt; 0
\end{cases}
\]</div>
<p>This is also known as the Heaviside Step Function.</p>
<p><img alt="Step Activation Function " src="../images/step_activation_derivative.png" /></p>
<h3 id="properties">Properties</h3>
<ul>
<li>Output Range: <span class="arithmatex">\(\{0, 1\}\)</span></li>
<li>Non-linear</li>
<li>Not differentiable at <span class="arithmatex">\(x = 0\)</span></li>
<li>No gradient, so not used in training modern neural networks</li>
<li>Used in early binary classifiers (e.g., Perceptrons)</li>
</ul>
<p>Its graph jumps from 0 to 1 at <span class="arithmatex">\(x = 0\)</span>, with no smooth transition.</p>
<h2 id="linear-activation">Linear Activation</h2>
<p>The Linear Activation Function (identity function) is defined as:</p>
<h3 id="definition_1">Definition</h3>
<div class="arithmatex">\[
f(x) = x
\]</div>
<h3 id="derivative">Derivative</h3>
<div class="arithmatex">\[
\frac{df}{dx} = 1
\]</div>
<p><img alt="Linear Activation Function " src="../images/linear_activation_derivative.png" /></p>
<h3 id="properties_1">Properties</h3>
<ul>
<li>Output Range: <span class="arithmatex">\((-\infty, \infty)\)</span></li>
<li>No non-linearity as it just passes input as output</li>
<li>Differentiable</li>
<li>Used in the output layer of regression models</li>
<li>Not suitable for hidden layers (makes stacked layers equivalent to a single layer)</li>
</ul>
<h2 id="sigmoid-function">Sigmoid Function</h2>
<p>The sigmoid (logistic) function maps any real value to the (0, 1) interval.</p>
<h3 id="function-definition">Function Definition</h3>
<div class="arithmatex">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<p><img alt="Sigmoid Activation Function" src="../images/sigmoid_activation_derivative.png" /></p>
<h3 id="properties_2">Properties</h3>
<ul>
<li>Output Range: <span class="arithmatex">\((0, 1)\)</span> i.e convenient to generate probabilities as output.</li>
<li>Non-linear</li>
<li>The function is differentiable and the gradient is smooth, i.e. no jumps in the ouput values.</li>
<li>Used in binary classification</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>The sigmoid’s derivative vanishes at its extreme input values (<span class="arithmatex">\(z \rightarrow - \infty\)</span> and <span class="arithmatex">\(z \rightarrow \infty\)</span>) and is thus proned to the issue called Vanishing Gradient problem.</li>
</ul>
<h3 id="derivative_1">Derivative</h3>
<p>We need to compute:</p>
<div class="arithmatex">\[
\frac{d}{dz} \sigma(z)
\]</div>
<p>Let:</p>
<div class="arithmatex">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</div>
<p>Differentiate:</p>
<div class="arithmatex">\[
\begin{align*}
\frac{d}{dz} \sigma(z) &amp;= \frac{d}{dz} \left(1 + e^{-z}\right)^{-1} \\
&amp;= -1 \cdot (1 + e^{-z})^{-2} \cdot (-e^{-z}) \\
&amp;= \frac{e^{-z}}{(1 + e^{-z})^2}
\end{align*}
\]</div>
<p>Now rewrite in terms of <span class="arithmatex">\(\sigma(z)\)</span>:</p>
<div class="arithmatex">\[
\boxed{
\frac{d}{dz} \sigma(z) = \sigma(z)(1 - \sigma(z))
}
\]</div>
<h2 id="tanh-function">Tanh Function</h2>
<p>The hyperbolic tangent maps real values to the range <span class="arithmatex">\((-1, 1)\)</span>. It&rsquo;s a scaled and shifted version of the sigmoid.</p>
<h3 id="function-definition_1">Function Definition</h3>
<div class="arithmatex">\[
\tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
\]</div>
<p><img alt="Tanh Activation Function" src="../images/tanh_activation_derivative.png" /></p>
<h3 id="properties_3">Properties</h3>
<ul>
<li>Output Range: <span class="arithmatex">\((-1, 1)\)</span></li>
<li>Zero-centered output</li>
<li>Smooth and differentiable</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>
<p>The gradient is much steeper than for the sigmoid (risk of jumps while descending)</p>
</li>
<li>
<p>There is also a Vanishing Gradient problem due to the derivative cancelling for <span class="arithmatex">\(z \rightarrow - \infty\)</span> and <span class="arithmatex">\(z \rightarrow \infty\)</span>.</p>
</li>
</ul>
<h3 id="derivative_2">Derivative</h3>
<p>We use:</p>
<div class="arithmatex">\[
\frac{d}{dz} \tanh(z) = 1 - \tanh^2(z)
\]</div>
<h4 id="proof">Proof</h4>
<p>Let <span class="arithmatex">\(t = \tanh(z)\)</span>:</p>
<div class="arithmatex">\[
\frac{d}{dz} \tanh(z) = \frac{d}{dz} \left( \frac{e^z - e^{-z}}{e^z + e^{-z}} \right)
\]</div>
<p>Using quotient rule:</p>
<p>Let:</p>
<ul>
<li><span class="arithmatex">\(u = e^z - e^{-z}\)</span></li>
<li><span class="arithmatex">\(v = e^z + e^{-z}\)</span></li>
</ul>
<p>Then:</p>
<div class="arithmatex">\[
\frac{d}{dz} \tanh(z) = \frac{u'v - uv'}{v^2}
\]</div>
<p>Compute:</p>
<ul>
<li><span class="arithmatex">\(u' = e^z + e^{-z}\)</span></li>
<li><span class="arithmatex">\(v' = e^z - e^{-z}\)</span></li>
</ul>
<p>So:</p>
<div class="arithmatex">\[
\frac{d}{dz} \tanh(z) = \frac{(e^z + e^{-z})(e^z + e^{-z}) - (e^z - e^{-z})(e^z - e^{-z})}{(e^z + e^{-z})^2}
\]</div>
<p>Simplify numerator:</p>
<div class="arithmatex">\[
(a + b)^2 - (a - b)^2 = 4ab \Rightarrow \text{So numerator becomes: } 4
\]</div>
<p>So:</p>
<div class="arithmatex">\[
\frac{d}{dz} \tanh(z) = \frac{4}{(e^z + e^{-z})^2}
\]</div>
<p>But:</p>
<div class="arithmatex">\[
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} \Rightarrow \tanh^2(z) = \left( \frac{e^z - e^{-z}}{e^z + e^{-z}} \right)^2
\]</div>
<p>And:</p>
<div class="arithmatex">\[
1 - \tanh^2(z) = \frac{(e^z + e^{-z})^2 - (e^z - e^{-z})^2}{(e^z + e^{-z})^2} = \frac{4}{(e^z + e^{-z})^2}
\]</div>
<p>Therefore:</p>
<div class="arithmatex">\[
\boxed{
\frac{d}{dz} \tanh(z) = 1 - \tanh^2(z)
}
\]</div>
<h2 id="relu-function">ReLU Function</h2>
<p>Rectified Linear Unit is widely used in deep learning due to its simplicity and efficiency.</p>
<h3 id="function-definition_2">Function Definition</h3>
<div class="arithmatex">\[
\text{ReLU}(z) = \max(0, z)
\]</div>
<p><img alt="ReLU Activation Function" src="../images/relu_activation_derivative.png" /></p>
<h3 id="properties_4">Properties</h3>
<ul>
<li>Output Range: <span class="arithmatex">\([0, \infty)\)</span></li>
<li>Not differentiable at <span class="arithmatex">\(z = 0\)</span></li>
<li>Computationally efficient</li>
<li>Sparse activations</li>
<li>Better gradient descent as the function does not saturate in both directions like the sigmoid and tanh. In other words, the Vanishing Gradient problem is half reduced.</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Unlike the hyperbolic tangent, it is not zero-centered</li>
<li>The range is infinite for positive input value (not bounded)</li>
<li>ReLU is not differentiable at zero (but this can be solved by choosing arbitrarily a value for the derivative of either 0 or 1 for <span class="arithmatex">\(z=0\)</span>)</li>
<li>The “Dying ReLU problem”</li>
</ul>
<h3 id="derivative_3">Derivative</h3>
<div class="arithmatex">\[
\frac{d}{dz} \text{ReLU}(z) =
\begin{cases}
1 &amp; \text{if } z &gt; 0 \\
0 &amp; \text{if } z &lt; 0 \\
\text{undefined (or subgradient = 0 or 1)} &amp; \text{if } z = 0
\end{cases}
\]</div>
<div class="arithmatex">\[
\boxed{
\frac{d}{dz} \text{ReLU}(z) = \mathbb{1}_{z &gt; 0}
}
\]</div>
<p>What is the Dying ReLU problem? When we look at the derivative, we see the gradient on the negative side is zero. During the backpropagation algorithm, the weights and biases are not updated and the neuron becomes stuck in an inactive state. We refer to it as ‘dead neuron.’ If a large number of nodes are stuck in dead states, the model capacity to fit the data is decreased.</p>
<p>To solve this serious issue, rectifier variants of the ReLU have been proposed:</p>
<h2 id="leaky-relu">Leaky ReLU</h2>
<p>Attempts to fix the dying ReLU problem (neurons output 0 and never recover).</p>
<h3 id="function-definition_3">Function Definition</h3>
<div class="arithmatex">\[
\text{Leaky ReLU}(z) =
\begin{cases}
z &amp; \text{if } z &gt; 0 \\
\alpha z &amp; \text{if } z \leq 0
\end{cases}
\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is a small constant (e.g. 0.01)</p>
<p><img alt="Leaky ReLU Activation Function" src="../images/leaky_relu_activation_derivative.png" /></p>
<p><strong>Cons</strong></p>
<ul>
<li>There is an extra parameter to tweak in the network, the slope value <span class="arithmatex">\(\alpha\)</span>, which is not trivial to get as its optimized value is different depending on the data to fit.</li>
</ul>
<h3 id="derivative_4">Derivative</h3>
<div class="arithmatex">\[
\frac{d}{dz} \text{Leaky ReLU}(z) =
\begin{cases}
1 &amp; \text{if } z &gt; 0 \\
\alpha &amp; \text{if } z \leq 0
\end{cases}
\]</div>
<p>Other activation functions include:</p>
<h2 id="exponential-linear-units-elus">Exponential Linear Units (ELUs)</h2>
<p>It does not have Rectifier in the name but the Exponential Linear Unit is another variant of ReLU.</p>
<h3 id="function-definition_4">Function Definition</h3>
<div class="arithmatex">\[
\text{ELU}(z) =
\begin{cases}
\alpha (e^z-1) &amp; \text{if } z &lt; 0 \\
\alpha z &amp; \text{if } z \geq 0
\end{cases}
\]</div>
<p>where <span class="arithmatex">\(z,\alpha \in \mathbb{R}\)</span> with <span class="arithmatex">\(\alpha\)</span> a hyper-parameter to be tuned.</p>
<p><img alt="ELU Activation Function" src="../images/elu_activation_derivative.png" /></p>
<p><strong>Pros</strong></p>
<ul>
<li>From high to low input values, the ELU smoothly decreases until it outputs the negative value <span class="arithmatex">\(-\alpha\)</span>. There is no more a ‘kick’ like in ReLU</li>
<li>ELU functions have shown to converge cost to zero faster and produce more accurate results</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>The parameter  <span class="arithmatex">\(\alpha\)</span> needs to be tuned; it is not learnt</li>
<li>For positive inputs, there is a risk of experiencing the Exploding Gradient problem (explanations further below in The risk of vanishing or exploding gradients)</li>
</ul>
<h2 id="scaled-exponential-linear-unit-selu">Scaled Exponential Linear Unit (SELU)</h2>
<h3 id="function-definition_5">Function Definition</h3>
<p>$$
\text{SELU}(z) = \lambda
\begin{cases}
\alpha (e^z-1) &amp; \text{if } z &lt; 0 \
\alpha z &amp; \text{if } z \geq 0
\end{cases}
$$
where <span class="arithmatex">\(z,\alpha \in \mathbb{R}\)</span></p>
<p>where <span class="arithmatex">\(\lambda = 1.0507 \text{ and } \alpha = 1.67326\)</span>. Why these specific values? The values come from a normalization procedure; the SELU activation introduces self-normalizing properties. It takes care of internal normalization which means each layer preserves the mean and variance from the previous layers. SELU enables this normalization by adjusting the mean and variance. It can be shown that, for self-normalizing neural networks (SNNs), neuron activations are pushed towards zero mean and unit variance when propagated through the network.</p>
<p><img alt="SELU Activation Function" src="../images/selu_activation_derivative.png" /></p>
<p><strong>Pros</strong></p>
<ul>
<li>All the rectifier’s advantages are at play</li>
<li>Thanks to internal normalization, the network converges faster.</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Not really a caveat in itself, but the SELU is outperforming other activation functions only for very deep networks.</li>
</ul>
<h2 id="gaussian-error-linear-unit-gelu">Gaussian Error Linear Unit (GELU)</h2>
<p>Another modification of ReLU is the Gaussian Error Linear Unit. It can be thought of as a smoother ReLU.
The definition is:</p>
<div class="arithmatex">\[
\text{GELU}(z) = z \cdot \Phi(z) = z \cdot \frac{1}{2} \left[1 + \text{erf}\left(\frac{z}{\sqrt{2}}\right)\right]
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(\Phi(z)\)</span> is the cumulative distribution function (CDF) of the standard normal distribution.</li>
<li><span class="arithmatex">\(\text{erf}(\cdot)\)</span> is the Gauss error function.</li>
</ul>
<p>GELU is the state-of-the-art activation function used in particular in models called Transformers. <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Transformer</a> model was introduced by Google Brain in 2017 to help in the multidisciplinary field of Natural Language Processing (NLP) that deals, among others, with tasks such as text translation or text summarization.</p>
<p><img alt="GELU Activation Function" src="../images/gelu_activation_derivative.png" /></p>
<p><strong>Pros</strong></p>
<ul>
<li>Differentiable for all input values </li>
<li>Avoids the Vanishing Gradient problem</li>
<li>The function is non-convex, non-monotonic and not linear in the positive domain: it has thus curvature at all points. This actually allowed GELUs to approximate better complicated functions that ReLUs or ELUs can as it weights inputs by their value and not their sign (like ReLu and ELU do)</li>
<li>The GELU, by construction, has a probabilistic interpretation (it is the expectaction of a stochastic regularizer)</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>GELU is time-consuming to compute</li>
</ul>
<h2 id="sigmoid-linear-unit-silu-and-swish">Sigmoid Linear Unit (SiLU) and Swish</h2>
<p>The SiLU and Swish are the same function, just introduced by different authors (the Swish authors are from Google Brain). It is a state-of-the-art function aiming at superceeding the hegemonic ReLU. The Swish is defined as a sigmoid multiplied with the identity:
$$
f(z) = \frac{z}{1+e^{-z}}
$$
The Swish function exhibits increased classification accuracy and consistently matches or outperforms ReLU activation function on deep networks (especially on image classification).</p>
<p><img alt="Swish Activation Function" src="../images/swish_activation_derivative.png" /></p>
<p><strong>Pros</strong></p>
<ul>
<li>It is differentiable on the whole range.</li>
<li>
<p>The function is smooth and non-monotonic (like GELU), which is an advantage to enhance input data during learning</p>
</li>
<li>
<p>Unlike the ReLU function, small negative values are not zeroed, allowing for a better modeling of the data. And large negative values are zeroed out (in other words, the node will die only if it needs to die)</p>
</li>
</ul>
<p><strong>Note:</strong> Swish function is only relevant if it is used in neural networks having a depth greater than 40 layers.</p>
<h2 id="the-risk-of-vanishing-or-exploding-gradients">The risk of vanishing or exploding gradients</h2>
<p>Training a neural network with a gradient-based learning method (the gradient descent is one) can lead to issues. The culprit, or rather cause, lies in the choice of the activation function:</p>
<h3 id="vanishing-gradient-problem">Vanishing Gradient problem</h3>
<p>As seen with the sigmoid and hyperbolic tangent, certain activation functions converge asymptotically towards the bounded range. Thus, at the extremities (large negative or large positive input values), a large change in the input will cause a very small modification of the output: there is a saturation. As a consequence the gradient will be also very small and the learning gain after one iteration very minimal, tending towards zero. This is to be avoid if we want the algorithm to learn a decent amount at each step.</p>
<h3 id="exploding-gradient-problem">Exploding Gradient problem</h3>
<p>If significant errors accumulate and the neural network updates the weights with larger and larger values, the difference between the prediction and observed values will increase further and further, leading to exploding gradients. It’s no more a descent but a failure to converge. Pragmatically, it is possible to see it when weights are so large that they overflow and return a NaN value (meaning Not A Number).</p>
<h2 id="impact-of-activation-functions-on-model-performance">Impact of Activation Functions on Model Performance</h2>
<p>The choice of activation function has a direct impact on the performance of a neural network in several ways:</p>
<ol>
<li>
<p><strong>Convergence Speed:</strong> Functions like ReLU allow faster training by avoiding the vanishing gradient problem while Sigmoid and Tanh can slow down convergence in deep networks.</p>
</li>
<li>
<p><strong>Gradient Flow:</strong> Activation functions like ReLU ensure better gradient flow, helping deeper layers learn effectively. In contrast Sigmoid can lead to small gradients, hindering learning in deep layers.</p>
</li>
<li>
<p><strong>Model Complexity:</strong> Activation functions like Softmax allow the model to handle complex multi-class problems, whereas simpler functions like ReLU or Leaky ReLU are used for basic layers.</p>
</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>Activation functions are essential to neural networks, enabling them to capture complex, non-linear relationships in data. We examined the mathematical formulations and derivatives of key activation functions, including Sigmoid, Tanh, ReLU, and Leaky ReLU—each offering unique strengths and trade-offs. From traditional functions to modern variants, the choice of activation function plays a critical role in a model’s performance and should be made based on the specific problem and network depth.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../pca/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Principal Component Analysis (PCA)">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Principal Component Analysis (PCA)
              </div>
            </div>
          </a>
        
        
          
          <a href="../loss_function/" class="md-footer__link md-footer__link--next" aria-label="Next: Loss Functions">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Loss Functions
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["content.code.copy", "content.math", "navigation.footer", "navigation.tabs", "toc.integrate", "header.autohide"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>